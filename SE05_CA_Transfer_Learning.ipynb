{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/se05.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/solutions/SE05_SL_Transfer_Learning.ipynb)\n",
    "\n",
    "## Workshop Overview\n",
    "***\n",
    "In this workshop, we explore transfer learning techniques for medical image segmentation. Starting from a custom dataset pipeline, we build a U-Net architecture from scratch and then leverage a pre-trained EfficientNet encoder to demonstrate the power of transfer learning.\n",
    "\n",
    "**Prerequisites**: CNNs (SE04), Neural networks (SE02), PyTorch fundamentals (SE01)\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand transfer learning concepts and when to apply them\n",
    "- Create custom PyTorch datasets for image segmentation tasks\n",
    "- Compute dataset statistics for normalization\n",
    "- Apply Albumentations for synchronized image and mask augmentation\n",
    "- Build a U-Net architecture for medical image segmentation\n",
    "- Implement and apply the Dice loss for binary segmentation\n",
    "- Apply transfer learning using a pre-trained EfficientNet encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab: downloading utils...\")\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt\",\n",
    "        \"-O\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"-x\",\n",
    "        \"-nH\",\n",
    "        \"--cut-dirs=3\",\n",
    "        \"-i\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(\"Running locally: skipping Colab utils download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_path = Path.cwd()\n",
    "if str(repo_path) not in sys.path:\n",
    "    sys.path.append(str(repo_path))\n",
    "\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "random.seed(101)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available. Please ensure you've enabled GPU in Runtime > Change runtime type\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Transfer Learning\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Transfer learning is a machine learning technique where a model developed for one task is reused as a starting point for a model on a second task. It's particularly effective for deep learning models, as it allows us to leverage pre-trained models' knowledge rather than starting from scratch.\n",
    "\n",
    "In previous sessions, we learned how to build and train neural networks from scratch. However, training large deep learning models requires:\n",
    "\n",
    "1. **Massive datasets** (often millions of examples)\n",
    "2. **Extensive computational resources** (often multiple GPUs)\n",
    "3. **Long training times** (days to weeks)\n",
    "\n",
    "Transfer learning addresses these challenges by letting us capitalise on existing models that have already been trained on large datasets.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/transfer.png\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "## 1.1 Transfer Learning Analogy\n",
    "***\n",
    "\n",
    "Transfer learning is inspired by human learning. Consider how we learn:\n",
    "\n",
    "| Human Learning | Machine Learning Parallel |\n",
    "|----------------|--------------------------|\n",
    "| A child learns to recognize basic shapes before identifying letters | A model learns edge detection before specific object recognition |\n",
    "| A musician who knows piano can learn guitar faster than a novice | A model trained on one image dataset can adapt quickly to a similar task |\n",
    "| Language skills transfer across related languages (e.g., Spanish to Italian) | NLP models pre-trained on one language can be fine-tuned for another |\n",
    "| Medical students learn general anatomy before specializing | Medical imaging models trained on general X-rays can be fine-tuned for specific conditions |\n",
    "| Engineers apply fundamental principles across different projects | Engineering models transfer physical principles across different applications |\n",
    "\n",
    "This mirrors how neural networks learn hierarchical features. Early layers learn general patterns that are often applicable across domains, while later layers learn task-specific features.\n",
    "\n",
    "\n",
    "## 1.2 When to Use Transfer Learning\n",
    "***\n",
    "\n",
    "Transfer learning is particularly useful in the following scenarios:\n",
    "\n",
    "| Scenario | Example | Benefit |\n",
    "|----------|---------|---------|\n",
    "| **Limited training data** | Medical imaging with few samples | Pre-trained features compensate for data scarcity |\n",
    "| **Similar domains** | From natural images to satellite imagery | Underlying features (edges, textures) transfer well |\n",
    "| **Time constraints** | Rapid prototyping needs | Accelerates model development cycle |\n",
    "| **Hardware limitations** | Training with limited GPU access | Reduces computational requirements |\n",
    "| **Preventing overfitting** | Small dataset applications | Regularization effect from pre-trained weights |\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Key Insight**: The effectiveness of transfer learning depends on the similarity between the source and target domains. The more similar they are, the more beneficial transfer learning becomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Case Study: Image Segmentation for Medical Imaging\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Image Segmentation**: The process of partitioning an image into multiple segments or regions, often used in medical imaging to identify and delineate structures within images (e.g., tumors, organs). It is a crucial step in many computer vision tasks, including object detection and recognition.\n",
    "\n",
    "For this session, we are going to be using the [**ISIC 2016 Skin Lesion Segmentation Challenge**](https://challenge.isic-archive.com/landing/2016/) dataset. This dataset contains dermoscopic images of skin lesions, along with their corresponding segmentation masks. The goal is to train a model to accurately segment the lesions from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('skin lesions',\n",
    "                                           dest_path=data_path,\n",
    "                                           extract=True,\n",
    "                                           remove_compressed=False)\n",
    "\n",
    "mask_path = utils.data.download_dataset('skin lesions masks',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True,\n",
    "                                   remove_compressed=False)\n",
    "\n",
    "test_path = utils.data.download_dataset('skin lesions test',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True,\n",
    "                                   remove_compressed=False)\n",
    "\n",
    "test_mask_path = utils.data.download_dataset('skin lesions test masks',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True,\n",
    "                                   remove_compressed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Challenges in Medical Image Segmentation\n",
    "***\n",
    "\n",
    "Medical image segmentation presents unique challenges compared to natural image segmentation:\n",
    "\n",
    "| Challenge | Description | Impact |\n",
    "|-----------|-------------|--------|\n",
    "| **Limited Data** | Medical datasets are typically smaller | Transfer learning becomes crucial |\n",
    "| **Class Imbalance** | Regions of interest often occupy a small portion of the image | Requires specialized loss functions |\n",
    "| **Ambiguous Boundaries** | Boundaries between tissues can be gradual or unclear | Makes precise segmentation difficult |\n",
    "| **Inter-observer Variability** | Different experts may segment the same image differently | Ground truth is not always definitive |\n",
    "| **High Stakes** | Errors can have serious consequences in medical applications | Demands higher accuracy and reliability |\n",
    "\n",
    "# 3. Preparing the Dataset\n",
    "***\n",
    "Segmentation tasks require both the input images and their corresponding masks. The masks are binary images where the pixels belonging to the object of interest (e.g., a tumor) are marked as 1 (or white), while the background is marked as 0 (or black). Thus, we need to load both the images and their masks for training.\n",
    "\n",
    "## 3.1 Custom Dataset Creation\n",
    "***\n",
    "\n",
    "In order for us to efficiently load the images and masks, we are going to create a custom dataset class. This class will inherit from the `torch.utils.data.Dataset` class and will handle loading the images and masks from the specified directories.\n",
    "\n",
    "The PyTorch `Dataset` class is an abstract class representing a dataset. Custom datasets should inherit from this class and override the following methods:\n",
    "\n",
    "| Method | Purpose | Implementation Requirements |\n",
    "|--------|---------|------------------------------|\n",
    "| `__init__` | Initialize the dataset | Define directories, transformations, and data loading parameters |\n",
    "| `__len__` | Return dataset size | Return the total number of samples |\n",
    "| `__getitem__` | Access a specific sample | Load and transform a sample with a given index |\n",
    "\n",
    "For image segmentation, our dataset needs to handle both input images and their corresponding segmentation masks:\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: Create a custom dataset class for loading\n",
    "\n",
    "``` python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the dataset, loading the images and masks from the specified directories.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of images in the dataset.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Defines how to get a single item (image and mask) from the dataset.\n",
    "        \n",
    "```\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 1**: Implement a custom `ISICDataset` class inheriting from `torch.utils.data.Dataset`, with `__init__`, `__len__`, and `__getitem__` methods that load images and masks using PIL and apply optional transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 1: Creating a Custom Dataset\n",
    "# In this exercise, you will implement a custom PyTorch Dataset class for loading the ISIC skin lesion images and masks.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, image_dir: Path | str, mask_dir: Path | str, img_transform=None, mask_transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Get sorted list of image files (*.jpg) from image_dir, and mask files (*.png) from mask_dir\n",
    "        self.images = ___\n",
    "        self.masks = ___\n",
    "\n",
    "    def __len__(self):\n",
    "        return ___  # Return the number of images in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.image_dir / img_name.name\n",
    "        # Mask filename: stem of img_name + '_segmentation.png'\n",
    "        mask_name = ___ + '_segmentation.png'\n",
    "        mask_path = self.mask_dir / mask_name\n",
    "\n",
    "        # Open image as RGB and mask as grayscale ('L')\n",
    "        image = Image.open(img_path).convert(___)\n",
    "        mask = Image.open(mask_path).convert(___)\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = ___(image)\n",
    "        if self.mask_transform:\n",
    "            mask = ___(mask)\n",
    "        return image, mask\n",
    "\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ds = ISICDataset(\n",
    "    image_dir=dataset_path,\n",
    "    mask_dir=mask_path,\n",
    "    img_transform=resize_transform,\n",
    "    mask_transform=resize_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compute the Mean and Standard Deviation of the Dataset\n",
    "***\n",
    "\n",
    "Computing dataset statistics is a critical step in preparing data for deep learning models. We need to normalize our images to help the model converge faster and perform better. By normalizing with the dataset's mean and standard deviation, we ensure that the input values have similar scales and distributions.\n",
    "\n",
    "The normalisation process follows this formula for each channel:\n",
    "\n",
    "$$x_{normalized} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the original pixel value\n",
    "- $\\mu$ is the mean of all pixels in the channel across the dataset\n",
    "- $\\sigma$ is the standard deviation of all pixels in the channel across the dataset\n",
    "\n",
    "First, we need to load the dataset and then compute the mean and standard deviation across all images.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 2**: Create a `DataLoader` for the dataset and iterate through it to compute the per-channel mean and standard deviation of all images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 2: Calculating Mean and Std of Images\n",
    "# In this exercise, you will create a DataLoader and compute the per-channel mean and standard deviation of the dataset.\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Create a DataLoader with batch size 16 and shuffle=False\n",
    "dl = DataLoader(___, batch_size=16, shuffle=___)\n",
    "\n",
    "n_channels = 3\n",
    "n_pixels = 0\n",
    "channel_sum = torch.zeros(n_channels)\n",
    "channel_squared_sum = torch.zeros(n_channels)\n",
    "\n",
    "for images, _ in tqdm(dl, desc=\"Calculating mean and std\"):\n",
    "    # Reshape to (batch_size, channels, height*width)\n",
    "    images = images.view(images.size(0), n_channels, ___)\n",
    "    # Update pixel count (batch_size * pixels_per_channel)\n",
    "    n_pixels += ___\n",
    "\n",
    "    # Update sums (sum over batch and spatial dimensions)\n",
    "    channel_sum += images.sum(dim=___)\n",
    "    channel_squared_sum += (images ** 2).sum(dim=___)\n",
    "\n",
    "# Calculate mean as sum divided by total pixel count\n",
    "mean = ___\n",
    "# Calculate std using: sqrt((sum_of_squares / count) - mean^2)\n",
    "std = torch.sqrt(___)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Augmentation\n",
    "***\n",
    "We are going to use the albumentations library for data augmentation. This library outperforms the torchvision library in terms of speed and flexibility. It provides the same transformations as torchvision and it is also compatible with PyTorch.\n",
    "\n",
    "### 3.3.1 Choosing Appropriate Augmentations for Medical Images\n",
    "\n",
    "| Augmentation Type | Purpose | Medical Imaging Considerations |\n",
    "|-------------------|---------|-------------------------------|\n",
    "| **Geometric Transforms** | Rotate, flip, resize | Should preserve diagnostic features |\n",
    "| **Color Adjustments** | Brightness, contrast, saturation | Use carefully to maintain diagnostic appearance |\n",
    "| **Noise Addition** | Add random noise | Models will be more robust to image noise |\n",
    "| **Elastic Deformations** | Simulate tissue deformation | Especially useful for soft tissue imaging |\n",
    "| **Cropping** | Focus on different regions | Ensures focus on different areas of lesion |\n",
    "\n",
    "### 3.3.2 Sync vs. Async Augmentation\n",
    "\n",
    "For segmentation tasks, we need to ensure that the same transformations are applied to both the image and its corresponding mask. This is called synchronized (sync) augmentation, as opposed to asynchronous (async) augmentation where different transformations are applied to inputs and targets.\n",
    "\n",
    "| Type | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| **Sync Augmentation** | Apply identical spatial transforms to image and mask | Required for segmentation tasks |\n",
    "| **Async Augmentation** | Apply different transforms | Typically used for classification only |\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Important**: When augmenting for segmentation tasks, always ensure that geometric transformations (flips, rotations, etc.) are applied identically to both the image and its mask to maintain pixel-to-pixel correspondence.\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 2**: Using albumentations for data augmentation\n",
    "\n",
    "``` python\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=mean, std=std, p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "```\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 3**: Build an Albumentations training pipeline (with resize to 64×64, flips, rotation, colour jitter, normalisation, and `ToTensorV2`) and a validation pipeline (with resize and normalisation only).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 3: Data Augmentation with Albumentations\n",
    "# In this exercise, you will create training and validation augmentation pipelines using the Albumentations library.\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Training pipeline: Resize to 64×64, HorizontalFlip (p=0.5), VerticalFlip (p=0.5),\n",
    "# Rotate (limit=10, p=0.5), ColorJitter, Normalize, ToTensorV2\n",
    "train_img_ts = A.Compose([\n",
    "    A.Resize(64, 64),\n",
    "    A.HorizontalFlip(p=___),\n",
    "    A.VerticalFlip(p=___),\n",
    "    A.Rotate(limit=___, p=0.5),\n",
    "    A.ColorJitter(brightness=___, contrast=___, saturation=___, hue=___, p=0.5),\n",
    "    A.Normalize(mean=mean.tolist(), std=std.tolist(), p=1.0),\n",
    "    ___()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Validation pipeline: Resize to 64×64, Normalize, ToTensorV2 (no augmentation)\n",
    "valid_img_ts = A.Compose([\n",
    "    A.Resize(64, 64),\n",
    "    A.Normalize(mean=mean.tolist(), std=std.tolist(), p=1.0),\n",
    "    ___()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Modifying the Dataset Class to use Albumentations\n",
    "***\n",
    "Since normal PyTorch transforms do not support the synchronized augmentation, we need to modify our dataset class to use albumentations. We will also add the normalization step in the `__getitem__` method.\n",
    "\n",
    "We are going to create a new class called that inherits from our `ISICDataset` class. This will allow us to override the `__getitem__` method and apply the albumentations transformations.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 4**: Create `ISICDatasetAlbumentations` inheriting from `ISICDataset`, override `__getitem__` to convert images and masks to NumPy arrays, apply Albumentations transforms synchronously, and ensure masks are binary (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4: Implementing the Albumentations Dataset Class\n",
    "# In this exercise, you will extend ISICDataset to apply synchronized Albumentations transforms to both images and masks.\n",
    "\n",
    "class ISICDatasetAlbumentations(ISICDataset):\n",
    "    def __init__(self, image_dir: Path | str, mask_dir: Path | str, transform=None):\n",
    "        super().__init__(image_dir, mask_dir, transform, None)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.image_dir / img_name.name\n",
    "        mask_name = img_name.stem + '_segmentation.png'\n",
    "        mask_path = self.mask_dir / mask_name\n",
    "\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Open image as RGB and mask as grayscale\n",
    "        image = Image.open(img_path).convert(___)\n",
    "        # Convert to numpy array for albumentations\n",
    "        image = np.array(___)\n",
    "        mask = Image.open(mask_path).convert(___)\n",
    "        mask = np.array(___)\n",
    "\n",
    "        # Normalize mask to 0-1 range if max > 1\n",
    "        if mask.max() > 1:\n",
    "            mask = ___  # divide by 255.0\n",
    "\n",
    "        if self.img_transform:\n",
    "            # Apply albumentations transform to both image and mask jointly\n",
    "            aug = self.img_transform(image=___, mask=___)\n",
    "            image = aug['image']\n",
    "            mask = aug['mask']\n",
    "\n",
    "            # Ensure mask is binary (0 or 1)\n",
    "            if isinstance(mask, torch.Tensor):\n",
    "                mask = (mask > ___).float()\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Splitting the Dataset into Train, and Validation Sets\n",
    "***\n",
    "Unfortunately, PyTorch `Dataset` class does not have a built-in method for splitting datasets. However, we can use the `torch.utils.data.random_split` function to split our dataset into training and validation sets. Then, we can create separate `DataLoader` instances for each split.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 3**: Splitting the dataset into training and validation sets\n",
    "\n",
    "``` python\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "```\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 5**: Split the full `ISICDatasetAlbumentations` dataset 80/20 into training and validation sets, then wrap each split in a `DataLoader` with batch size 16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 5: Splitting the Dataset into Train and Validation Sets\n",
    "# In this exercise, you will split the dataset 80/20 and create DataLoaders for each split.\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "full_ds = ISICDatasetAlbumentations(\n",
    "    image_dir=dataset_path,\n",
    "    mask_dir=mask_path,\n",
    "    transform=___  # training transforms\n",
    ")\n",
    "\n",
    "# Calculate 80/20 split sizes\n",
    "train_size = int(___ * len(full_ds))\n",
    "valid_size = len(full_ds) - ___\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_ds, valid_ds = random_split(full_ds, [___, ___])\n",
    "\n",
    "# Create DataLoaders (batch size 16; shuffle training, don't shuffle validation)\n",
    "train_dl = DataLoader(train_ds, batch_size=___, shuffle=___)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=___, shuffle=___)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a batch of images and masks\n",
    "utils.plotting.show_binary_segmentation_batch(train_dl,\n",
    "                                              n_images=10,\n",
    "                                              mean=mean,\n",
    "                                              std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Baseline Model: U-Net Architecture\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\" /> **U-Net**: A convolutional neural network architecture designed for biomedical image segmentation. It consists of a contracting path (encoder) and an expansive path (decoder), allowing it to capture both context and localization information.\n",
    "\n",
    "The U-Net architecture is widely used in medical image segmentation tasks due to its ability to learn both local and global features. The architecture is shown below:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/unet.png\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The U-Net architecture consists of two main parts: the encoder and the decoder, connected by skip connections. Each component plays a specific role:\n",
    "\n",
    "| Component | Description | Purpose |\n",
    "|-----------|-------------|---------|\n",
    "| **Encoder (Contracting Path)** | Series of convolutional and pooling layers | Captures context and semantic information |\n",
    "| **Decoder (Expanding Path)** | Series of upsampling and convolutional layers | Enables precise localization |\n",
    "| **Skip Connections** | Connect encoder layers to decoder layers | Preserve spatial information lost during downsampling |\n",
    "| **Bottleneck** | Deepest layer connecting encoder and decoder | Captures the most complex features |\n",
    "\n",
    "## 4.1 Transposed Convolution\n",
    "***\n",
    "\n",
    "For this architecture we are going to use a special type of convolutional layer that upsamples the input feature maps. This layer is called a transposed convolutional layer (also known as a deconvolutional layer). It is used to increase the spatial dimensions of the input feature maps, allowing the model to learn more complex features.\n",
    "\n",
    "| Parameter | Description | Effect on Output |\n",
    "|-----------|-------------|-----------------|\n",
    "| **Kernel Size** | Size of the filter | Determines area of influence |\n",
    "| **Stride** | Step size of the filter | Controls amount of upsampling |\n",
    "| **Padding** | Zero-padding added to input | Affects output size |\n",
    "| **Output Padding** | Additional padding for output | Fine-tunes output dimensions |\n",
    "\n",
    "The formula for calculating the output size of a transposed convolutional layer is:\n",
    "\n",
    "$$\\text{Output Size} = (\\text{Input Size} - 1) \\times \\text{Stride} - 2 \\times \\text{Padding} + \\text{Kernel Size} + \\text{Output Padding}$$\n",
    "\n",
    "## 4.2 U-Net Implementation in PyTorch\n",
    "***\n",
    "We are going to implement the U-Net architecture using PyTorch. The implementation will consist of the following components:\n",
    "\n",
    "| Component | Description |\n",
    "|----------|-------------|\n",
    "| `DoubleConv` | A block that consists of two convolutional layers followed by batch normalization and ReLU activation. |\n",
    "| `Down` | A block that consists of a max pooling layer followed by a `DoubleConv` block. |\n",
    "| `Up` | A block that consists of a transposed convolutional layer followed by a `DoubleConv` block. |\n",
    "| `UNet` | The main U-Net architecture that consists of the encoder and decoder blocks. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 6: Implementing the DoubleConv Block\n",
    "# In this exercise, you will implement the foundational double-convolution block used in U-Net.\n",
    "\n",
    "class DoubleConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Create a sequential module with two conv layers, each followed by BatchNorm2d and ReLU\n",
    "        # Use kernel_size=3, padding=1 for both conv layers\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=___, padding=___),\n",
    "            torch.nn.BatchNorm2d(___),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=___, padding=___),\n",
    "            torch.nn.BatchNorm2d(___),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ___(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 7: Implementing the Down Block\n",
    "# In this exercise, you will implement the encoder (downsampling) block with max pooling followed by DoubleConv.\n",
    "\n",
    "class Down(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Create a sequential module: MaxPool2d(2) followed by DoubleConv\n",
    "        self.maxpool_conv = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(___),\n",
    "            DoubleConv(___, ___)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ___(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 8: Implementing the Up Block\n",
    "# In this exercise, you will implement the decoder (upsampling) block with transposed convolution and skip connections.\n",
    "\n",
    "class Up(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # ConvTranspose2d: in_channels -> in_channels//2, kernel_size=2, stride=2\n",
    "        self.up = torch.nn.ConvTranspose2d(___, ___, kernel_size=___, stride=___)\n",
    "        # DoubleConv takes concatenated channels (in_channels full) -> out_channels\n",
    "        self.conv = DoubleConv(___, ___)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Calculate padding to match x2's spatial dimensions\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "        \n",
    "        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                                          diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # Concatenate along the channel dimension, then apply conv\n",
    "        x = torch.cat([___, ___], dim=1)\n",
    "        return self.conv(___)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 9: Implementing the UNet Model\n",
    "# In this exercise, you will assemble the full U-Net architecture using the DoubleConv, Down, and Up blocks.\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Initial convolution block\n",
    "        self.inc = DoubleConv(___, 64)\n",
    "        \n",
    "        # Encoder path (increasing channel depths)\n",
    "        self.down1 = Down(64, ___)\n",
    "        self.down2 = Down(___, 256)\n",
    "        self.down3 = Down(256, ___)\n",
    "        self.down4 = Down(___, 1024)\n",
    "        \n",
    "        # Decoder path (decreasing channel depths)\n",
    "        self.up1 = Up(1024, ___)\n",
    "        self.up2 = Up(___, 256)\n",
    "        self.up3 = Up(256, ___)\n",
    "        self.up4 = Up(___, 64)\n",
    "        \n",
    "        # Output layer: 64 channels -> out_channels, kernel_size=1\n",
    "        self.outc = torch.nn.Conv2d(64, ___, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.inc(___)\n",
    "        x2 = self.down1(___)\n",
    "        x3 = self.down2(___)\n",
    "        x4 = self.down3(___)\n",
    "        x5 = self.down4(___)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        x = self.up1(x5, ___)\n",
    "        x = self.up2(x, ___)\n",
    "        x = self.up3(x, ___)\n",
    "        x = self.up4(x, ___)\n",
    "        \n",
    "        output = self.outc(___)\n",
    "        return torch.nn.functional.sigmoid(___)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Segmentation Loss Functions\n",
    "***\n",
    "While we can use a simple loss function like binary cross-entropy for segmentation tasks since we are dealing with binary masks, it is often not sufficient. This is because the model may learn to predict the background class (0) more often than the foreground class (1), leading to poor performance on the actual segmentation task.\n",
    "\n",
    "To address this, we are going to introduce a more sophisticated loss function called the **Dice Loss**. The Dice Loss is based on the Dice coefficient, which measures the overlap between two sets. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Dice} = \\frac{2 |X \\cap Y|}{|X| + |Y|}$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the predicted segmentation mask\n",
    "- $Y$ is the ground truth segmentation mask\n",
    "- $|X|$ is the number of pixels in the predicted mask\n",
    "- $|Y|$ is the number of pixels in the ground truth mask\n",
    "- $|X \\cap Y|$ is the number of pixels in the intersection of the predicted and ground truth masks\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/dice.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The Dice Loss is defined as:\n",
    "$$\n",
    "\\text{Dice Loss} = 1 - \\text{Dice}\n",
    "$$\n",
    "\n",
    "### 4.3.1 Comparison of Segmentation Loss Functions\n",
    "***\n",
    "| Loss Function | Formula | Advantages | Disadvantages |\n",
    "|---------------|---------|------------|---------------|\n",
    "| **Binary Cross-Entropy** | $-\\sum(y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y}))$ | Easy to implement, works well for balanced classes | Poor performance with class imbalance |\n",
    "| **Dice Loss** | $1 - \\frac{2\\|X \\cap Y\\|}{\\|X\\| + \\|Y\\|}$ | Handles class imbalance well, directly optimizes overlap | May get stuck in local minima |\n",
    "| **Focal Loss** | $-\\alpha(1-\\hat{y})^\\gamma y\\log(\\hat{y})$ | Focuses on hard examples, addresses class imbalance | Requires tuning of hyperparameters |\n",
    "| **IoU Loss** | $1 - \\frac{\\|X \\cap Y\\|}{\\|X \\cup Y\\|}$ | Directly optimizes intersection over union | Can be unstable for small regions |\n",
    "| **Combo Loss** | $\\alpha\\cdot BCE + (1-\\alpha)\\cdot Dice$ | Combines benefits of both BCE and Dice | Requires tuning of weighting parameter |\n",
    "\n",
    "This loss function is particularly useful for imbalanced datasets, where the number of pixels in the foreground class is much smaller than the number of pixels in the background class. The Dice Loss penalizes the model more for misclassifying foreground pixels than background pixels, leading to better performance on the segmentation task.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 10**: Implement the `DiceLoss` class inheriting from `nn.Module`, with a `smooth` parameter and a `forward` method that computes the Dice loss from flattened predictions and targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 10: Implementing the Dice Loss Function\n",
    "# In this exercise, you will implement the Dice loss function for binary image segmentation.\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        self.smooth = ___\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Flatten the prediction and target tensors to 1D\n",
    "        y_pred = y_pred.view(___)\n",
    "        # Convert target to float\n",
    "        y_true = y_true.view(___).float()\n",
    "        \n",
    "        # Calculate intersection (element-wise multiplication, then sum)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        # Calculate union (sum of each tensor)\n",
    "        union = y_pred.sum() + y_true.sum()\n",
    "        \n",
    "        # Calculate Dice coefficient with smoothing: (2*intersection + smooth) / (union + smooth)\n",
    "        dice = ___\n",
    "        \n",
    "        # Return loss clamped between 0 and 1\n",
    "        return torch.clamp(1 - dice, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, criterion, and optimizer\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "criterion = DiceLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 2\n",
    "\n",
    "model_v1 = utils.ml.train_model(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimiser,\n",
    "    train_loader=train_dl,\n",
    "    val_loader=valid_dl,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping=True,\n",
    "    patience=3,\n",
    "    save_path=Path.cwd() / \"my_models\" / \"se05_model_v1.pt\",\n",
    "    plot_loss=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a batch of images and masks\n",
    "test_ds = ISICDatasetAlbumentations(\n",
    "    image_dir=test_path,\n",
    "    mask_dir=test_mask_path,\n",
    "    transform=valid_img_ts\n",
    ")\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "utils.plotting.show_binary_segmentation_predictions(model_v1, test_dl, n_images=10, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transfer Learning with Pre-trained Models\n",
    "***\n",
    "In essence, the process of transfer learning involves taking a model that has been trained on a large dataset (the source domain) and adapting it to a new, smaller dataset (the target domain). This is done by reusing the learned features from the source model and fine-tuning them for the target task. The most common approach is to use the pre-trained model as a feature extractor, where the lower layers of the model are frozen and using a classifier head that is trained on the new dataset.\n",
    "\n",
    "The process can be visualized as follows:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/transfer_custom.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "## 5.1 Pre-trained Models for Computer Vision\n",
    "***\n",
    "\n",
    "Several pre-trained models are available for computer vision tasks. Each has its own architecture, number of parameters, and performance characteristics:\n",
    "\n",
    "| Model | Parameters | Input Size | Year | Top-1 Accuracy (ImageNet) | Architecture Highlights |\n",
    "|-------|------------|------------|------|---------------------------|-------------------------|\n",
    "| **ResNet** | 11.7M - 60M | 224×224 | 2015 | 76.1% - 80.6% | Residual connections to combat vanishing gradients |\n",
    "| **VGG** | 138M - 144M | 224×224 | 2014 | 71.3% - 75.6% | Simple architecture with small filters (3×3) |\n",
    "| **Inception** | 6.8M - 54M | 299×299 | 2014 | 77.5% - 82.8% | Multi-scale processing with parallel paths |\n",
    "| **DenseNet** | 8M - 44M | 224×224 | 2017 | 74.5% - 79.5% | Dense connections between layers for feature reuse |\n",
    "| **EfficientNet** | 5.3M - 66M | 224×224 - 600×600 | 2019 | 78.8% - 85.7% | Balanced scaling of depth, width, and resolution |\n",
    "| **MobileNet** | 4.2M - 6.9M | 224×224 | 2017 | 70.6% - 75.2% | Designed for mobile devices with depthwise separable convolutions |\n",
    "\n",
    "In this section, we are going to use a pre-trained model for the segmentation task. We are going to use an EfficientNet model that has been pre-trained on the ImageNet dataset. The EfficientNet model is a state-of-the-art convolutional neural network architecture that achieves high accuracy with fewer parameters compared to other architectures.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/efficient.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "## 5.2 Transfer Learning Process\n",
    "***\n",
    "\n",
    "The typical transfer learning workflow consists of these steps:\n",
    "\n",
    "| Step | Description | Technique |\n",
    "|------|-------------|-----------|\n",
    "| **1. Select Source Model** | Choose a pre-trained model relevant to your target task | Select models trained on large datasets like ImageNet |\n",
    "| **2. Feature Extraction** | Use the pre-trained model as a fixed feature extractor | Freeze pre-trained layers, replace and retrain output layers |\n",
    "| **3. Fine-tuning** | Carefully adapt pre-trained weights to the new task | Gradually unfreeze layers, train with lower learning rates |\n",
    "| **4. Model Adaptation** | Modify architecture if needed for the target task | Add or remove layers as needed for the new domain |\n",
    "\n",
    "## 5.3 Types of Transfer Learning\n",
    "***\n",
    "\n",
    "There are several approaches to implementing transfer learning:\n",
    "\n",
    "| Approach | Description | Best Used When |\n",
    "|----------|-------------|---------------|\n",
    "| **Feature Extraction** | Freeze pre-trained network, replace and retrain classifier | Target task is similar but dataset is small |\n",
    "| **Fine-Tuning** | Retrain some or all layers of pre-trained network | Target task has sufficient data but benefits from pre-training |\n",
    "| **One-shot Learning** | Learn from just one or very few examples | Extreme data scarcity |\n",
    "| **Domain Adaptation** | Adapt to new data distribution without labels | Source and target domains have distribution shift |\n",
    "| **Multi-task Learning** | Train model on multiple related tasks simultaneously | Related tasks can benefit from shared representations |\n",
    "\n",
    "## 5.4 EfficientNet as an Encoder\n",
    "***\n",
    "\n",
    "The EfficientNet model is going to act as the encoder part of the U-Net architecture. We are going to replace the encoder part of the U-Net architecture with the EfficientNet model. The decoder part of the U-Net architecture will remain the same.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/unet_efficient.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "### 5.4.1 Transfer Learning Process for Segmentation\n",
    "***\n",
    "| Step | Description | Implementation Detail |\n",
    "|------|-------------|----------------------|\n",
    "| **1. Extract Encoder** | Use pre-trained EfficientNet as encoder | Remove classification head |\n",
    "| **2. Add Decoder** | Create U-Net style decoder | Transposed convolutions with skip connections |\n",
    "| **3. Freeze Weights** | Prevent pre-trained encoder from changing | Set `requires_grad=False` on encoder layers |\n",
    "| **4. Train Decoder** | Train only the decoder initially | Optimize only unfrozen parameters |\n",
    "| **5. Fine-tune (Optional)** | Gradually unfreeze encoder layers | Use lower learning rate for pre-trained layers |\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 4**: Loading the EfficientNet model and freezing layers\n",
    "\n",
    "``` python\n",
    "from torchvision import models as tvm\n",
    "\n",
    "efficientnet = tvm.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "efficientnet = efficientnet.features  # Extract the feature extractor part\n",
    "efficientnet = torch.nn.Sequential(*list(efficientnet.children())[:-1])  # Remove the classification head\n",
    "\n",
    "for param in efficientnet.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "```\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 11**: Implement `UNetEfficient` using a pre-trained EfficientNet-B0 encoder with frozen weights, a center bottleneck, and a `SmartUp` decoder path with skip connections, producing binary segmentation outputs via sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 11: Implementing U-Net with EfficientNet Encoder\n",
    "# In this exercise, you will build a U-Net variant using a pre-trained EfficientNet-B0 as a frozen encoder.\n",
    "\n",
    "import torchvision.models as tvm\n",
    "\n",
    "class SmartUp(torch.nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(SmartUp, self).__init__()\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # ConvTranspose2d: in_channels -> skip_channels, kernel_size=2, stride=2\n",
    "        self.up = torch.nn.ConvTranspose2d(___, ___, kernel_size=___, stride=___)\n",
    "        # DoubleConv takes skip_channels*2 (concatenation) -> out_channels\n",
    "        self.conv = DoubleConv(___ * 2, ___)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Handle size mismatches with bilinear interpolation\n",
    "        if x1.size()[2:] != x2.size()[2:]:\n",
    "            x1 = torch.nn.functional.interpolate(x1, size=x2.size()[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        x = torch.cat([___, ___], dim=1)\n",
    "        return self.conv(___)\n",
    "    \n",
    "class UNetEfficient(torch.nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNetEfficient, self).__init__()\n",
    "        \n",
    "        # Load pretrained EfficientNet-B0\n",
    "        efficient_net = tvm.efficientnet_b0(weights=___)\n",
    "        \n",
    "        # Extract feature layers - using UNet encoder nomenclature\n",
    "        # Hint: efficient_net.features is a list of blocks [0..8]\n",
    "        self.inc = torch.nn.Sequential(*list(efficient_net.features)[:2])  # Initial block\n",
    "        self.down1 = efficient_net.features[___ ]                           # MBConv block 2\n",
    "        self.down2 = efficient_net.features[___ ]                           # MBConv block 3\n",
    "        self.down3 = efficient_net.features[___ ]                           # MBConv block 4\n",
    "        self.down4 = torch.nn.Sequential(                                   # MBConv blocks 5,6,7\n",
    "            efficient_net.features[___],\n",
    "            efficient_net.features[___],\n",
    "            efficient_net.features[___]\n",
    "        )\n",
    "        \n",
    "        # Freeze encoder layers to preserve pretrained weights\n",
    "        encoders = [self.inc, self.down1, self.down2, self.down3, self.down4]\n",
    "        for encoder in encoders:\n",
    "            for param in encoder.parameters():\n",
    "                param.requires_grad = ___  # Set to False to freeze\n",
    "        \n",
    "        # Output channel sizes of each encoder stage\n",
    "        enc1_channels = 16\n",
    "        enc2_channels = 24\n",
    "        enc3_channels = 40\n",
    "        enc4_channels = 80\n",
    "        enc5_channels = 320\n",
    "        \n",
    "        # Center bottleneck\n",
    "        self.center = DoubleConv(___, 512)\n",
    "        \n",
    "        # Decoder path with SmartUp blocks\n",
    "        self.up1 = SmartUp(512, ___, enc4_channels)\n",
    "        self.up2 = SmartUp(enc4_channels, ___, enc3_channels)\n",
    "        self.up3 = SmartUp(enc3_channels, ___, enc2_channels)\n",
    "        self.up4 = SmartUp(enc2_channels, ___, enc1_channels)\n",
    "        \n",
    "        # Final 1×1 output convolution\n",
    "        self.outc = torch.nn.Conv2d(___, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_size = x.size()[2:]\n",
    "        \n",
    "        # Encoder path\n",
    "        x1 = self.inc(___)\n",
    "        x2 = self.down1(___)\n",
    "        x3 = self.down2(___)\n",
    "        x4 = self.down3(___)\n",
    "        x5 = self.down4(___)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.center(___)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        x = self.up1(x, ___)\n",
    "        x = self.up2(x, ___)\n",
    "        x = self.up3(x, ___)\n",
    "        x = self.up4(x, ___)\n",
    "        \n",
    "        output = self.outc(___)\n",
    "        \n",
    "        # Resize to original input dimensions if needed\n",
    "        if output.size()[2:] != input_size:\n",
    "            output = torch.nn.functional.interpolate(output, size=input_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return torch.nn.functional.sigmoid(___)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the EfficientNet UNet model\n",
    "model_efficient = UNetEfficient(in_channels=3, out_channels=1).to(device)\n",
    "criterion_efficient = DiceLoss()\n",
    "optimizer_efficient = torch.optim.Adam(filter(lambda p: p.requires_grad, model_efficient.parameters()), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "model_efficient = utils.ml.train_model(\n",
    "    model=model_efficient,\n",
    "    criterion=criterion_efficient,\n",
    "    optimiser=optimizer_efficient,\n",
    "    train_loader=train_dl,\n",
    "    val_loader=valid_dl,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping=True,\n",
    "    patience=3,\n",
    "    save_path= Path.cwd() / \"my_models\" / \"se05_model_v2.pt\",\n",
    "    plot_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with EfficientNet UNet\n",
    "utils.plotting.compare_binary_segmentation_models(\n",
    "    model_v1, model_efficient, test_dl, n_images=10, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Advantages of Transfer Learning\n",
    "***\n",
    "Transfer learning is particularly effective in computer vision and natural language processing (NLP) tasks, where large pre-trained models are available. The key advantages of transfer learning include:\n",
    "\n",
    "| Advantage | Description | Impact |\n",
    "|-----------|-------------|--------|\n",
    "| **Reduced Training Time** | Start with pre-learned features instead of random weights | Training can be 5-10x faster than from scratch |\n",
    "| **Less Training Data** | Leverage knowledge from the source domain | Can work with hundreds vs. thousands of examples |\n",
    "| **Better Performance** | Often achieves higher accuracy than training from scratch | Especially beneficial with limited target data |\n",
    "| **Faster Convergence** | Models typically reach optimal performance in fewer epochs | Reduces computational costs of model development |\n",
    "| **Lower Computational Cost** | Requires fewer resources for training | Makes deep learning accessible with limited hardware |\n",
    "| **Knowledge Retention** | Preserves useful features learned from large datasets | Captures generalizable representations across domains |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
