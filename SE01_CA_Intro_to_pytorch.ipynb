{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/se_01.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/SE01_CA_Intro_to_pytorch.ipynb)\n",
    "\n",
    "## Workshop Overview\n",
    "***\n",
    "Welcome! This workshop introduces PyTorch with a focus on scientific computing applications. Whether you're analyzing experimental data, running simulations, or building models for physical systems, PyTorch provides powerful tools for numerical computation and GPU acceleration.\n",
    "\n",
    "**Prerequisites**: Basic Python, familiarity with NumPy arrays and Pandas DataFrames\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand PyTorch tensors as GPU-accelerated arrays\n",
    "- Convert between NumPy, Pandas, and PyTorch\n",
    "- Perform scientific computations efficiently\n",
    "- Load and process real scientific datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 1. Why PyTorch for Scientific Computing?\n",
    "***\n",
    "[PyTorch](https://pytorch.org/) is an open-source machine learning framework that excels at scientific computing tasks requiring:\n",
    "\n",
    "- **GPU Acceleration**: Move computations from CPU to GPU with a single line of code\n",
    "- **Automatic Differentiation**: Compute gradients automatically (essential for optimization and inverse problems)\n",
    "- **NumPy Compatibility**: Seamless conversion between NumPy and PyTorch\n",
    "- **Dynamic Computation**: Build and modify computational graphs on-the-fly\n",
    "\n",
    "## PyTorch vs. Traditional Scientific Python\n",
    "\n",
    "While NumPy and SciPy are excellent for CPU-based scientific computing, PyTorch offers:\n",
    "\n",
    "1. **Hardware Flexibility**: Same code runs on CPU or GPU\n",
    "2. **Autograd**: Built-in automatic differentiation for optimization\n",
    "3. **Deep Learning Integration**: Easily transition from data processing to model building\n",
    "4. **Production Ready**: Deploy research code to production environments\n",
    "\n",
    "## Setting up the environment\n",
    "\n",
    "We are going to use different python modules throughout this course. It is not necessary to be familiar with all of them at the moment. Some of these libraries enable us to work with data and perform numerical operations, while others are used for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch Setup Information\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"✗ No GPU detected - using CPU\")\n",
    "    print(\"  (For GPU: Runtime > Change runtime type > Hardware accelerator > GPU)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 2. Scientific Python Refresher: NumPy & Pandas\n",
    "***\n",
    "\n",
    "Before diving into PyTorch, let's briefly review NumPy and Pandas. If you're already comfortable with these libraries, feel free to skim this section.\n",
    "\n",
    "## 2.1 NumPy: Arrays for Numerical Data\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: NumPy arrays are n-dimensional containers for homogeneous data (all elements have the same type). They're ideal for representing grids, matrices, sensor readings, or any structured numerical data.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Common Use Cases**:\n",
    "- Time series data (temperature, pressure, voltage readings)\n",
    "- Spatial data (2D/3D grids from simulations)\n",
    "- Experimental measurements\n",
    "- Image data from microscopes or telescopes\n",
    "\n",
    "Let's create some example scientific data using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Temperature sensor readings over time (1D array)\n",
    "time_hours = np.linspace(0, 24, 25)  # 0 to 24 hours\n",
    "temperature_celsius = 20 + 5 * np.sin(2 * np.pi * time_hours / 24) + np.random.normal(0, 0.5, 25)\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Print the shape, data type, mean, std deviation, and first 5 readings\n",
    "print(\"Temperature Sensor Data\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {temperature_celsius.___}\")  # Get the shape attribute\n",
    "print(f\"Data type: {temperature_celsius.___}\")  # Get the dtype attribute\n",
    "print(f\"Mean temperature: {temperature_celsius.___():.2f} °C\")  # Calculate mean\n",
    "print(f\"Std deviation: {temperature_celsius.___():.2f} °C\")  # Calculate standard deviation\n",
    "print(f\"First 5 readings: {temperature_celsius[___]}\")  # Slice first 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: 2D grid (e.g., spatial temperature distribution)\n",
    "grid_size = 50\n",
    "x = np.linspace(-5, 5, grid_size)\n",
    "y = np.linspace(-5, 5, grid_size)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Simulate a Gaussian temperature distribution\n",
    "temperature_2d = 100 * np.exp(-(X**2 + Y**2) / 10)\n",
    "\n",
    "print(\"\\n2D Temperature Field (Heat Source)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {temperature_2d.shape}\")\n",
    "print(f\"Max temperature: {temperature_2d.max():.2f} °C\")\n",
    "print(f\"Min temperature: {temperature_2d.min():.2f} °C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 3: 3D volumetric data (e.g., MRI scan, simulation output)\n",
    "volume_data = np.random.randn(10, 10, 10)  # Small 3D volume\n",
    "print(\"\\n3D Volumetric Data\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {volume_data.shape} (depth × height × width)\")\n",
    "print(f\"Total elements: {volume_data.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising previous data using Plotly\n",
    "# 1D Line Plot for Temperature Sensor Readings\n",
    "fig1 = px.line(x=time_hours, y=temperature_celsius, title=\"Temperature Sensor Readings Over Time\",\n",
    "               labels={\"x\": \"Time (hours)\", \"y\": \"Temperature (°C)\"})\n",
    "fig1.show() \n",
    "\n",
    "# 2D Heatmap for Temperature Field\n",
    "fig2 = px.imshow(temperature_2d, title=\"2D Temperature Field (Heat Source)\",\n",
    "                 labels={\"x\": \"X-axis\", \"y\": \"Y-axis\", \"color\": \"Temperature (°C)\"})\n",
    "fig2.show()\n",
    "\n",
    "# 3D Surface Plot for Volumetric Data\n",
    "fig3 = go.Figure(data=[go.Surface(z=volume_data[5, :, :])])\n",
    "fig3.update_layout(title='3D Volumetric Data (Slice at depth=5)',\n",
    "                   scene=dict(xaxis_title='X-axis', yaxis_title='Y-axis', zaxis_title='Value'))\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pandas: DataFrames for Tabular Data\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: Pandas DataFrames are 2D labeled data structures with columns of potentially different types. They're like spreadsheets or SQL tables in Python.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/>  **Common Use Cases**:\n",
    "- Experimental measurements with metadata (e.g., sample ID, conditions, measurements)\n",
    "- Time series with multiple sensors\n",
    "- Results from parameter sweeps\n",
    "- Loading CSV/Excel files from lab equipment\n",
    "\n",
    "Let's create a sample experimental dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample experimental dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "experimental_data = pd.DataFrame({\n",
    "    'sample_id': [f'S{i:03d}' for i in range(n_samples)],\n",
    "    'concentration_mM': np.random.uniform(0.1, 10.0, n_samples),\n",
    "    'temperature_C': np.random.normal(25, 2, n_samples),\n",
    "    'pressure_bar': np.random.normal(1.0, 0.05, n_samples),\n",
    "    'reaction_rate': np.random.lognormal(0, 0.5, n_samples)\n",
    "})\n",
    "\n",
    "print(\"Experimental Dataset Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples: {len(experimental_data)}\")\n",
    "print(f\"Columns: {list(experimental_data.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "experimental_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "experimental_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing data: convert to NumPy arrays\n",
    "concentrations = experimental_data['concentration_mM'].values\n",
    "print(f\"\\nExtracted concentrations shape: {concentrations.shape}\")\n",
    "print(f\"Data type: {concentrations.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 3. PyTorch Tensors: NumPy Arrays on Steroids\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/>  **Definition**: A **tensor** is PyTorch's core data structure. Essentially it can be boiled down to a multidimensional array (like NumPy's `ndarray`) that can live on the GPU and track gradients for automatic differentiation.\n",
    "\n",
    "**Think of tensors as**: NumPy arrays + GPU support + automatic differentiation\n",
    "\n",
    "## Why Tensors Matter for Scientists\n",
    "\n",
    "1. **Hardware Flexibility**: Move computations between CPU and GPU with one line\n",
    "2. **Performance**: GPU acceleration for large-scale computations (matrix operations, FFTs, etc.)\n",
    "3. **Gradient Computation**: Essential for optimization, inverse problems, and machine learning\n",
    "4. **Seamless Integration**: Convert to/from NumPy and Pandas easily\n",
    "\n",
    "## Tensor Terminology\n",
    "\n",
    "Similar to arrays, tensors have different **ranks** (number of dimensions):\n",
    "\n",
    "- **Scalar** (rank 0): A single number, e.g., temperature = 273.15\n",
    "- **Vector** (rank 1): 1D array, e.g., time series `[t₀, t₁, t₂, ...]`\n",
    "- **Matrix** (rank 2): 2D array, e.g., grayscale image or grid data\n",
    "- **3D Tensor** (rank 3): e.g., RGB image (height × width × channels) or volumetric data\n",
    "- **nD Tensor** (rank n): Higher-dimensional data (e.g., batch of 3D volumes)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/tensors.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "## 3.1 Creating Tensors\n",
    "***\n",
    "\n",
    "Let's create tensors using `torch.tensor()`. \n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: For details, see the [PyTorch Tensor Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) and [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating scalar tensors (rank 0)\n",
    "# Scalars represent single values like temperature, pressure, or energy\n",
    "\n",
    "###################\n",
    "# TODO: Create an integer scalar tensor with value 42\n",
    "measurement_count = ___  # COMPLETE: Pass the integer 42\n",
    "print(f\"Measurement count: {measurement_count}\")\n",
    "print(f\"  Type: {type(measurement_count)}\")\n",
    "print(f\"  Shape: {measurement_count.shape}\")\n",
    "print(f\"  Data type: {measurement_count.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float scalar (common for physical measurements)\n",
    "###################\n",
    "# TODO: Create a float scalar tensor with value 273.15\n",
    "temperature_kelvin = ___  # COMPLETE: Pass the float 273.15\n",
    "print(f\"Temperature: {temperature_kelvin} K\")\n",
    "print(f\"  Shape: {temperature_kelvin.shape}\")\n",
    "print(f\"  Data type: {temperature_kelvin.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Python value\n",
    "###################\n",
    "# TODO: Extract the Python value from the tensor using .item()\n",
    "print(f\"\\nExtracted value: {temperature_kelvin.___} (type: {type(temperature_kelvin.___)})\")  # COMPLETE: Use .item() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Create your own scientific measurements\n",
    "\n",
    "# Create a scalar for pressure in atmospheres\n",
    "pressure_atm = torch.tensor(1.013)\n",
    "\n",
    "# Create a scalar for Avogadro's number (as float32 for efficiency)\n",
    "avogadro_approx = torch.tensor(6.022e23, dtype=torch.float32)\n",
    "\n",
    "print(\"Your Scientific Constants:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Pressure: {pressure_atm.item():.3f} atm\")\n",
    "print(f\"  Shape: {pressure_atm.shape}, Dtype: {pressure_atm.dtype}\")\n",
    "print(f\"\\nAvogadro's number: {avogadro_approx.item():.3e}\")\n",
    "print(f\"  Shape: {avogadro_approx.shape}, Dtype: {avogadro_approx.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we created a scalar tensor with a single element. Looking at its attributes, we can see that the tensor has a shape of `torch.Size([])`, which means that it has no dimensions. We can also see that the tensor has a data type of `torch.int64`, which means that it is an integer tensor.\n",
    "\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Note**: The data type of a tensor is determined by the data type of the elements that it contains. It is important to be aware of the data type of a tensor, as it can affect the results of operations that are performed on it. Good practice is to always specify the data type of a tensor when creating it.\n",
    "\n",
    "\n",
    "As we can see our single element is now stored in a type of container, which means that we can perform operations on it but not directly on the element itself. To access the element, we can use the method `item()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the data type of a tensor by passing the `dtype` argument to the `torch.Tensor` constructor. Alternatively, we can use the 'torch.tensor.type` method to change the data type of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: Practice changing tensor data types\n",
    "# Create a scalar tensor with a specific data type\n",
    "scalar_tensor = torch.tensor(42, dtype=torch.___)  # COMPLETE: Use torch.float32\n",
    "print(scalar_tensor)\n",
    "\n",
    "# Change the data type of a tensor\n",
    "scalar_tensor = scalar_tensor.type(torch.___)  # COMPLETE: Use torch.int64\n",
    "print(scalar_tensor)\n",
    "\n",
    "# Another way to change the data type of a tensor\n",
    "scalar_tensor = scalar_tensor.___()  # COMPLETE: Use .int() method\n",
    "print(scalar_tensor)\n",
    "\n",
    "# # Not recommended as it can be confusing \n",
    "# with the .to() method that is used to move tensors\n",
    "# to different devices\n",
    "scalar_tensor = scalar_tensor.to(torch.___) # COMPLETE: Use torch.float64\n",
    "print(scalar_tensor) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Initializing tensors\n",
    "***\n",
    "\n",
    "PyTorch provides multiple ways to initialize tensors. Sometimes, we want to create a tensor with specific values, while other times we want to create a tensor with random values. PyTorch provides several functions for creating tensors with different initializations. \n",
    "\n",
    "Below is a table summarizing some of the most commonly used tensor creation functions in PyTorch.\n",
    "\n",
    "| Function | Description | Example | Output Shape |\n",
    "|----------|-------------|---------|--------------|\n",
    "| `torch.tensor()` | Creates tensor from data | `torch.tensor([1, 2, 3])` | `(3,)` |\n",
    "| `torch.zeros()` | Creates tensor of zeros | `torch.zeros(2, 3)` | `(2, 3)` |\n",
    "| `torch.ones()` | Creates tensor of ones | `torch.ones(2, 3)` | `(2, 3)` |\n",
    "| `torch.rand()` | Uniform random [0, 1] | `torch.rand(2, 3)` | `(2, 3)` |\n",
    "| `torch.randn()` | Normal distribution μ=0, σ=1 | `torch.randn(2, 3)` | `(2, 3)` |\n",
    "| `torch.arange()` | Integer sequence | `torch.arange(5)` | `(5,)` |\n",
    "| `torch.linspace()` | Evenly spaced sequence | `torch.linspace(0, 1, 5)` | `(5,)` |\n",
    "| `torch.eye()` | Identity matrix | `torch.eye(3)` | `(3, 3)` |\n",
    "| `torch.randint()` | Random integers | `torch.randint(0, 10, (2, 3))` | `(2, 3)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common tensor creation methods for scientific computing\n",
    "\n",
    "print(\"Tensor Initialization Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. From a list (e.g., measurement data)\n",
    "time_points = torch.tensor([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "print(f\"Time points: {time_points}\")\n",
    "print(f\"  Shape: {time_points.shape}\\n\")\n",
    "\n",
    "###################\n",
    "# TODO: Complete the tensor initialization methods\n",
    "# 2. Zeros (useful for initializing arrays)\n",
    "grid_2d = torch.___(3, 3)  # COMPLETE: Create a 3x3 tensor of zeros\n",
    "print(f\"Zeros (3×3 grid):\\n{grid_2d}\\n\")\n",
    "\n",
    "# 3. Ones (useful for masks or initial conditions)\n",
    "mask = torch.___(2, 4)  # COMPLETE: Create a 2x4 tensor of ones\n",
    "print(f\"Ones (2×4 mask):\\n{mask}\\n\")\n",
    "\n",
    "# 4. Random values (for Monte Carlo simulations)\n",
    "random_samples = torch.___(3, 3)  # COMPLETE: Create 3x3 random uniform [0,1]\n",
    "print(f\"Random uniform [0,1] (3×3):\\n{random_samples}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: Complete more tensor initialization methods\n",
    "# 5. Random normal (Gaussian noise)\n",
    "noise = torch.___(5)  # COMPLETE: Create random normal distribution, Mean=0, Std=1\n",
    "print(f\"Gaussian noise: {noise}\\n\")\n",
    "\n",
    "# 6. Sequences (like np.arange)\n",
    "indices = torch.___(0, 10, 2)  # COMPLETE: Create sequence from 0 to 10 with step 2\n",
    "print(f\"Sequence (0 to 10, step 2): {indices}\\n\")\n",
    "\n",
    "# 7. Linearly spaced (like np.linspace)\n",
    "wavelengths = torch.___(400, 700, 7)  # COMPLETE: Create 7 evenly spaced values from 400 to 700\n",
    "print(f\"Wavelengths (400-700 nm): {wavelengths}\\n\")\n",
    "\n",
    "# 8. Identity matrix (for linear algebra)\n",
    "identity_3x3 = torch.___(3)  # COMPLETE: Create 3x3 identity matrix\n",
    "print(f\"Identity matrix (3×3):\\n{identity_3x3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Indexing tensors\n",
    "***\n",
    "Indexing tensors is similar to indexing arrays in Python. We can use square brackets `[]` to access elements in a tensor. This is useful for extracting specific elements or slices of a tensor. Below is a table summarizing the different ways to index tensors in PyTorch.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/>  **Tips**:\n",
    "> - Use `:` to select all elements in a dimension\n",
    "> - Use negative indices to count from the end: -1 is last element\n",
    "> - Ellipsis (`...`) represents multiple full slices\n",
    "> - Step values can be negative for reverse order\n",
    "> - Boolean masks must match tensor dimensions\n",
    "\n",
    "| Method | Syntax | Description | Example | Result |\n",
    "|--------|--------|-------------|---------|---------|\n",
    "| Basic Indexing | `tensor[ix,jx]` | Access single element | `t[0,1]` | Element at row 0, col 1 |\n",
    "| Slicing | `tensor[start:end]` | Extract subset | `t[1:3]` | Elements from index 1 to 2 |\n",
    "| Striding | `tensor[::step]` | Extract with step | `t[::2]` | Every second element |\n",
    "| Negative Indexing | `tensor[-1]` | Count from end | `t[-1]` | Last element |\n",
    "| Boolean Indexing | `tensor[mask]` | Filter with condition | `t[t > 0]` | Elements > 0 |\n",
    "| Ellipsis | `tensor[...]` | All dimensions | `t[...,0]` | All dims except last |\n",
    "| Combined | `tensor[1:3,...,::2]` | Mix methods | `t[1:3,...,0]` | Complex selection |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 1**: Indexing a tensor\n",
    "\n",
    "```python\n",
    "# Get corners of a matrix\n",
    "corners = tensor[...,[0,-1]]  # First and last elements of last dimension\n",
    "\n",
    "# Get last row of a matrix\n",
    "last_row = tensor[-1,...]  # Last row of all columns\n",
    "\n",
    "# Extract diagonal\n",
    "diagonal = tensor.diagonal()  # More efficient than indexing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample 4×4 data matrix (e.g., sensor grid readings)\n",
    "sensor_data = torch.tensor([\n",
    "    [20.1, 20.5, 21.0, 21.5],\n",
    "    [19.8, 20.2, 20.8, 21.2],\n",
    "    [19.5, 19.9, 20.5, 21.0],\n",
    "    [19.2, 19.6, 20.2, 20.8]\n",
    "])\n",
    "\n",
    "print(\"Sensor Grid Data (4×4): Temperature in °C\")\n",
    "print(sensor_data)\n",
    "print(\"\\nIndexing Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "###################\n",
    "# TODO: Complete the indexing operations\n",
    "# 1. Single element\n",
    "value = sensor_data[___, ___]  # COMPLETE: Get element at row 2, column 3\n",
    "print(f\"Sensor at position (2,3): {value.item():.1f} °C\\n\")\n",
    "\n",
    "# 2. Entire row (all sensors in one row)\n",
    "second_row = sensor_data[___, ___]  # COMPLETE: Get row 1, all columns (use :)\n",
    "print(f\"Second row (all columns): {second_row}\\n\")\n",
    "\n",
    "# 3. Entire column (one sensor over all rows)\n",
    "last_column = sensor_data[___, ___]  # COMPLETE: Get all rows, last column (column 3)\n",
    "print(f\"Last column (all rows): {last_column}\\n\")\n",
    "\n",
    "# 4. Submatrix (bottom-right 2×2 region)\n",
    "bottom_right = sensor_data[___:, ___:]  # COMPLETE: Slice from row 2 to end, column 2 to end\n",
    "print(f\"Bottom-right 2×2:\\n{bottom_right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sensor Grid Data (4×4): Temperature in °C\")\n",
    "print(sensor_data)\n",
    "print(\"\\nIndexing Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "###################\n",
    "# TODO: Complete advanced indexing operations\n",
    "# 5. Striding (every other element in first row)\n",
    "every_other = sensor_data[0, ___]  # COMPLETE: Get every 2nd element (use ::2)\n",
    "print(f\"First row, every other element: {every_other}\\n\")\n",
    "\n",
    "# 6. Boolean masking (find hot spots > 20.5°C)\n",
    "hot_spots = sensor_data ___ 20.5  # COMPLETE: Create boolean mask (use > operator)\n",
    "print(f\"Hot spots (> 20.5°C):\\n{hot_spots}\")\n",
    "print(f\"\\nValues at hot spots: {sensor_data[___]}\")  # COMPLETE: Apply mask\n",
    "\n",
    "# 7. Negative indexing (last element)\n",
    "last_element = sensor_data[___, ___]  # COMPLETE: Get last row, last column (use -1)\n",
    "print(f\"\\nLast element (bottom-right): {last_element.item():.1f} °C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 4. Tensor operations\n",
    "***\n",
    "\n",
    "PyTorch allows us to manipulate tensors in different ways. Since PyTorch is built on top of NumPy, the same operations can be accessed through the `torch` module or alternatively through the `numpy` module. Due to the pythonic nature of PyTorch, we can also use the same operations as we would in Python.\n",
    "\n",
    "### Basic Operations Cheatsheet\n",
    "\n",
    "| Category | Description | Methods | PyTorch Method | Example |\n",
    "|----------|-------------|----------|----------------|---------|\n",
    "| Arithmetic | Basic math operations | +, -, *, /, ** | `add(), sub(), mul(), div(), pow(), sqrt()` | `a + b` |\n",
    "| Comparison | Compare values | >, <, ==, != | `gt(), lt(), eq(), ne()` | `a > 0` |\n",
    "| Reduction | Reduce dimensions | `sum(), mean(), max()` | `sum(), mean(), max()` | `a.sum()` |\n",
    "| Statistical | Statistical operations | `std(), var()` | `std(), var()` | `a.mean()` |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> 1. **Type Matching**: Ensure tensors have compatible data types\n",
    "> 2. **Shape Broadcasting**: Understand how PyTorch broadcasts shapes\n",
    "> 3. **GPU Memory**: Be careful with large tensor operations on GPU\n",
    "> 4. **Inplace Operations**: Use `_` suffix for inplace operations\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\" style=\"filter: invert(19%) sepia(75%) saturate(6158%) hue-rotate(312deg) brightness(87%) contrast(116%);\"/> **Common Mistakes to Avoid**: \n",
    "> - Mixing tensor types without conversion\n",
    "> - Forgetting to handle device placement (CPU/GPU)\n",
    "> - Not checking tensor shapes before operations\n",
    "> - Unnecessary copying of large tensors\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 2**: Inplace operations\n",
    "   ```python\n",
    "   # Instead of: x = x + 1\n",
    "   x.add_(1)  # Inplace addition\n",
    "   y.add_(x)  # Inplace addition with another tensor\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor operations for scientific computing\n",
    "\n",
    "# Example: Two sets of experimental measurements\n",
    "experiment_A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "experiment_B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(\"Tensor Operations Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment A:\\n{experiment_A}\\n\")\n",
    "print(f\"Experiment B:\\n{experiment_B}\\n\")\n",
    "\n",
    "###################\n",
    "# TODO: Complete the tensor operations\n",
    "# Element-wise addition\n",
    "combined = experiment_A ___ experiment_B  # COMPLETE: Add the two tensors\n",
    "print(f\"A + B (element-wise):\\n{combined}\\n\")\n",
    "\n",
    "# Element-wise multiplication (Hadamard product)\n",
    "product = experiment_A ___ experiment_B  # COMPLETE: Multiply element-wise\n",
    "print(f\"A * B (element-wise):\\n{product}\\n\")\n",
    "\n",
    "# Matrix multiplication (linear transformations)\n",
    "matmul_result = experiment_A ___ experiment_B  # COMPLETE: Matrix multiply (use @ operator)\n",
    "print(f\"A @ B (matrix multiplication):\\n{matmul_result}\\n\")\n",
    "\n",
    "# Square root (useful for RMS calculations)\n",
    "sqrt_A = torch.___(experiment_A)  # COMPLETE: Use torch.sqrt()\n",
    "print(f\"√A:\\n{sqrt_A}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: Complete the statistical operations\n",
    "# Statistical operations\n",
    "print(f\"Mean of A: {experiment_A.___().item():.2f}\")  # COMPLETE: Calculate mean\n",
    "print(f\"Standard deviation of A: {experiment_A.___().item():.2f}\")  # COMPLETE: Calculate std\n",
    "print(f\"Sum along rows: {experiment_A.___(dim=1)}\")  # COMPLETE: Sum along dimension 1\n",
    "print(f\"Max along columns: {experiment_A.___(dim=0).values}\")  # COMPLETE: Max along dimension 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Matrix operations\n",
    "***\n",
    "Matrix multiplication is a common operation in algebra and is used in many machine learning algorithms. We can perform:\n",
    "\n",
    "- **Matrix multiplication**: This is the standard matrix multiplication operation, which is denoted by the `@` operator in Python. This operation is also known as the dot product.\n",
    "- **Element-wise multiplication**: This is the multiplication of two matrices of the same shape, which is denoted by the `*` operator in Python. This operation is also known as the Hadamard product.\n",
    "- **Matrix transpose**: This is the operation of flipping a matrix over its diagonal, which is denoted by the `.T` attribute in Python. This operation is also known as the matrix transpose.\n",
    "- **Matrix inverse**: This is the operation of finding the inverse of a matrix, which is denoted by the `torch.inverse()` function in Python. This operation is also known as the matrix inverse.\n",
    "\n",
    "***\n",
    "| Operation | Description | Method | Example |\n",
    "|-----------|-------------|--------|---------|\n",
    "| Matrix Multiplication | Standard matrix product | @ or matmul() | `a @ b` |\n",
    "| Transpose | Flip matrix dimensions | .T or transpose() | `a.T` |\n",
    "| Inverse | Matrix inverse | inverse() | `torch.inverse(a)` |\n",
    "| Determinant | Matrix determinant | det() | `torch.det(a)` |\n",
    "| Eigenvalues | Eigenvalues and vectors | eig() | `torch.eig(a)` |\n",
    "| Singular Value Decomposition | SVD decomposition | svd() | `torch.svd(a)` |\n",
    "| Cholesky Decomposition | Cholesky factorization | cholesky() | `torch.cholesky(a)` |\n",
    "\n",
    "***\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/matrix_mul.gif\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations for linear algebra (common in scientific computing)\n",
    "\n",
    "# Example: Covariance matrix from experimental data\n",
    "covariance_matrix = torch.tensor([[4.0, 2.0], [2.0, 3.0]], dtype=torch.float32)\n",
    "\n",
    "print(\"Matrix Operations for Linear Algebra\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Covariance Matrix:\\n{covariance_matrix}\\n\")\n",
    "\n",
    "###################\n",
    "# TODO: Complete the matrix operations\n",
    "# Matrix transpose\n",
    "transposed = covariance_matrix.___  # COMPLETE: Get transpose (use .T attribute)\n",
    "print(f\"Transposed:\\n{transposed}\\n\")\n",
    "\n",
    "# Matrix determinant (measure of volume/invertibility)\n",
    "det = torch.___(covariance_matrix)  # COMPLETE: Calculate determinant\n",
    "print(f\"Determinant: {det.item():.4f}\")\n",
    "\n",
    "# Matrix inverse (for solving linear systems)\n",
    "inverse = torch.___(covariance_matrix)  # COMPLETE: Calculate inverse\n",
    "print(f\"Inverse:\\n{inverse}\\n\")\n",
    "\n",
    "# Verify: A @ A^(-1) = I\n",
    "identity_check = covariance_matrix ___ inverse  # COMPLETE: Matrix multiply (use @)\n",
    "print(f\"A @ A^(-1) ≈ I:\\n{identity_check}\\n\")\n",
    "\n",
    "# Matrix-matrix multiplication (transformations)\n",
    "transformation = covariance_matrix @ covariance_matrix\n",
    "print(f\"A @ A:\\n{transformation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tensor Broadcasting\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: [Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) is a powerful feature of NumPy and PyTorch that allows us to perform operations on arrays of different shapes without having to explicitly reshape them. \n",
    "\n",
    "Since PyTorch is built on top of NumPy we can use its broadcasting capabilities. Broadcasting is how NumPy handles arrays with different shapes during arithmetic operations. It allows us to perform operations on arrays of different shapes without having to explicitly reshape them. This is done by automatically expanding the smaller array to match the shape of the larger array.\n",
    "\n",
    "For example, if we have a 1D array of shape `(3,)` and a 2D array of shape `(3, 2)`, we can add them together without having to reshape the 1D array. NumPy will automatically expand the 1D array to match the shape of the 2D array.\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 3**: Broadcasting example\n",
    "\n",
    "```python\n",
    "# Create a 1D tensor of shape (3,)\n",
    "a = torch.tensor([1, 2, 3])\n",
    "# Create a 2D tensor of shape (3, 2)\n",
    "b = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "# Add the two tensors together\n",
    "c = a + b  # Broadcasting occurs here\n",
    "print(c)  # Output: tensor([[ 2,  4], [ 6,  8], [10, 12]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting: Automatic shape alignment for operations\n",
    "\n",
    "# Example: Normalizing sensor data\n",
    "raw_data = torch.tensor([[10.0, 20.0], [30.0, 40.0]])\n",
    "offset = torch.tensor([5.0])  # Scalar to subtract\n",
    "scale = torch.tensor([1.0, 2.0])  # Per-column scaling\n",
    "\n",
    "print(\"Broadcasting Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Raw sensor data:\\n{raw_data}\\n\")\n",
    "\n",
    "# Broadcast scalar to matrix (subtract offset from all elements)\n",
    "centered = raw_data - offset\n",
    "print(f\"After subtracting offset {offset.item()}:\\n{centered}\\n\")\n",
    "\n",
    "# Broadcast row vector to matrix (per-column scaling)\n",
    "scaled = raw_data * scale\n",
    "print(f\"After per-column scaling {scale}:\\n{scaled}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Broadcasting Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Raw sensor data:\\n{raw_data}\\n\")\n",
    "\n",
    "###################\n",
    "# TODO: Complete the normalization using broadcasting\n",
    "# More complex: normalize each column (mean=0, std=1)\n",
    "mean_per_column = ___  # COMPLETE: Compute mean along rows (dim=0)\n",
    "std_per_column = ___  # COMPLETE: Compute std along rows (dim=0)\n",
    "normalized =  ___  # COMPLETE: Normalize (subtract mean, divide by std)\n",
    "\n",
    "print(f\"Column means: {mean_per_column}\")\n",
    "print(f\"\\nColumn stds: {std_per_column}\")\n",
    "print(f\"\\nNormalized data:\\n{normalized}\\n\")\n",
    "\n",
    "# Verify normalization\n",
    "print(f\"\\nNormalized means: {normalized.mean(dim=0)}\")\n",
    "print(f\"\\nNormalized stds: {normalized.std(dim=0, unbiased=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Reshaping Methods\n",
    "***\n",
    "Sometimes, we need to change the shape of a tensor without changing its data. We do this in order to prepare the tensor for a specific operation or to match the shape of another tensor. PyTorch provides several methods for reshaping tensors. Below is a table summarizing some of the most commonly used reshaping methods in PyTorch.\n",
    "\n",
    "| Method | Description | Example | Note |\n",
    "|--------|-------------|---------|------|\n",
    "| `reshape()` | New shape, maybe new memory | `x.reshape(2,3)` | May copy data |\n",
    "| `view()` | New shape, same memory | `x.view(2,3)` | Must be contiguous |\n",
    "| `squeeze()` | Remove single dims | `x.squeeze()` | Removes size 1 dims |\n",
    "| `unsqueeze()` | Add single dim | `x.unsqueeze(0)` | Adds size 1 dim |\n",
    "| `expand()` | Broadcast dimensions | `x.expand(2,3)` | No data copy |\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 4**: Reshaping a tensor\n",
    "\n",
    "```python\n",
    "# Create a 1D tensor of shape (6,)\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "# Reshape to (2, 3)\n",
    "print(x.reshape(2, 3))  # Output: tensor([[1, 2, 3], [4, 5, 6]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping tensors: crucial for data preparation\n",
    "\n",
    "# Example: 1D time series that needs to be reshaped for processing\n",
    "time_series = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"Reshaping Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original time series: {time_series}, shape: {time_series.shape}\\n\")\n",
    "\n",
    "###################\n",
    "# TODO: Complete the reshaping operations\n",
    "# Reshape to 2D (e.g., for batch processing)\n",
    "reshaped_2x3 = time_series.___(2, 3)  # COMPLETE: Reshape to 2x3\n",
    "print(f\"Reshaped to 2×3:\\n{reshaped_2x3}\\n\")\n",
    "\n",
    "# Reshape to 3D (e.g., adding batch dimension)\n",
    "reshaped_3d = time_series.reshape(___, ___, ___)  # COMPLETE: Reshape to 1x2x3\n",
    "print(f\"Reshaped to 1×2×3:\\n{reshaped_3d}\\n\")\n",
    "\n",
    "# Use -1 to infer one dimension\n",
    "auto_reshape = time_series.reshape(3, ___)  # COMPLETE: Use -1 to auto-infer size\n",
    "print(f\"Reshape with -1 (3, -1):\\n{auto_reshape}\\n\")\n",
    "\n",
    "# Unsqueeze: Add a dimension (useful for broadcasting)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "column_vector = vector.___(1)  # COMPLETE: Add dimension at position 1\n",
    "print(f\"Original vector: {vector.shape}\")\n",
    "print(f\"Column vector: {column_vector.shape}\")\n",
    "print(f\"Column vector:\\n{column_vector}\\n\")\n",
    "\n",
    "# Squeeze: Remove dimensions of size 1\n",
    "squeezed = column_vector.___()  # COMPLETE: Remove size-1 dimensions\n",
    "print(f\"After squeeze: {squeezed.shape}, {squeezed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 5. Automatic Differentiation (Autograd)\n",
    "***\n",
    "\n",
    "Automatic differentiation is one of the most powerful features of PyTorch. It allows us to compute gradients automatically, which is essential for training neural networks. PyTorch uses a technique called **reverse mode differentiation** to compute gradients efficiently. This technique is based on the chain rule of calculus and allows us to compute gradients for complex functions with many variables.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: [Autograd](https://pytorch.org/docs/stable/autograd.html) is the automatic differentiation engine in PyTorch. It provides a way to compute gradients automatically for tensors with `requires_grad=True`.\n",
    "\n",
    "Take for instance the following function:\n",
    "\n",
    "$$f(x) = x^2 + 42y^2 + 3$$\n",
    "\n",
    "where $x$ and $y$ are tensors. The gradient of this function with respect to $x$ and $y$ is given by:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = 2x$$\n",
    "$$\\frac{\\partial f}{\\partial y} = 84y$$\n",
    "\n",
    "using the chain rule of calculus. PyTorch allows us to compute these gradients automatically using the `backward()` method.\n",
    "\n",
    "The chain rule is a fundamental concept in calculus that allows us to compute the derivative of a composite function. It states that if we have two functions $f(x)$ and $g(x)$, then the derivative of their composition $f(g(x))$ is given by:\n",
    "\n",
    "$$\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "where $f'(g(x))$ is the derivative of $f$ with respect to $g$, and $g'(x)$ is the derivative of $g$ with respect to $x$. This means that we can compute the derivative of a composite function by computing the derivatives of its constituent functions and multiplying them together.\n",
    "\n",
    "## Autograd Concepts\n",
    "***\n",
    "\n",
    "In PyTorch, the autograd engine keeps track of all operations performed on tensors with `requires_grad=True`. It builds a computation graph dynamically as operations are performed. This graph is used to compute gradients when we call the `backward()` method.\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|---------|\n",
    "| `requires_grad` | Flag to track gradients | `x = torch.tensor(1.0, requires_grad=True)` |\n",
    "| `backward()` | Compute gradients | `y.backward()` |\n",
    "| `grad` | Access gradients | `x.grad` |\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 5**: Using autograd to compute gradients\n",
    "\n",
    "```python\n",
    "# Create tensor with gradient tracking\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Compute function\n",
    "y = x * x\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "\n",
    "# Access gradient\n",
    "x.grad  # Should be 2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: Complete the automatic differentiation example\n",
    "# Automatic differentiation: Essential for optimization and inverse problems\n",
    "\n",
    "# Example: Optimize a physical parameter\n",
    "# Suppose we have y = 3x³ + 2x² - 5x + 1\n",
    "# We want to find dy/dx at x = 2.0\n",
    "\n",
    "x = torch.tensor([2.0], ___)  # COMPLETE: Enable gradient tracking (use True)\n",
    "\n",
    "# Compute function (e.g., energy as a function of position)\n",
    "y = ___\n",
    "\n",
    "print(\"Automatic Differentiation Example\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Function: y = 3x³ + 2x² - 5x + 1\")\n",
    "print(f\"At x = {x.item():.1f}:\")\n",
    "print(f\"  y = {y.item():.2f}\")\n",
    "\n",
    "# Compute gradient automatically\n",
    "y.___()  # COMPLETE: Call backward() to compute gradients\n",
    "\n",
    "print(f\"  dy/dx = {x.___.item():.2f}\")  # COMPLETE: Access gradient with .grad\n",
    "print(\"\\nAnalytical verification:\")\n",
    "print(f\"  dy/dx = 9x² + 4x - 5\")\n",
    "print(f\"  At x = 2: 9(4) + 4(2) - 5 = 36 + 8 - 5 = 39 ✓\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 6. Converting Between NumPy and PyTorch\n",
    "***\n",
    "\n",
    "One of PyTorch's greatest strengths is seamless interoperability with NumPy. This allows you to:\n",
    "- Use existing NumPy-based code and data\n",
    "- Leverage PyTorch's GPU acceleration\n",
    "- Easily integrate with the scientific Python ecosystem (SciPy, Pandas, Matplotlib, etc.)\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Important**: NumPy arrays and PyTorch tensors can **share memory** on CPU, meaning changes to one affect the other (unless you explicitly copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting between NumPy and PyTorch\n",
    "\n",
    "print(\"NumPy ↔ PyTorch Conversion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "###################\n",
    "# TODO: Complete the NumPy-PyTorch conversions\n",
    "# 1. NumPy to PyTorch\n",
    "# Use the temperature data from our earlier NumPy example\n",
    "numpy_temps = np.array([20.5, 21.0, 19.8, 22.1, 20.3])\n",
    "torch_temps = torch.___(numpy_temps)  # COMPLETE: Convert from NumPy to PyTorch\n",
    "\n",
    "print(\"NumPy → PyTorch:\")\n",
    "print(f\"  NumPy array: {numpy_temps}\")\n",
    "print(f\"  PyTorch tensor: {torch_temps}\")\n",
    "print(f\"  Tensor dtype: {torch_temps.dtype}\\n\")\n",
    "\n",
    "# 2. PyTorch to NumPy\n",
    "torch_data = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "numpy_data = torch_data.___()  # COMPLETE: Convert from PyTorch to NumPy\n",
    "\n",
    "print(\"PyTorch → NumPy:\")\n",
    "print(f\"  PyTorch tensor: {torch_data}\")\n",
    "print(f\"  NumPy array: {numpy_data}\")\n",
    "print(f\"  Array dtype: {numpy_data.dtype}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Memory sharing demonstration\n",
    "print(\"Memory Sharing:\")\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"Original NumPy: {np_array}\")\n",
    "print(f\"PyTorch tensor: {torch_tensor}\")\n",
    "\n",
    "# Modify NumPy array\n",
    "np_array[0] = 999.0\n",
    "print(f\"\\nAfter modifying NumPy[0] = 999:\")\n",
    "print(f\"  NumPy: {np_array}\")\n",
    "print(f\"  PyTorch: {torch_tensor} (changed too!)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 7. Working with Scientific Datasets: Higgs Boson\n",
    "***\n",
    "\n",
    "Now let's apply what we've learned to a real scientific dataset. We'll use the [Higgs Boson Dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS) from the UCI Machine Learning Repository.\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "The dataset contains 11 million Monte Carlo-generated collision events from the CERN Large Hadron Collider. Each event has:\n",
    "- **31 features**: 21 low-level kinematic properties and 7 high-level features derived by physicists\n",
    "- **1 label**: Binary classification (signal or background)\n",
    "\n",
    "**Scientific Context**: The Higgs boson is a fundamental particle discovered at CERN in 2012. Identifying Higgs events among background noise is a classic classification problem in high-energy physics.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Note**: We'll load only the first 10,000 rows for this workshop to keep things fast.\n",
    "\n",
    "## 7.1 Loading the Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Higgs Boson dataset\n",
    "# This may take a moment as we're downloading from UCI repository\n",
    "\n",
    "url = \"https://github.com/CLDiego/uom_fse_dl_workshop/raw/refs/heads/main/training_data/HIGGS.zip\"\n",
    "\n",
    "print(\"Loading Higgs Boson Dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Default column names (used if download fails)\n",
    "column_names = ['label'] + [f'feature_{i}' for i in range(1, 29)]\n",
    "\n",
    "# Load first 10,000 rows using the first row as header\n",
    "try:\n",
    "    higgs_df = pd.read_csv(url, nrows=10000, compression='zip', header=0)\n",
    "    column_names = list(higgs_df.columns)\n",
    "\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Falling back to sample data...\")\n",
    "    # Create sample data if download fails\n",
    "    higgs_df = pd.DataFrame(\n",
    "        np.random.randn(10000, 29),\n",
    "        columns=column_names\n",
    "    )\n",
    "    higgs_df['label'] = np.random.randint(0, 2, 10000)\n",
    "\n",
    "print(f\"\\nDataset Shape: {higgs_df.shape}\")\n",
    "print(f\"  Samples: {len(higgs_df)}\")\n",
    "print(f\"  Features: {len(higgs_df.columns) - 1}\")\n",
    "print(f\"  Labels: {higgs_df['Label'].nunique()} classes\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(higgs_df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "\n",
    "Let's examine the structure and statistics of our particle physics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "higgs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary (first 5 features):\")\n",
    "print(\"=\" * 60)\n",
    "print(higgs_df.iloc[:, :6].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Converting Data to Tensors\n",
    "\n",
    "To use PyTorch, we need to convert our Pandas DataFrame to tensors. We'll separate:\n",
    "- **Features (X)**: The kinematic measurements\n",
    "- **Labels (y)**: The binary class (signal=1, background=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: Convert the dataset to PyTorch tensors\n",
    "# Separate features and labels\n",
    "X = higgs_df.drop('Label', axis=1).values  # All features as NumPy array\n",
    "y = higgs_df['Label'].values  # Labels as NumPy array\n",
    "\n",
    "# Convert labels from b and s to 0 and 1 \n",
    "y = np.where(y == '___', 1, 0)  # COMPLETE: Replace 's' with the signal label\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.___(X).float()  # COMPLETE: Convert NumPy to PyTorch\n",
    "y_tensor = torch.from_numpy(y).___()   # COMPLETE: Use .long() for classification labels\n",
    "\n",
    "print(\"Dataset Conversion to Tensors\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features (X):\")\n",
    "print(f\"  Shape: {X_tensor.shape} (samples × features)\")\n",
    "print(f\"  Data type: {X_tensor.dtype}\")\n",
    "print(f\"  Min: {X_tensor.min().item():.4f}, Max: {X_tensor.max().item():.4f}\")\n",
    "\n",
    "print(f\"\\nLabels (y):\")\n",
    "print(f\"  Shape: {y_tensor.shape}\")\n",
    "print(f\"  Data type: {y_tensor.dtype}\")\n",
    "print(f\"  Unique values: {torch.unique(y_tensor)}\")\n",
    "\n",
    "# Show a single sample\n",
    "print(f\"\\nExample collision event (first sample):\")\n",
    "print(f\"  Features: {X_tensor[0, :5]} ... (showing first 5 of 28)\")\n",
    "print(f\"  Label: {y_tensor[0].item()} ({'signal' if y_tensor[0] == 1 else 'background'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 8. Creating a Custom Dataset Class\n",
    "\n",
    "For more complex data handling, PyTorch provides the `torch.utils.data.Dataset` class. By creating a custom dataset class, we can:\n",
    "- Load data on-demand (useful for large datasets that don't fit in memory)\n",
    "- Apply transformations consistently\n",
    "- Integrate seamlessly with PyTorch's `DataLoader` for batching\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: See the [PyTorch Data Tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more details.\n",
    "\n",
    "A custom Dataset class must implement:\n",
    "1. `__init__`: Initialize the dataset (load file paths, read metadata, etc.)\n",
    "2. `__len__`: Return the number of samples\n",
    "3. `__getitem__`: Return a single sample given an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HiggsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Higgs Boson particle collision data.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame with 'label' column and feature columns\n",
    "            transform: Optional transform to apply to features (e.g., normalization)\n",
    "        \"\"\"\n",
    "        self.features = torch.from_numpy(\n",
    "            dataframe.drop('Label', axis=1).values\n",
    "        ).float()\n",
    "        self.labels = torch.from_numpy(\n",
    "            dataframe['Label'].apply(lambda x: 1 if x == 's' else 0).values\n",
    "        ).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample (features, label) at index idx.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (features, label) where features is a tensor of shape (28,)\n",
    "                   and label is a scalar tensor\n",
    "        \"\"\"\n",
    "        features = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transformation if specified\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "            \n",
    "        return features, label\n",
    "\n",
    "# Create dataset instance\n",
    "higgs_dataset = HiggsDataset(higgs_df)\n",
    "\n",
    "print(\"Custom HiggsDataset Created\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset size: {len(higgs_dataset)} samples\")\n",
    "print(f\"\\nAccessing samples:\")\n",
    "\n",
    "# Access individual samples\n",
    "for i in range(3):\n",
    "    features, label = higgs_dataset[i]\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Features shape: {features.shape}\")\n",
    "    print(f\"  First 5 features: {features[:5]}\")\n",
    "    print(f\"  Label: {label.item()} ({'signal' if label == 1 else 'background'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Using DataLoader for Batching\n",
    "\n",
    "The `DataLoader` wraps a Dataset and provides:\n",
    "- **Batching**: Combine multiple samples into batches\n",
    "- **Shuffling**: Randomize sample order (important for training)\n",
    "- **Parallel loading**: Use multiple workers for faster data loading\n",
    "- **Automatic batching**: Handles stacking samples into batch tensors\n",
    "\n",
    "This is essential for training neural networks, where we process data in mini-batches rather than one sample at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(\n",
    "    higgs_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Randomize order\n",
    "    num_workers=0  # Set to 0 for compatibility; increase for parallel loading\n",
    ")\n",
    "\n",
    "print(\"DataLoader Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Total samples: {len(higgs_dataset)}\")\n",
    "\n",
    "# Iterate through the first batch\n",
    "for batch_idx, (features_batch, labels_batch) in enumerate(dataloader):\n",
    "    if batch_idx == 0:  # Only show first batch\n",
    "        print(f\"\\nFirst Batch:\")\n",
    "        print(f\"  Features batch shape: {features_batch.shape} (batch × features)\")\n",
    "        print(f\"  Labels batch shape: {labels_batch.shape}\")\n",
    "        print(f\"\\n  Sample features from batch:\")\n",
    "        pprint(features_batch[:2, :5])  # First 2 samples, first 5 features\n",
    "        print(f\"\\n  Sample labels from batch:\")\n",
    "        print(f\"  {labels_batch[:10]}\")  # First 10 labels\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Data pipeline ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 9. GPU Acceleration\n",
    "***\n",
    "\n",
    "One of PyTorch's main advantages is seamless GPU support. GPUs (Graphics Processing Units) excel at parallel computations, making them ideal for:\n",
    "- Large matrix operations\n",
    "- Deep learning model training\n",
    "- Monte Carlo simulations\n",
    "- Image processing\n",
    "- Any computation that can be parallelized\n",
    "\n",
    "## Moving Tensors to GPU\n",
    "\n",
    "To use the GPU, we need to move tensors from CPU to GPU memory using the `.to()` method.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Important**: All tensors in an operation must be on the same device (CPU or GPU). Mixing devices will cause errors.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/idea.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Tip**: For code that works on both CPU and GPU, use `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` and move all tensors to `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"GPU Acceleration Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available - computations will run on CPU\")\n",
    "\n",
    "# Example: Move a tensor to the device\n",
    "data = torch.randn(1000, 1000)\n",
    "print(f\"\\nOriginal tensor device: {data.device}\")\n",
    "\n",
    "# Move to GPU (if available)\n",
    "data_gpu = data.to(device)\n",
    "print(f\"After .to(device): {data_gpu.device}\")\n",
    "\n",
    "# Perform computation on the device\n",
    "result = data_gpu @ data_gpu.T  # Matrix multiplication\n",
    "print(f\"Result device: {result.device}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "\n",
    "# Move back to CPU (needed for NumPy conversion)\n",
    "result_cpu = result.cpu()\n",
    "print(f\"\\nMoved back to CPU: {result_cpu.device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ GPU operations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use GPU vs CPU\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/idea.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Decision Guide**:\n",
    "\n",
    "**Use GPU for**:\n",
    "- Training deep neural networks\n",
    "- Large matrix operations (size > 1000×1000)\n",
    "- Batch processing of images/signals\n",
    "- Long-running simulations\n",
    "- Operations repeated many times\n",
    "\n",
    "**Use CPU for**:\n",
    "- Small computations (overhead of GPU transfer isn't worth it)\n",
    "- Operations not parallelizable\n",
    "- Debugging (easier to inspect values)\n",
    "- When GPU memory is limited\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Device Consistency Rule**: All tensors and models in an operation must be on the same device. Common pattern:\n",
    "\n",
    "```python\n",
    "# Good practice: move everything to device at the start\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Now operations work\n",
    "output = model(x)\n",
    "loss = loss_fn(output, y)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uom-fse-dl-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
