{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/se_02.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/solutions/SE02_SL_Artificial_neural_networks.ipynb)\n",
    "\n",
    "## Workshop Overview\n",
    "***\n",
    "In this workshop, we demystify the \"black box\" of neural networks by building from the ground up‚Äîstarting with linear models (which you already understand from regression) and progressing to non-linear neural architectures. You'll see exactly how neurons combine to solve problems that simple linear models cannot.\n",
    "\n",
    "**Prerequisites**: Linear regression concepts, basic PyTorch (SE01)\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand a neuron as a generalization of linear regression\n",
    "- Manually implement and visualize decision boundaries\n",
    "- Recognize when linear models fail (e.g., non-separable data)\n",
    "- Build multi-layer perceptrons (MLPs) to solve non-linear problems\n",
    "- Visualize probability fields and decision boundaries interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab: downloading utils...\")\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt\",\n",
    "        \"-O\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"-x\",\n",
    "        \"-nH\",\n",
    "        \"--cut-dirs=3\",\n",
    "        \"-i\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(\"Running locally: skipping Colab utils download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Setup paths for helper utilities\n",
    "helper_utils = Path(Path.cwd().parent)\n",
    "if str(helper_utils) not in sys.path:\n",
    "    sys.path.append(str(helper_utils))\n",
    "\n",
    "# Core scientific computing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization\n",
    "from utils.plotting import plot_2d_classification, plot_training_loss, plot_model_comparison\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch Setup Information\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚úó No GPU detected - using CPU\")\n",
    "    print(\"  (For GPU: Runtime > Change runtime type > Hardware accelerator > GPU)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Font styling (Share Tech Mono) is handled automatically in visualization utilities -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 1. The Anatomy of a Neuron\n",
    "***\n",
    "\n",
    "## From Regression to Neural Networks\n",
    "\n",
    "If you've worked with linear regression before, you already understand the foundation of neural networks. A single neuron is mathematically equivalent to linear regression with an activation function applied to the output.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: A **neuron** (or perceptron) is a computational unit that:\n",
    "> 1. Takes multiple inputs $\\mathbf{x} = [x_1, x_2, ..., x_n]$\n",
    "> 2. Computes a weighted sum plus bias: $z = \\mathbf{w}^T \\mathbf{x} + b$\n",
    "> 3. Applies an activation function: $a = \\sigma(z)$\n",
    "\n",
    "The basic structure of a neuron can be seen below:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/non_linenar_neuron.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "### The Scientific Context\n",
    "\n",
    "Many physical systems start with linear assumptions:\n",
    "- **Ohm's Law**: $V = IR$ (voltage is linear in current)\n",
    "- **Newton's Second Law**: $F = ma$ (force is linear in acceleration)\n",
    "- **Hooke's Law**: $F = kx$ (spring force is linear in displacement)\n",
    "\n",
    "These linear models work well within certain regimes, but real-world systems often exhibit non-linearities. Neural networks give us a principled way to model these non-linear relationships while maintaining mathematical transparency.\n",
    "\n",
    "## 1.1 A Single Neuron as Linear Regression\n",
    "\n",
    "Let's start with the simplest case: a neuron performing **binary classification** on linearly separable data. This is conceptually identical to logistic regression.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a 2D input $\\mathbf{x} = [x_1, x_2]$:\n",
    "\n",
    "$$z = w_1 x_1 + w_2 x_2 + b$$\n",
    "\n",
    "$$\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w} = [w_1, w_2]$ are the **weights** (determine the decision boundary orientation)\n",
    "- $b$ is the **bias** (shifts the decision boundary)\n",
    "- $\\sigma$ is the **sigmoid activation function** (maps $z$ to probability $[0, 1]$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Generating Linearly Separable Data\n",
    "***\n",
    "\n",
    "Let's create a synthetic dataset where two classes can be perfectly separated by a straight line. This is analogous to having two distinct groups in an experiment (e.g., control vs. treatment, healthy vs. diseased) where measurements clearly distinguish them.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 1**: Generate and visualize a linearly separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linearly separable data using sklearn\n",
    "np.random.seed(42)\n",
    "X_linear, y_linear = make_classification(\n",
    "    n_samples=100,           # Number of data points\n",
    "    n_features=2,            # 2D for visualization\n",
    "    n_redundant=0,           # No redundant features\n",
    "    n_informative=2,         # Both features are informative\n",
    "    n_clusters_per_class=1,  # Single cluster per class\n",
    "    class_sep=1.0,           # Clear separation between classes\n",
    "    flip_y=0.0,              # No label noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_linear_tensor = torch.FloatTensor(X_linear)\n",
    "y_linear_tensor = torch.FloatTensor(y_linear).unsqueeze(1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Linear Dataset Generated\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {X_linear.shape} (samples √ó features)\")\n",
    "print(f\"Class distribution: {np.bincount(y_linear)}\")\n",
    "print(f\"Class 0: {np.sum(y_linear == 0)} samples\")\n",
    "print(f\"Class 1: {np.sum(y_linear == 1)} samples\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plot_2d_classification(\n",
    "    X_linear, y_linear,\n",
    "    title=\"Linearly Separable Data: Two Distinct Classes\".upper(),\n",
    "    show_boundary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Manual Implementation: Understanding the Forward Pass\n",
    "***\n",
    "\n",
    "Before using PyTorch's built-in layers, let's manually implement a neuron to understand exactly what's happening under the hood. This transparency is crucial for debugging and understanding more complex architectures later.\n",
    "\n",
    "### The Forward Pass (Matrix Formulation)\n",
    "\n",
    "For a batch of $N$ samples with $D$ features:\n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{X} \\mathbf{W}^T + \\mathbf{b}$$\n",
    "\n",
    "$$\\mathbf{A} = \\sigma(\\mathbf{Z})$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ is the input matrix (each row is a sample)\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{D}$ is the weight vector\n",
    "- $\\mathbf{b} \\in \\mathbb{R}$ is the bias scalar\n",
    "- $\\mathbf{Z} \\in \\mathbb{R}^{N}$ is the linear output (logits)\n",
    "- $\\mathbf{A} \\in \\mathbb{R}^{N}$ is the activated output (probabilities)\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Physical Analogy**: Think of $\\mathbf{Z}$ as an \"energy\" or \"potential\" that gets converted to a \"probability\" through the sigmoid activation function, similar to a Boltzmann distribution in statistical mechanics.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 2**: Implement the forward pass manually using PyTorch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias randomly\n",
    "torch.manual_seed(42)\n",
    "weights_manual = torch.randn(2, requires_grad=True)\n",
    "bias_manual = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(\"Initial Parameters:\")\n",
    "print(f\"  Weights: {weights_manual.detach().numpy()}\")\n",
    "print(f\"  Bias: {bias_manual.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual forward pass: Z = X @ W^T + b\n",
    "def forward_pass(X, weights, bias):\n",
    "    \"\"\"\n",
    "    Compute forward pass manually.\n",
    "    \n",
    "    Z = X @ W^T + b  (linear combination)\n",
    "    A = sigmoid(Z)   (activation)\n",
    "    \"\"\"\n",
    "    # Linear transformation: matrix multiplication + bias\n",
    "    z_manual = torch.matmul(X, weights.unsqueeze(1)) + bias\n",
    "    \n",
    "    # Sigmoid activation: œÉ(z) = 1 / (1 + exp(-z))\n",
    "    a_manual = 1 / (1 + torch.exp(-z_manual))\n",
    "    \n",
    "    return z_manual, a_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "z, predictions_manual = forward_pass(X_linear_tensor, weights_manual, bias_manual)\n",
    "\n",
    "print(f\"\\nOutput logits (z) - first 5: {z[:5].squeeze().detach().numpy()}\")\n",
    "print(f\"Output probabilities - first 5: {predictions_manual[:5].squeeze().detach().numpy()}\")\n",
    "print(f\"Predicted classes - first 5: {(predictions_manual[:5] > 0.5).int().squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize initial decision boundary\n",
    "plot_2d_classification(\n",
    "    X_linear, y_linear,\n",
    "    weights=weights_manual,\n",
    "    bias=bias_manual,\n",
    "    title=\"Initial Random Decision Boundary\".upper(),\n",
    "    show_boundary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Interpretation**: The decision boundary is a straight line in 2D (hyperplane in higher dimensions) defined by $w_1 x_1 + w_2 x_2 + b = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Learning via Gradient Descent: Sensitivity Analysis\n",
    "***\n",
    "\n",
    "The initial decision boundary is random and performs poorly. How do we improve it? Through **gradient descent** (an iterative optimization algorithm).\n",
    "\n",
    "### The Learning Algorithm\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: **Gradient descent** iteratively updates parameters in the direction that most reduces the loss function:\n",
    "\n",
    "$$\\mathbf{w}_{new} = \\mathbf{w}_{old} - \\eta \\frac{\\partial L}{\\partial \\mathbf{w}}$$\n",
    "\n",
    "$$b_{new} = b_{old} - \\eta \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where:\n",
    "- $\\eta$ is the **learning rate** (step size)\n",
    "- $\\frac{\\partial L}{\\partial \\mathbf{w}}$ is the **gradient** of loss with respect to weights\n",
    "- $L$ is the **loss function** measuring prediction error\n",
    "\n",
    "### Loss Function: Binary Cross-Entropy\n",
    "\n",
    "For binary classification, we use Binary Cross-Entropy (BCE) loss:\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "This measures the \"distance\" between predicted probabilities $\\hat{y}$ and true labels $y$.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Scientific Interpretation**: Gradient descent is a form of \"sensitivity analysis\". We are asking \"how much does the output change if I nudge this parameter?\" This is fundamental to inverse problems in physics (e.g., seismic inversion, tomography).\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 3**: Train the neuron using gradient descent and visualize the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset parameters\n",
    "torch.manual_seed(42)\n",
    "weights_manual = torch.randn(2, requires_grad=True)\n",
    "bias_manual = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "# Track training progress\n",
    "losses = []\n",
    "\n",
    "print(\"Training the neuron...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "    # Forward pass\n",
    "    z, predictions = forward_pass(X_linear_tensor, weights_manual, bias_manual)\n",
    "    \n",
    "    # Compute Binary Cross-Entropy loss\n",
    "    # BCE = -[y*log(p) + (1-y)*log(1-p)]\n",
    "    epsilon = 1e-7  # Small constant to avoid log(0)\n",
    "    loss = -torch.mean(\n",
    "        y_linear_tensor * torch.log(predictions + epsilon) +\n",
    "        (1 - y_linear_tensor) * torch.log(1 - predictions + epsilon)\n",
    "    )\n",
    "    \n",
    "    # Backward pass: compute gradients automatically\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    with torch.no_grad():\n",
    "        weights_manual -= learning_rate * weights_manual.grad\n",
    "        bias_manual -= learning_rate * bias_manual.grad\n",
    "        \n",
    "        # Zero gradients for next iteration\n",
    "        weights_manual.grad.zero_()\n",
    "        bias_manual.grad.zero_()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final Loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final Weights: {weights_manual.detach().numpy()}\")\n",
    "print(f\"Final Bias: {bias_manual.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "# Perform a final forward pass to get predictions\n",
    "final_z, final_predictions = forward_pass(X_linear_tensor, weights_manual, bias_manual)\n",
    "\n",
    "# Convert probabilities to binary class predictions\n",
    "predicted_classes = (final_predictions > 0.5).int().squeeze()\n",
    "\n",
    "# Accuracy = (number of correct predictions) / (total predictions)\n",
    "accuracy = (predicted_classes == y_linear_tensor.squeeze()).float().mean()\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy.item()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress with loss curve\n",
    "fig_loss = plot_training_loss(\n",
    "    losses, \n",
    "    title=\"Training Loss Over Time\".upper(),\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "fig_loss.show()\n",
    "\n",
    "# Visualize learned decision boundary\n",
    "fig_boundary = plot_2d_classification(\n",
    "    X_linear, y_linear,\n",
    "    weights=weights_manual,\n",
    "    bias=bias_manual,\n",
    "    title=\"Learned Decision Boundary\".upper(),\n",
    "    show_boundary=True\n",
    ")\n",
    "fig_boundary.show()\n",
    "\n",
    "print(\"\\n‚úì The neuron successfully learned to separate the two classes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Transition to PyTorch: The Standard Recipe\n",
    "***\n",
    "\n",
    "Now that we understand what's happening under the hood, let's use PyTorch's built-in `nn.Linear` layer. This is the standard approach in research and production.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: `torch.nn.Linear(in_features, out_features)` implements the transformation $y = xW^T + b$ where $W$ and $b$ are learnable parameters.\n",
    "\n",
    "### Benefits of PyTorch Modules\n",
    "\n",
    "1. **Automatic parameter management**: Weights and biases are created automatically\n",
    "2. **GPU compatibility**: Seamlessly move to GPU with `.to('cuda')`\n",
    "3. **Integration with optimizers**: Works with `torch.optim` for advanced optimization\n",
    "4. **Standard interface**: Follows the `nn.Module` pattern used everywhere in research\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 4**: Implement the same neuron using `nn.Linear` and verify results match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear model using PyTorch's nn.Linear\n",
    "torch.manual_seed(42)\n",
    "linear_model = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy with sigmoid built-in\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=0.1)\n",
    "\n",
    "# Track losses\n",
    "losses_lib = []\n",
    "\n",
    "print(\"Training with nn.Linear...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "    # Forward pass\n",
    "    logits = linear_model(X_linear_tensor)\n",
    "    predictions_lib = torch.sigmoid(logits)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions_lib, y_linear_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update parameters\n",
    "    \n",
    "    losses_lib.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final Loss: {losses_lib[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned parameters\n",
    "weights_lib = linear_model.weight.detach().numpy()[0]\n",
    "bias_lib = linear_model.bias.detach().item()\n",
    "print(f\"Final Weights: {weights_lib}\")\n",
    "print(f\"Final Bias: {bias_lib:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "with torch.no_grad():\n",
    "    final_logits = linear_model(X_linear_tensor)\n",
    "    final_preds_lib = torch.sigmoid(final_logits)\n",
    "    predicted_classes_lib = (final_preds_lib > 0.5).int().squeeze()\n",
    "    accuracy_lib = (predicted_classes_lib == y_linear_tensor.squeeze()).float().mean()\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy_lib.item()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify results are similar to manual implementation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparison: Manual vs. Library Implementation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loss difference: {abs(losses[-1] - losses_lib[-1]):.6f}\")\n",
    "print(f\"Both implementations converged: {abs(losses[-1] - losses_lib[-1]) < 0.01}\")\n",
    "print(\"‚úì PyTorch's nn.Linear produces equivalent results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 2. The Non-Linear Wall: When Lines Don't Work\n",
    "***\n",
    "\n",
    "## The Fundamental Limitation of Linear Models\n",
    "\n",
    "Linear models (including single neurons) can only create **linear decision boundaries**‚Äîstraight lines in 2D, planes in 3D, hyperplanes in higher dimensions. But what happens when the data isn't linearly separable?\n",
    "\n",
    "### Real-World Non-Linear Systems\n",
    "\n",
    "Many physical and biological systems exhibit non-linear behavior:\n",
    "- **Phase transitions**: Solid/liquid/gas states don't follow linear boundaries\n",
    "- **Chemical kinetics**: Reaction rates often follow non-linear curves (Michaelis-Menten, Hill equations)\n",
    "- **Bifurcations**: Small parameter changes can cause dramatic system transitions\n",
    "- **Classification problems**: Cancer vs. healthy cells may cluster in non-linear patterns\n",
    "\n",
    "## 2.1 The Circles Dataset: A Non-Linear Challenge\n",
    "***\n",
    "\n",
    "Let's create a dataset where one class forms a circle inside another class's ring. No straight line can separate these classes.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 5**: Generate concentric circles and attempt linear classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate concentric circles dataset\n",
    "np.random.seed(42)\n",
    "X_circles, y_circles = make_circles(\n",
    "    n_samples=200,\n",
    "    noise=0.1,\n",
    "    factor=0.5,  # Spacing between circles\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_circles_tensor = torch.FloatTensor(X_circles)\n",
    "y_circles_tensor = torch.FloatTensor(y_circles).unsqueeze(1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Circles Dataset Generated\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {X_circles.shape} (samples √ó features)\")\n",
    "print(f\"Class distribution: {np.bincount(y_circles)}\")\n",
    "print(f\"Inner circle (Class 0): {np.sum(y_circles == 0)} samples\")\n",
    "print(f\"Outer ring (Class 1): {np.sum(y_circles == 1)} samples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize the circles\n",
    "plot_2d_classification(\n",
    "    X_circles, y_circles,\n",
    "    title=\"Non-Linearly Separable Data: Concentric Circles\".upper(),\n",
    "    show_boundary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The Failure of Linear Classification\n",
    "***\n",
    "\n",
    "Let's train our linear neuron on the circles dataset and see what happens. Spoiler: it will fail spectacularly, but understanding *why* is crucial.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 6**: Train a linear classifier on non-linear data and observe the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear model on the circles dataset\n",
    "torch.manual_seed(42)\n",
    "linear_model_circles = nn.Linear(in_features=2, out_features=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(linear_model_circles.parameters(), lr=0.1)\n",
    "\n",
    "losses_linear_circles = []\n",
    "epochs_circles = 500\n",
    "\n",
    "print(\"Attempting to fit a straight line to circular data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in tqdm(range(epochs_circles), desc=\"Training\"):\n",
    "    logits = linear_model_circles(X_circles_tensor)\n",
    "    predictions = torch.sigmoid(logits)\n",
    "    loss = criterion(predictions, y_circles_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_linear_circles.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "with torch.no_grad():\n",
    "    final_logits = linear_model_circles(X_circles_tensor)\n",
    "    final_preds = torch.sigmoid(final_logits)\n",
    "    predicted_classes = (final_preds > 0.5).int().squeeze()\n",
    "    accuracy_linear = (predicted_classes == y_circles_tensor.squeeze()).float().mean()\n",
    "\n",
    "print(f\"Final Loss: {losses_linear_circles[-1]:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_linear.item()*100:.1f}%\")\n",
    "print(f\"\\n‚ùå Accuracy ‚âà 50% = Random guessing!\")\n",
    "print(\"The linear model cannot learn the circular pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the failed attempt\n",
    "weights_circles_linear = linear_model_circles.weight.detach()[0]\n",
    "bias_circles_linear = linear_model_circles.bias.detach().item()\n",
    "\n",
    "plot_2d_classification(\n",
    "    X_circles, y_circles,\n",
    "    weights=weights_circles_linear,\n",
    "    bias=bias_circles_linear,\n",
    "    title=\"Linear Model Failure: Straight Line Cannot Fit Circles\".upper(),\n",
    "    show_boundary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 3. The Neural Solution: Multi-Layer Perceptrons\n",
    "***\n",
    "\n",
    "## Breaking Through the Linear Barrier\n",
    "\n",
    "The solution: **stack multiple neurons in layers** and introduce **non-linear activation functions** between them. This creates a **Multi-Layer Perceptron (MLP)**, capable of learning arbitrarily complex decision boundaries.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: A **Multi-Layer Perceptron (MLP)** is a feedforward neural network with:\n",
    "> - An **input layer** (receives features)\n",
    "> - One or more **hidden layers** (perform transformations)\n",
    "> - An **output layer** (produces predictions)\n",
    "> - **Non-linear activation functions** between layers (enable non-linear mappings)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/ann.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "### Why Multiple Layers Work\n",
    "\n",
    "**Universal Approximation Theorem**: An MLP with a single hidden layer containing enough neurons can approximate any continuous function to arbitrary precision.\n",
    "\n",
    "**Intuition**: \n",
    "- First hidden layer learns **feature combinations** (e.g., $x_1^2 + x_2^2$ for circles)\n",
    "- Subsequent layers combine these features to create complex boundaries\n",
    "- Non-linear activations allow \"bending\" of decision surfaces\n",
    "\n",
    "## 3.1 Common Activation Functions\n",
    "***\n",
    "\n",
    "Activation functions introduce non-linearity. Common choices:\n",
    "\n",
    "| Activation | Formula | Use Case |\n",
    "|-----------|---------|----------|\n",
    "| **Sigmoid** | $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ | Output layer (probabilities) |\n",
    "| **ReLU** | $$\\text{ReLU}(x) = \\max(0, x)$$ | Hidden layers (fast, effective) |\n",
    "| **Tanh** | $$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$ | Hidden layers (zero-centered) |\n",
    "| **GeLU** | $$\\text{GeLU}(x) = x \\cdot \\Phi(x)$$ | Modern architectures (smooth) |\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Choosing Activations**: \n",
    "- Use **ReLU** or **GeLU** in hidden layers (default choice)\n",
    "- Use **sigmoid** for binary classification output\n",
    "- Use **softmax** for multi-class classification output\n",
    "\n",
    "## 3.2 Building an MLP in PyTorch\n",
    "***\n",
    "\n",
    "Let's build a simple MLP to solve the circles problem:\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input (2D) ‚Üí Hidden (16 neurons, ReLU) ‚Üí Hidden (8 neurons, ReLU) ‚Üí Output (1 neuron, Sigmoid)\n",
    "```\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 7**: Implement an MLP classifier for the circles dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for binary classification.\n",
    "    \n",
    "    Architecture:\n",
    "        Input(2) ‚Üí Linear(16) ‚Üí ReLU ‚Üí Linear(8) ‚Üí ReLU ‚Üí Linear(1) ‚Üí Sigmoid\n",
    "    \n",
    "    This demonstrates the standard PyTorch pattern:\n",
    "    1. Define layers in __init__\n",
    "    2. Define forward pass in forward()\n",
    "    3. Activation functions are applied explicitly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim1=16, hidden_dim2=8):\n",
    "        super(CircleClassifier, self).__init__()\n",
    "        \n",
    "        # Layer 1: Input ‚Üí First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        \n",
    "        # Layer 2: First hidden ‚Üí Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        \n",
    "        # Layer 3: Second hidden ‚Üí Output\n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Flow: x ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí Sigmoid ‚Üí output\n",
    "        \"\"\"\n",
    "        # First hidden layer with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Second hidden layer with ReLU activation\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Output layer with Sigmoid activation\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "torch.manual_seed(42)\n",
    "model = CircleClassifier(input_dim=2, hidden_dim1=16, hidden_dim2=8)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MLP Architecture\")\n",
    "print(\"=\" * 60)\n",
    "print(model)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:20s}: {param.shape} = {param.numel()} parameters\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training the MLP\n",
    "***\n",
    "\n",
    "Now let's train the MLP on the circles dataset and watch it succeed where the linear model failed.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 8**: Train the MLP and visualize the non-linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer (more sophisticated than SGD)\n",
    "\n",
    "epochs_mlp = 500\n",
    "losses_mlp = []\n",
    "\n",
    "print(\"Training MLP on circles dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in tqdm(range(epochs_mlp), desc=\"Training MLP\"):\n",
    "    # Forward pass (sigmoid is already in the model)\n",
    "    predictions = model(X_circles_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, y_circles_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_mlp.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Calculate current accuracy\n",
    "        with torch.no_grad():\n",
    "            current_preds = model(X_circles_tensor)\n",
    "            predicted_classes = (current_preds > 0.5).int().squeeze()\n",
    "            accuracy = (predicted_classes == y_circles_tensor.squeeze()).float().mean()\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy.item()*100:.1f}%\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "with torch.no_grad():\n",
    "    final_predictions = model(X_circles_tensor)\n",
    "    predicted_classes = (final_predictions > 0.5).int().squeeze()\n",
    "    final_accuracy = (predicted_classes == y_circles_tensor.squeeze()).float().mean()\n",
    "\n",
    "print(f\"Final Loss: {losses_mlp[-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {final_accuracy.item()*100:.1f}%\")\n",
    "print(f\"\\n‚úì The MLP successfully learned the circular pattern!\")\n",
    "print(f\"  Linear model accuracy: {accuracy_linear.item()*100:.1f}%\")\n",
    "print(f\"  MLP accuracy: {final_accuracy.item()*100:.1f}%\")\n",
    "print(f\"  Improvement: +{(final_accuracy.item() - accuracy_linear.item())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Visualizing the Non-Linear Decision Boundary\n",
    "***\n",
    "\n",
    "The true power of MLPs becomes apparent when we visualize the **probability field**‚Äîthe predicted probability $P(y=1)$ at every point in the feature space. For the circles dataset, this field should show a \"well\" in the center (low probability) surrounded by a \"hill\" (high probability).\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Physical Analogy**: Think of the probability field as a potential energy surface or phase diagram. The decision boundary (P=0.5) is analogous to a phase transition or separatrix in dynamical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úì The probability contours clearly 'hug' the circular structure!\")\n",
    "print(\"  Blue region (P ‚âà 0): Inner circle\")\n",
    "print(\"  Red region (P ‚âà 1): Outer ring\")\n",
    "print(\"  Boundary (P = 0.5): Curved separation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the probability field with contours\n",
    "plot_2d_classification(\n",
    "    X_circles, y_circles,\n",
    "    title=\"MLP Decision Boundary: Non-Linear Probability Field\",\n",
    "    show_boundary=False,\n",
    "    model=model,\n",
    "    show_probabilities=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Comparative Analysis: Linear vs. Non-Linear\n",
    "***\n",
    "\n",
    "Let's create a side-by-side comparison to see the dramatic difference between linear and non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig = plot_model_comparison(\n",
    "    X=X_circles,\n",
    "    y=y_circles,\n",
    "    linear_weights=weights_circles_linear,\n",
    "    linear_bias=bias_circles_linear,\n",
    "    mlp_model=model,\n",
    "    losses_linear=losses_linear_circles,\n",
    "    losses_mlp=losses_mlp,\n",
    "    accuracy_linear=accuracy_linear.item(),\n",
    "    accuracy_mlp=final_accuracy.item(),\n",
    "    width=1400,\n",
    "    height=450\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"=\" * 60)x\n",
    "print(\"1. Linear Model: Straight boundary, ~50% accuracy (random)\")\n",
    "print(\"2. MLP Model: Curved boundary perfectly hugs the circle\")\n",
    "print(\"3. Loss: MLP converges to near-zero, linear plateaus high\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 4. Summary and Key Takeaways\n",
    "***\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "### 1. **Single Neurons = Linear Regression + Activation**\n",
    "   - A neuron computes $\\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$\n",
    "   - This is logistic regression for binary classification\n",
    "   - Decision boundary is a straight line (hyperplane)\n",
    "   - Works perfectly for linearly separable data\n",
    "\n",
    "### 2. **Linear Models Have Fundamental Limitations**\n",
    "   - Cannot learn non-linear patterns (circles, spirals, XOR, etc.)\n",
    "   - Accuracy plateaus at ~50% for non-separable data\n",
    "   - No amount of training can overcome this structural limitation\n",
    "   - Real-world data often exhibits non-linear structure\n",
    "\n",
    "### 3. **Multi-Layer Perceptrons Break the Linear Barrier**\n",
    "   - Stack multiple layers with non-linear activations\n",
    "   - Hidden layers learn feature combinations\n",
    "   - Can approximate arbitrary continuous functions\n",
    "   - Decision boundaries \"bend\" to fit complex patterns\n",
    "\n",
    "### 4. **The PyTorch Standard Operating Procedure**\n",
    "   ```python\n",
    "   class Model(nn.Module):\n",
    "       def __init__(self):\n",
    "           # Define layers\n",
    "       \n",
    "       def forward(self, x):\n",
    "           # Define forward pass\n",
    "   \n",
    "   model = Model()\n",
    "   optimizer = optim.Adam(model.parameters())\n",
    "   criterion = nn.BCELoss()\n",
    "   \n",
    "   for epoch in range(epochs):\n",
    "       predictions = model(inputs)\n",
    "       loss = criterion(predictions, targets)\n",
    "       optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "   ```\n",
    "\n",
    "## Physical and Scientific Parallels\n",
    "\n",
    "| Concept | Neural Network | Physical System |\n",
    "|---------|---------------|-----------------|\n",
    "| **Decision Boundary** | P(y=1) = 0.5 surface | Phase transition, separatrix |\n",
    "| **Probability Field** | P(y=1) at each point | Potential energy, field strength |\n",
    "| **Gradient Descent** | Parameter optimization | Variational methods, energy minimization |\n",
    "| **Non-linearity** | ReLU, Sigmoid, Tanh | Saturation, bifurcations, hysteresis |\n",
    "| **Hidden Layers** | Feature extraction | Latent variables, order parameters |\n",
    "\n",
    "\n",
    "**Next Steps**:\n",
    "1. Try modifying the MLP architecture (more layers, different activations)\n",
    "2. Experiment with the `make_moons` dataset (another non-linear challenge)\n",
    "3. Visualize how hidden layer features evolve during training\n",
    "4. Explore other datasets from your scientific domain\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Remember**: Neural networks are tools, not magic. Understanding their mechanics empowers you to apply them rigorously to scientific problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Bonus Challenge: The Moons Dataset\n",
    "***\n",
    "\n",
    "Ready to test your understanding? The `make_moons` dataset presents another non-linear challenge‚Äîtwo interleaving half-circles.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Bonus Exercise**: Apply what you've learned to solve the moons problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moons dataset\n",
    "np.random.seed(42)\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_moons_tensor = torch.FloatTensor(X_moons)\n",
    "y_moons_tensor = torch.FloatTensor(y_moons).unsqueeze(1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Moons Dataset Generated\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {X_moons.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_moons)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize the moons\n",
    "plot_2d_classification(\n",
    "    X_moons, y_moons,\n",
    "    title=\"Bonus Challenge: Interleaving Moons\",\n",
    "    show_boundary=False\n",
    ")\n",
    "\n",
    "# TODO: Your turn! \n",
    "# 1. Create a new MLP class called MoonClassifier\n",
    "# 2. Train it on X_moons_tensor and y_moons_tensor\n",
    "# 3. Visualize the decision boundary\n",
    "# 4. Compare linear vs. MLP performance\n",
    "\n",
    "print(\"\\nüí° Hint: Use the same structure as CircleClassifier!\")\n",
    "print(\"üí° Try experimenting with different architectures:\")\n",
    "print(\"   - Change the number of hidden layers\")\n",
    "print(\"   - Change the number of neurons per layer\")\n",
    "print(\"   - Try different activation functions (Tanh, LeakyReLU, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Solution\n",
    "\n",
    "Here's one possible solution for the moons dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: MLP for moons dataset\n",
    "class MoonClassifier(nn.Module):\n",
    "    \"\"\"MLP classifier for the moons dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim1=16, hidden_dim2=8):\n",
    "        super(MoonClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Create and train model\n",
    "torch.manual_seed(42)\n",
    "model_moons = MoonClassifier()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_moons.parameters(), lr=0.01)\n",
    "\n",
    "losses_moons = []\n",
    "for epoch in range(500):\n",
    "    predictions = model_moons(X_moons_tensor)\n",
    "    loss = criterion(predictions, y_moons_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_moons.append(loss.item())\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    final_predictions = model_moons(X_moons_tensor)\n",
    "    predicted_classes = (final_predictions > 0.5).int().squeeze()\n",
    "    accuracy_moons = (predicted_classes == y_moons_tensor.squeeze()).float().mean()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Moons Challenge Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final Loss: {losses_moons[-1]:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_moons.item()*100:.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize result\n",
    "plot_2d_classification(\n",
    "    X_moons, y_moons,\n",
    "    title=\"MLP Solution: Moons Dataset\",\n",
    "    show_boundary=False,\n",
    "    model=model_moons,\n",
    "    show_probabilities=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Challenge complete! The MLP successfully learned the moon pattern.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uom-fse-dl-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
