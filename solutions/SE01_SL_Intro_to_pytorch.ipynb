{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/se_01.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/solutions/SE01_SL_Intro_to_pytorch.ipynb)\n",
    "\n",
    "## Workshop Overview\n",
    "***\n",
    "Welcome! This workshop introduces PyTorch with a focus on scientific computing applications. Whether you're analyzing experimental data, running simulations, or building models for physical systems, PyTorch provides powerful tools for numerical computation and GPU acceleration.\n",
    "\n",
    "**Prerequisites**: Basic Python, familiarity with NumPy arrays and Pandas DataFrames\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand PyTorch tensors as GPU-accelerated arrays\n",
    "- Convert between NumPy, Pandas, and PyTorch\n",
    "- Perform scientific computations efficiently\n",
    "- Load and process real scientific datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 1. Why PyTorch for Scientific Computing?\n",
    "***\n",
    "[PyTorch](https://pytorch.org/) is an open-source machine learning framework that excels at scientific computing tasks requiring:\n",
    "\n",
    "- **GPU Acceleration**: Move computations from CPU to GPU with a single line of code\n",
    "- **Automatic Differentiation**: Compute gradients automatically (essential for optimization and inverse problems)\n",
    "- **NumPy Compatibility**: Seamless conversion between NumPy and PyTorch\n",
    "- **Dynamic Computation**: Build and modify computational graphs on-the-fly\n",
    "\n",
    "## PyTorch vs. Traditional Scientific Python\n",
    "\n",
    "While NumPy and SciPy are excellent for CPU-based scientific computing, PyTorch offers:\n",
    "\n",
    "1. **Hardware Flexibility**: Same code runs on CPU or GPU\n",
    "2. **Autograd**: Built-in automatic differentiation for optimization\n",
    "3. **Deep Learning Integration**: Easily transition from data processing to model building\n",
    "4. **Production Ready**: Deploy research code to production environments\n",
    "\n",
    "## Setting up the environment\n",
    "\n",
    "We are going to use different python modules throughout this course. It is not necessary to be familiar with all of them at the moment. Some of these libraries enable us to work with data and perform numerical operations, while others are used for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PyTorch Setup Information\n",
      "============================================================\n",
      "PyTorch Version: 2.10.0\n",
      "NumPy Version: 2.4.2\n",
      "Pandas Version: 3.0.0\n",
      "------------------------------------------------------------\n",
      "✗ No GPU detected - using CPU\n",
      "  (For GPU: Runtime > Change runtime type > Hardware accelerator > GPU)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch Setup Information\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"✗ No GPU detected - using CPU\")\n",
    "    print(\"  (For GPU: Runtime > Change runtime type > Hardware accelerator > GPU)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 2. Scientific Python Refresher: NumPy & Pandas\n",
    "***\n",
    "\n",
    "Before diving into PyTorch, let's briefly review NumPy and Pandas. If you're already comfortable with these libraries, feel free to skim this section.\n",
    "\n",
    "## 2.1 NumPy: Arrays for Numerical Data\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: NumPy arrays are n-dimensional containers for homogeneous data (all elements have the same type). They're ideal for representing grids, matrices, sensor readings, or any structured numerical data.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Common Use Cases**:\n",
    "- Time series data (temperature, pressure, voltage readings)\n",
    "- Spatial data (2D/3D grids from simulations)\n",
    "- Experimental measurements\n",
    "- Image data from microscopes or telescopes\n",
    "\n",
    "Let's create some example scientific data using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Sensor Data\n",
      "----------------------------------------\n",
      "Shape: (25,)\n",
      "Data type: float64\n",
      "Mean temperature: 19.95 °C\n",
      "Std deviation: 3.52 °C\n",
      "First 5 readings: [20.5922101  20.73108021 22.17143002 23.74498951 24.58756822]\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Temperature sensor readings over time (1D array)\n",
    "time_hours = np.linspace(0, 24, 25)  # 0 to 24 hours\n",
    "temperature_celsius = 20 + 5 * np.sin(2 * np.pi * time_hours / 24) + np.random.normal(0, 0.5, 25)\n",
    "\n",
    "print(\"Temperature Sensor Data\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {temperature_celsius.shape}\")\n",
    "print(f\"Data type: {temperature_celsius.dtype}\")\n",
    "print(f\"Mean temperature: {temperature_celsius.mean():.2f} °C\")\n",
    "print(f\"Std deviation: {temperature_celsius.std():.2f} °C\")\n",
    "print(f\"First 5 readings: {temperature_celsius[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2D Temperature Field (Heat Source)\n",
      "----------------------------------------\n",
      "Shape: (50, 50)\n",
      "Max temperature: 99.79 °C\n",
      "Min temperature: 0.67 °C\n"
     ]
    }
   ],
   "source": [
    "# Example 2: 2D grid (e.g., spatial temperature distribution)\n",
    "grid_size = 50\n",
    "x = np.linspace(-5, 5, grid_size)\n",
    "y = np.linspace(-5, 5, grid_size)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "# Simulate a Gaussian temperature distribution\n",
    "temperature_2d = 100 * np.exp(-(X**2 + Y**2) / 10)\n",
    "\n",
    "print(\"\\n2D Temperature Field (Heat Source)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {temperature_2d.shape}\")\n",
    "print(f\"Max temperature: {temperature_2d.max():.2f} °C\")\n",
    "print(f\"Min temperature: {temperature_2d.min():.2f} °C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3D Volumetric Data\n",
      "----------------------------------------\n",
      "Shape: (10, 10, 10) (depth × height × width)\n",
      "Total elements: 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 3: 3D volumetric data (e.g., MRI scan, simulation output)\n",
    "volume_data = np.random.randn(10, 10, 10)  # Small 3D volume\n",
    "print(\"\\n3D Volumetric Data\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {volume_data.shape} (depth × height × width)\")\n",
    "print(f\"Total elements: {volume_data.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising previous data using Plotly\n",
    "# 1D Line Plot for Temperature Sensor Readings\n",
    "fig1 = px.line(x=time_hours, y=temperature_celsius, title=\"Temperature Sensor Readings Over Time\",\n",
    "               labels={\"x\": \"Time (hours)\", \"y\": \"Temperature (°C)\"})\n",
    "fig1.show() \n",
    "\n",
    "# 2D Heatmap for Temperature Field\n",
    "fig2 = px.imshow(temperature_2d, title=\"2D Temperature Field (Heat Source)\",\n",
    "                 labels={\"x\": \"X-axis\", \"y\": \"Y-axis\", \"color\": \"Temperature (°C)\"})\n",
    "fig2.show()\n",
    "\n",
    "# 3D Surface Plot for Volumetric Data\n",
    "fig3 = go.Figure(data=[go.Surface(z=volume_data[5, :, :])])\n",
    "fig3.update_layout(title='3D Volumetric Data (Slice at depth=5)',\n",
    "                   scene=dict(xaxis_title='X-axis', yaxis_title='Y-axis', zaxis_title='Value'))\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pandas: DataFrames for Tabular Data\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: Pandas DataFrames are 2D labeled data structures with columns of potentially different types. They're like spreadsheets or SQL tables in Python.\n",
    "\n",
    "<img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/>  **Common Use Cases**:\n",
    "- Experimental measurements with metadata (e.g., sample ID, conditions, measurements)\n",
    "- Time series with multiple sensors\n",
    "- Results from parameter sweeps\n",
    "- Loading CSV/Excel files from lab equipment\n",
    "\n",
    "Let's create a sample experimental dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental Dataset Summary\n",
      "============================================================\n",
      "Number of samples: 100\n",
      "Columns: ['sample_id', 'concentration_mM', 'temperature_C', 'pressure_bar', 'reaction_rate']\n"
     ]
    }
   ],
   "source": [
    "# Create a sample experimental dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "experimental_data = pd.DataFrame({\n",
    "    'sample_id': [f'S{i:03d}' for i in range(n_samples)],\n",
    "    'concentration_mM': np.random.uniform(0.1, 10.0, n_samples),\n",
    "    'temperature_C': np.random.normal(25, 2, n_samples),\n",
    "    'pressure_bar': np.random.normal(1.0, 0.05, n_samples),\n",
    "    'reaction_rate': np.random.lognormal(0, 0.5, n_samples)\n",
    "})\n",
    "\n",
    "print(\"Experimental Dataset Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples: {len(experimental_data)}\")\n",
    "print(f\"Columns: {list(experimental_data.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>concentration_mM</th>\n",
       "      <th>temperature_C</th>\n",
       "      <th>pressure_bar</th>\n",
       "      <th>reaction_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S000</td>\n",
       "      <td>3.807947</td>\n",
       "      <td>25.174094</td>\n",
       "      <td>1.000650</td>\n",
       "      <td>1.104651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S001</td>\n",
       "      <td>9.512072</td>\n",
       "      <td>24.401985</td>\n",
       "      <td>1.072677</td>\n",
       "      <td>0.740738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S002</td>\n",
       "      <td>7.346740</td>\n",
       "      <td>25.183522</td>\n",
       "      <td>0.986767</td>\n",
       "      <td>1.035517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S003</td>\n",
       "      <td>6.026719</td>\n",
       "      <td>21.024862</td>\n",
       "      <td>1.136008</td>\n",
       "      <td>0.824765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S004</td>\n",
       "      <td>1.644585</td>\n",
       "      <td>24.560656</td>\n",
       "      <td>1.031283</td>\n",
       "      <td>1.058400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id  concentration_mM  temperature_C  pressure_bar  reaction_rate\n",
       "0      S000          3.807947      25.174094      1.000650       1.104651\n",
       "1      S001          9.512072      24.401985      1.072677       0.740738\n",
       "2      S002          7.346740      25.183522      0.986767       1.035517\n",
       "3      S003          6.026719      21.024862      1.136008       0.824765\n",
       "4      S004          1.644585      24.560656      1.031283       1.058400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "experimental_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concentration_mM</th>\n",
       "      <th>temperature_C</th>\n",
       "      <th>pressure_bar</th>\n",
       "      <th>reaction_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.754789</td>\n",
       "      <td>24.997839</td>\n",
       "      <td>1.001844</td>\n",
       "      <td>1.148774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.945145</td>\n",
       "      <td>1.825859</td>\n",
       "      <td>0.054868</td>\n",
       "      <td>0.500276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.154669</td>\n",
       "      <td>21.024862</td>\n",
       "      <td>0.837937</td>\n",
       "      <td>0.376799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.012688</td>\n",
       "      <td>23.589745</td>\n",
       "      <td>0.962951</td>\n",
       "      <td>0.780174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.695010</td>\n",
       "      <td>25.155610</td>\n",
       "      <td>1.003062</td>\n",
       "      <td>1.044161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.329011</td>\n",
       "      <td>25.967383</td>\n",
       "      <td>1.034063</td>\n",
       "      <td>1.369346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.870181</td>\n",
       "      <td>29.926484</td>\n",
       "      <td>1.192637</td>\n",
       "      <td>2.934659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       concentration_mM  temperature_C  pressure_bar  reaction_rate\n",
       "count        100.000000     100.000000    100.000000     100.000000\n",
       "mean           4.754789      24.997839      1.001844       1.148774\n",
       "std            2.945145       1.825859      0.054868       0.500276\n",
       "min            0.154669      21.024862      0.837937       0.376799\n",
       "25%            2.012688      23.589745      0.962951       0.780174\n",
       "50%            4.695010      25.155610      1.003062       1.044161\n",
       "75%            7.329011      25.967383      1.034063       1.369346\n",
       "max            9.870181      29.926484      1.192637       2.934659"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical summary\n",
    "experimental_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted concentrations shape: (100,)\n",
      "Data type: float64\n"
     ]
    }
   ],
   "source": [
    "# Accessing data: convert to NumPy arrays\n",
    "concentrations = experimental_data['concentration_mM'].values\n",
    "print(f\"\\nExtracted concentrations shape: {concentrations.shape}\")\n",
    "print(f\"Data type: {concentrations.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 3. PyTorch Tensors: NumPy Arrays on Steroids\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/>  **Definition**: A **tensor** is PyTorch's core data structure. Essentially it can be boiled down to a multidimensional array (like NumPy's `ndarray`) that can live on the GPU and track gradients for automatic differentiation.\n",
    "\n",
    "**Think of tensors as**: NumPy arrays + GPU support + automatic differentiation\n",
    "\n",
    "## Why Tensors Matter for Scientists\n",
    "\n",
    "1. **Hardware Flexibility**: Move computations between CPU and GPU with one line\n",
    "2. **Performance**: GPU acceleration for large-scale computations (matrix operations, FFTs, etc.)\n",
    "3. **Gradient Computation**: Essential for optimization, inverse problems, and machine learning\n",
    "4. **Seamless Integration**: Convert to/from NumPy and Pandas easily\n",
    "\n",
    "## Tensor Terminology\n",
    "\n",
    "Similar to arrays, tensors have different **ranks** (number of dimensions):\n",
    "\n",
    "- **Scalar** (rank 0): A single number, e.g., temperature = 273.15\n",
    "- **Vector** (rank 1): 1D array, e.g., time series `[t₀, t₁, t₂, ...]`\n",
    "- **Matrix** (rank 2): 2D array, e.g., grayscale image or grid data\n",
    "- **3D Tensor** (rank 3): e.g., RGB image (height × width × channels) or volumetric data\n",
    "- **nD Tensor** (rank n): Higher-dimensional data (e.g., batch of 3D volumes)\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"../figs\\tensors.png\" alt=\"Visual representation of tensors\" align=\"center\" style=\"width: 60%; height: auto; margin: 0 auto;\" />\n",
    "</figure>\n",
    "\n",
    "## 3.1 Creating Tensors\n",
    "***\n",
    "\n",
    "Let's create tensors using `torch.tensor()`. \n",
    "\n",
    "> <img src=\"../figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: For details, see the [PyTorch Tensor Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) and [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurement count: 42\n",
      "  Type: <class 'torch.Tensor'>\n",
      "  Shape: torch.Size([])\n",
      "  Data type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating scalar tensors (rank 0)\n",
    "# Scalars represent single values like temperature, pressure, or energy\n",
    "\n",
    "# Integer scalar\n",
    "measurement_count = torch.tensor(42)\n",
    "print(f\"Measurement count: {measurement_count}\")\n",
    "print(f\"  Type: {type(measurement_count)}\")\n",
    "print(f\"  Shape: {measurement_count.shape}\")\n",
    "print(f\"  Data type: {measurement_count.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 273.1499938964844 K\n",
      "  Shape: torch.Size([])\n",
      "  Data type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Float scalar (common for physical measurements)\n",
    "temperature_kelvin = torch.tensor(273.15)\n",
    "print(f\"Temperature: {temperature_kelvin} K\")\n",
    "print(f\"  Shape: {temperature_kelvin.shape}\")\n",
    "print(f\"  Data type: {temperature_kelvin.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted value: 273.1499938964844 (type: <class 'float'>)\n"
     ]
    }
   ],
   "source": [
    "# Extracting the Python value\n",
    "print(f\"\\nExtracted value: {temperature_kelvin.item()} (type: {type(temperature_kelvin.item())})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Scientific Constants:\n",
      "============================================================\n",
      "Pressure: 1.013 atm\n",
      "  Shape: torch.Size([]), Dtype: torch.float32\n",
      "\n",
      "Avogadro's number: 6.022e+23\n",
      "  Shape: torch.Size([]), Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Practice: Create your own scientific measurements\n",
    "\n",
    "# Create a scalar for pressure in atmospheres\n",
    "pressure_atm = torch.tensor(1.013)\n",
    "\n",
    "# Create a scalar for Avogadro's number (as float32 for efficiency)\n",
    "avogadro_approx = torch.tensor(6.022e23, dtype=torch.float32)\n",
    "\n",
    "print(\"Your Scientific Constants:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Pressure: {pressure_atm.item():.3f} atm\")\n",
    "print(f\"  Shape: {pressure_atm.shape}, Dtype: {pressure_atm.dtype}\")\n",
    "print(f\"\\nAvogadro's number: {avogadro_approx.item():.3e}\")\n",
    "print(f\"  Shape: {avogadro_approx.shape}, Dtype: {avogadro_approx.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we created a scalar tensor with a single element. Looking at its attributes, we can see that the tensor has a shape of `torch.Size([])`, which means that it has no dimensions. We can also see that the tensor has a data type of `torch.int64`, which means that it is an integer tensor.\n",
    "\n",
    "\n",
    "> <img src=\"../figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Note**: The data type of a tensor is determined by the data type of the elements that it contains. It is important to be aware of the data type of a tensor, as it can affect the results of operations that are performed on it. Good practice is to always specify the data type of a tensor when creating it.\n",
    "\n",
    "\n",
    "As we can see our single element is now stored in a type of container, which means that we can perform operations on it but not directly on the element itself. To access the element, we can use the method `item()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the data type of a tensor by passing the `dtype` argument to the `torch.Tensor` constructor. Alternatively, we can use the 'torch.tensor.type` method to change the data type of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.)\n",
      "tensor(42)\n",
      "tensor(42, dtype=torch.int32)\n",
      "tensor(42., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create a scalar tensor with a specific data type\n",
    "scalar_tensor = torch.tensor(42, dtype=torch.float32)\n",
    "print(scalar_tensor)\n",
    "\n",
    "# Change the data type of a tensor\n",
    "scalar_tensor = scalar_tensor.type(torch.int64)\n",
    "print(scalar_tensor)\n",
    "\n",
    "# Another way to change the data type of a tensor\n",
    "scalar_tensor = scalar_tensor.int()\n",
    "print(scalar_tensor)\n",
    "\n",
    "# # Not recommended as it can be confusing \n",
    "# with the .to() method that is used to move tensors\n",
    "# to different devices\n",
    "scalar_tensor = scalar_tensor.to(torch.float64) \n",
    "print(scalar_tensor) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Initializing tensors\n",
    "***\n",
    "\n",
    "PyTorch provides multiple ways to initialize tensors. Sometimes, we want to create a tensor with specific values, while other times we want to create a tensor with random values. PyTorch provides several functions for creating tensors with different initializations. Below is a table summarizing some of the most commonly used tensor creation functions in PyTorch.\n",
    "\n",
    "| Function | Description | Example | Output Shape |\n",
    "|----------|-------------|---------|--------------|\n",
    "| `torch.tensor()` | Creates tensor from data | `torch.tensor([1, 2, 3])` | `(3,)` |\n",
    "| `torch.zeros()` | Creates tensor of zeros | `torch.zeros(2, 3)` | `(2, 3)` |\n",
    "| `torch.ones()` | Creates tensor of ones | `torch.ones(2, 3)` | `(2, 3)` |\n",
    "| `torch.rand()` | Uniform random [0, 1] | `torch.rand(2, 3)` | `(2, 3)` |\n",
    "| `torch.randn()` | Normal distribution μ=0, σ=1 | `torch.randn(2, 3)` | `(2, 3)` |\n",
    "| `torch.arange()` | Integer sequence | `torch.arange(5)` | `(5,)` |\n",
    "| `torch.linspace()` | Evenly spaced sequence | `torch.linspace(0, 1, 5)` | `(5,)` |\n",
    "| `torch.eye()` | Identity matrix | `torch.eye(3)` | `(3, 3)` |\n",
    "| `torch.randint()` | Random integers | `torch.randint(0, 10, (2, 3))` | `(2, 3)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Initialization Examples\n",
      "============================================================\n",
      "Time points: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000])\n",
      "  Shape: torch.Size([5])\n",
      "\n",
      "Zeros (3×3 grid):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Ones (2×4 mask):\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Random uniform [0,1] (3×3):\n",
      "tensor([[0.7887, 0.3782, 0.8066],\n",
      "        [0.6709, 0.8155, 0.0268],\n",
      "        [0.3179, 0.8554, 0.2039]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common tensor creation methods for scientific computing\n",
    "\n",
    "print(\"Tensor Initialization Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. From a list (e.g., measurement data)\n",
    "time_points = torch.tensor([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "print(f\"Time points: {time_points}\")\n",
    "print(f\"  Shape: {time_points.shape}\\n\")\n",
    "\n",
    "# 2. Zeros (useful for initializing arrays)\n",
    "grid_2d = torch.zeros(3, 3)\n",
    "print(f\"Zeros (3×3 grid):\\n{grid_2d}\\n\")\n",
    "\n",
    "# 3. Ones (useful for masks or initial conditions)\n",
    "mask = torch.ones(2, 4)\n",
    "print(f\"Ones (2×4 mask):\\n{mask}\\n\")\n",
    "\n",
    "# 4. Random values (for Monte Carlo simulations)\n",
    "random_samples = torch.rand(3, 3)  # Uniform [0, 1]\n",
    "print(f\"Random uniform [0,1] (3×3):\\n{random_samples}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise: tensor([ 0.9885,  0.7861, -0.7283, -0.9239, -0.8895])\n",
      "\n",
      "Sequence (0 to 10, step 2): tensor([0, 2, 4, 6, 8])\n",
      "\n",
      "Wavelengths (400-700 nm): tensor([400., 450., 500., 550., 600., 650., 700.])\n",
      "\n",
      "Identity matrix (3×3):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 5. Random normal (Gaussian noise)\n",
    "noise = torch.randn(5)  # Mean=0, Std=1\n",
    "print(f\"Gaussian noise: {noise}\\n\")\n",
    "\n",
    "# 6. Sequences (like np.arange)\n",
    "indices = torch.arange(0, 10, 2)\n",
    "print(f\"Sequence (0 to 10, step 2): {indices}\\n\")\n",
    "\n",
    "# 7. Linearly spaced (like np.linspace)\n",
    "wavelengths = torch.linspace(400, 700, 7)  # nm (visible spectrum)\n",
    "print(f\"Wavelengths (400-700 nm): {wavelengths}\\n\")\n",
    "\n",
    "# 8. Identity matrix (for linear algebra)\n",
    "identity_3x3 = torch.eye(3)\n",
    "print(f\"Identity matrix (3×3):\\n{identity_3x3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Indexing tensors\n",
    "***\n",
    "Indexing tensors is similar to indexing arrays in Python. We can use square brackets `[]` to access elements in a tensor. This is useful for extracting specific elements or slices of a tensor. Below is a table summarizing the different ways to index tensors in PyTorch.\n",
    "\n",
    "> <img src=\"../figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/>  **Tips**:\n",
    "> - Use `:` to select all elements in a dimension\n",
    "> - Use negative indices to count from the end: -1 is last element\n",
    "> - Ellipsis (`...`) represents multiple full slices\n",
    "> - Step values can be negative for reverse order\n",
    "> - Boolean masks must match tensor dimensions\n",
    "\n",
    "| Method | Syntax | Description | Example | Result |\n",
    "|--------|--------|-------------|---------|---------|\n",
    "| Basic Indexing | `tensor[ix,jx]` | Access single element | `t[0,1]` | Element at row 0, col 1 |\n",
    "| Slicing | `tensor[start:end]` | Extract subset | `t[1:3]` | Elements from index 1 to 2 |\n",
    "| Striding | `tensor[::step]` | Extract with step | `t[::2]` | Every second element |\n",
    "| Negative Indexing | `tensor[-1]` | Count from end | `t[-1]` | Last element |\n",
    "| Boolean Indexing | `tensor[mask]` | Filter with condition | `t[t > 0]` | Elements > 0 |\n",
    "| Ellipsis | `tensor[...]` | All dimensions | `t[...,0]` | All dims except last |\n",
    "| Combined | `tensor[1:3,...,::2]` | Mix methods | `t[1:3,...,0]` | Complex selection |\n",
    "\n",
    "***\n",
    "> <img src=\"../figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 1**: Indexing a tensor\n",
    "\n",
    "```python\n",
    "# Get corners of a matrix\n",
    "corners = tensor[...,[0,-1]]  # First and last elements of last dimension\n",
    "\n",
    "# Get last row of a matrix\n",
    "last_row = tensor[-1,...]  # Last row of all columns\n",
    "\n",
    "# Extract diagonal\n",
    "diagonal = tensor.diagonal()  # More efficient than indexing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Grid Data (4×4): Temperature in °C\n",
      "tensor([[20.1000, 20.5000, 21.0000, 21.5000],\n",
      "        [19.8000, 20.2000, 20.8000, 21.2000],\n",
      "        [19.5000, 19.9000, 20.5000, 21.0000],\n",
      "        [19.2000, 19.6000, 20.2000, 20.8000]])\n",
      "\n",
      "Indexing Examples:\n",
      "============================================================\n",
      "Sensor at position (2,3): 21.0 °C\n",
      "\n",
      "Second row (all columns): tensor([19.8000, 20.2000, 20.8000, 21.2000])\n",
      "\n",
      "Last column (all rows): tensor([21.5000, 21.2000, 21.0000, 20.8000])\n",
      "\n",
      "Bottom-right 2×2:\n",
      "tensor([[20.5000, 21.0000],\n",
      "        [20.2000, 20.8000]])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample 4×4 data matrix (e.g., sensor grid readings)\n",
    "sensor_data = torch.tensor([\n",
    "    [20.1, 20.5, 21.0, 21.5],\n",
    "    [19.8, 20.2, 20.8, 21.2],\n",
    "    [19.5, 19.9, 20.5, 21.0],\n",
    "    [19.2, 19.6, 20.2, 20.8]\n",
    "])\n",
    "\n",
    "print(\"Sensor Grid Data (4×4): Temperature in °C\")\n",
    "print(sensor_data)\n",
    "print(\"\\nIndexing Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Single element\n",
    "value = sensor_data[2, 3]\n",
    "print(f\"Sensor at position (2,3): {value.item():.1f} °C\\n\")\n",
    "\n",
    "# 2. Entire row (all sensors in one row)\n",
    "second_row = sensor_data[1, :]\n",
    "print(f\"Second row (all columns): {second_row}\\n\")\n",
    "\n",
    "# 3. Entire column (one sensor over all rows)\n",
    "last_column = sensor_data[:, 3]\n",
    "print(f\"Last column (all rows): {last_column}\\n\")\n",
    "\n",
    "# 4. Submatrix (bottom-right 2×2 region)\n",
    "bottom_right = sensor_data[2:, 2:]\n",
    "print(f\"Bottom-right 2×2:\\n{bottom_right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Grid Data (4×4): Temperature in °C\n",
      "tensor([[20.1000, 20.5000, 21.0000, 21.5000],\n",
      "        [19.8000, 20.2000, 20.8000, 21.2000],\n",
      "        [19.5000, 19.9000, 20.5000, 21.0000],\n",
      "        [19.2000, 19.6000, 20.2000, 20.8000]])\n",
      "\n",
      "Indexing Examples:\n",
      "============================================================\n",
      "First row, every other element: tensor([20.1000, 21.0000])\n",
      "\n",
      "Hot spots (> 20.5°C):\n",
      "tensor([[False, False,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False,  True]])\n",
      "\n",
      "Values at hot spots: tensor([21.0000, 21.5000, 20.8000, 21.2000, 21.0000, 20.8000])\n",
      "\n",
      "Last element (bottom-right): 20.8 °C\n"
     ]
    }
   ],
   "source": [
    "print(\"Sensor Grid Data (4×4): Temperature in °C\")\n",
    "print(sensor_data)\n",
    "print(\"\\nIndexing Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 5. Striding (every other element in first row)\n",
    "every_other = sensor_data[0, ::2]\n",
    "print(f\"First row, every other element: {every_other}\\n\")\n",
    "\n",
    "# 6. Boolean masking (find hot spots > 20.5°C)\n",
    "hot_spots = sensor_data > 20.5\n",
    "print(f\"Hot spots (> 20.5°C):\\n{hot_spots}\")\n",
    "print(f\"\\nValues at hot spots: {sensor_data[hot_spots]}\")\n",
    "\n",
    "# 7. Negative indexing (last element)\n",
    "last_element = sensor_data[-1, -1]\n",
    "print(f\"\\nLast element (bottom-right): {last_element.item():.1f} °C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 4. Tensor operations\n",
    "***\n",
    "\n",
    "PyTorch allows us to manipulate tensors in different ways. Since PyTorch is built on top of NumPy, the same operations can be accessed through the `torch` module or alternatively through the `numpy` module. Due to the pythonic nature of PyTorch, we can also use the same operations as we would in Python.\n",
    "\n",
    "### Basic Operations Cheatsheet\n",
    "\n",
    "| Category | Description | Methods | PyTorch Method | Example |\n",
    "|----------|-------------|----------|----------------|---------|\n",
    "| Arithmetic | Basic math operations | +, -, *, /, ** | `add(), sub(), mul(), div(), pow(), sqrt()` | `a + b` |\n",
    "| Comparison | Compare values | >, <, ==, != | `gt(), lt(), eq(), ne()` | `a > 0` |\n",
    "| Reduction | Reduce dimensions | `sum(), mean(), max()` | `sum(), mean(), max()` | `a.sum()` |\n",
    "| Statistical | Statistical operations | `std(), var()` | `std(), var()` | `a.mean()` |\n",
    "\n",
    "***\n",
    "> <img src=\"../figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> 1. **Type Matching**: Ensure tensors have compatible data types\n",
    "> 2. **Shape Broadcasting**: Understand how PyTorch broadcasts shapes\n",
    "> 3. **GPU Memory**: Be careful with large tensor operations on GPU\n",
    "> 4. **Inplace Operations**: Use `_` suffix for inplace operations\n",
    "\n",
    "***\n",
    "> <img src=\"../figs/icons/list.svg\" width=\"20\" style=\"filter: invert(19%) sepia(75%) saturate(6158%) hue-rotate(312deg) brightness(87%) contrast(116%);\"/> **Common Mistakes to Avoid**: \n",
    "> - Mixing tensor types without conversion\n",
    "> - Forgetting to handle device placement (CPU/GPU)\n",
    "> - Not checking tensor shapes before operations\n",
    "> - Unnecessary copying of large tensors\n",
    "\n",
    "***\n",
    "> <img src=\"../figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 2**: Inplace operations\n",
    "   ```python\n",
    "   # Instead of: x = x + 1\n",
    "   x.add_(1)  # Inplace addition\n",
    "   y.add_(x)  # Inplace addition with another tensor\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Operations Examples\n",
      "============================================================\n",
      "Experiment A:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "Experiment B:\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "\n",
      "A + B (element-wise):\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n",
      "A * B (element-wise):\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "\n",
      "A @ B (matrix multiplication):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "√A:\n",
      "tensor([[1.0000, 1.4142],\n",
      "        [1.7321, 2.0000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor operations for scientific computing\n",
    "\n",
    "# Example: Two sets of experimental measurements\n",
    "experiment_A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "experiment_B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(\"Tensor Operations Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment A:\\n{experiment_A}\\n\")\n",
    "print(f\"Experiment B:\\n{experiment_B}\\n\")\n",
    "\n",
    "# Element-wise addition\n",
    "combined = experiment_A + experiment_B\n",
    "print(f\"A + B (element-wise):\\n{combined}\\n\")\n",
    "\n",
    "# Element-wise multiplication (Hadamard product)\n",
    "product = experiment_A * experiment_B\n",
    "print(f\"A * B (element-wise):\\n{product}\\n\")\n",
    "\n",
    "# Matrix multiplication (linear transformations)\n",
    "matmul_result = experiment_A @ experiment_B\n",
    "print(f\"A @ B (matrix multiplication):\\n{matmul_result}\\n\")\n",
    "\n",
    "# Square root (useful for RMS calculations)\n",
    "sqrt_A = torch.sqrt(experiment_A)\n",
    "print(f\"√A:\\n{sqrt_A}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of A: 2.50\n",
      "Standard deviation of A: 1.29\n",
      "Sum along rows: tensor([3., 7.])\n",
      "Max along columns: tensor([3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Statistical operations\n",
    "print(f\"Mean of A: {experiment_A.mean().item():.2f}\")\n",
    "print(f\"Standard deviation of A: {experiment_A.std().item():.2f}\")\n",
    "print(f\"Sum along rows: {experiment_A.sum(dim=1)}\")\n",
    "print(f\"Max along columns: {experiment_A.max(dim=0).values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Matrix operations\n",
    "***\n",
    "Matrix multiplication is a common operation in algebra and is used in many machine learning algorithms. We can perform:\n",
    "\n",
    "- **Matrix multiplication**: This is the standard matrix multiplication operation, which is denoted by the `@` operator in Python. This operation is also known as the dot product.\n",
    "- **Element-wise multiplication**: This is the multiplication of two matrices of the same shape, which is denoted by the `*` operator in Python. This operation is also known as the Hadamard product.\n",
    "- **Matrix transpose**: This is the operation of flipping a matrix over its diagonal, which is denoted by the `.T` attribute in Python. This operation is also known as the matrix transpose.\n",
    "- **Matrix inverse**: This is the operation of finding the inverse of a matrix, which is denoted by the `torch.inverse()` function in Python. This operation is also known as the matrix inverse.\n",
    "\n",
    "***\n",
    "| Operation | Description | Method | Example |\n",
    "|-----------|-------------|--------|---------|\n",
    "| Matrix Multiplication | Standard matrix product | @ or matmul() | `a @ b` |\n",
    "| Transpose | Flip matrix dimensions | .T or transpose() | `a.T` |\n",
    "| Inverse | Matrix inverse | inverse() | `torch.inverse(a)` |\n",
    "| Determinant | Matrix determinant | det() | `torch.det(a)` |\n",
    "| Eigenvalues | Eigenvalues and vectors | eig() | `torch.eig(a)` |\n",
    "| Singular Value Decomposition | SVD decomposition | svd() | `torch.svd(a)` |\n",
    "| Cholesky Decomposition | Cholesky factorization | cholesky() | `torch.cholesky(a)` |\n",
    "\n",
    "***\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto; display: flex; justify-content: center; align-items: center; overflow: hidden;\">\n",
    "    <img src=\"../figs\\matrix_mul.gif\" alt=\"Matrix Multiplication\" style=\"width: 40%; height: 220px; object-fit: cover;\">\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Operations for Linear Algebra\n",
      "============================================================\n",
      "Covariance Matrix:\n",
      "tensor([[4., 2.],\n",
      "        [2., 3.]])\n",
      "\n",
      "Transposed:\n",
      "tensor([[4., 2.],\n",
      "        [2., 3.]])\n",
      "\n",
      "Determinant: 8.0000\n",
      "Inverse:\n",
      "tensor([[ 0.3750, -0.2500],\n",
      "        [-0.2500,  0.5000]])\n",
      "\n",
      "A @ A^(-1) ≈ I:\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "\n",
      "A @ A:\n",
      "tensor([[20., 14.],\n",
      "        [14., 13.]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix operations for linear algebra (common in scientific computing)\n",
    "\n",
    "# Example: Covariance matrix from experimental data\n",
    "covariance_matrix = torch.tensor([[4.0, 2.0], [2.0, 3.0]], dtype=torch.float32)\n",
    "\n",
    "print(\"Matrix Operations for Linear Algebra\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Covariance Matrix:\\n{covariance_matrix}\\n\")\n",
    "\n",
    "# Matrix transpose\n",
    "transposed = covariance_matrix.T\n",
    "print(f\"Transposed:\\n{transposed}\\n\")\n",
    "\n",
    "# Matrix determinant (measure of volume/invertibility)\n",
    "det = torch.det(covariance_matrix)\n",
    "print(f\"Determinant: {det.item():.4f}\")\n",
    "\n",
    "# Matrix inverse (for solving linear systems)\n",
    "inverse = torch.inverse(covariance_matrix)\n",
    "print(f\"Inverse:\\n{inverse}\\n\")\n",
    "\n",
    "# Verify: A @ A^(-1) = I\n",
    "identity_check = covariance_matrix @ inverse\n",
    "print(f\"A @ A^(-1) ≈ I:\\n{identity_check}\\n\")\n",
    "\n",
    "# Matrix-matrix multiplication (transformations)\n",
    "transformation = covariance_matrix @ covariance_matrix\n",
    "print(f\"A @ A:\\n{transformation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tensor Broadcasting\n",
    "***\n",
    "\n",
    "> <img src=\"../figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: [Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) is a powerful feature of NumPy and PyTorch that allows us to perform operations on arrays of different shapes without having to explicitly reshape them. \n",
    "\n",
    "Since PyTorch is built on top of NumPy we can use its broadcasting capabilities. Broadcasting is how NumPy handles arrays with different shapes during arithmetic operations. It allows us to perform operations on arrays of different shapes without having to explicitly reshape them. This is done by automatically expanding the smaller array to match the shape of the larger array.\n",
    "\n",
    "For example, if we have a 1D array of shape `(3,)` and a 2D array of shape `(3, 2)`, we can add them together without having to reshape the 1D array. NumPy will automatically expand the 1D array to match the shape of the 2D array.\n",
    "\n",
    "***\n",
    "> <img src=\"../figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 3**: Broadcasting example\n",
    "\n",
    "```python\n",
    "# Create a 1D tensor of shape (3,)\n",
    "a = torch.tensor([1, 2, 3])\n",
    "# Create a 2D tensor of shape (3, 2)\n",
    "b = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "# Add the two tensors together\n",
    "c = a + b  # Broadcasting occurs here\n",
    "print(c)  # Output: tensor([[ 2,  4], [ 6,  8], [10, 12]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting Examples\n",
      "============================================================\n",
      "Raw sensor data:\n",
      "tensor([[10., 20.],\n",
      "        [30., 40.]])\n",
      "\n",
      "After subtracting offset 5.0:\n",
      "tensor([[ 5., 15.],\n",
      "        [25., 35.]])\n",
      "\n",
      "After per-column scaling tensor([1., 2.]):\n",
      "tensor([[10., 40.],\n",
      "        [30., 80.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting: Automatic shape alignment for operations\n",
    "\n",
    "# Example: Normalizing sensor data\n",
    "raw_data = torch.tensor([[10.0, 20.0], [30.0, 40.0]])\n",
    "offset = torch.tensor([5.0])  # Scalar to subtract\n",
    "scale = torch.tensor([1.0, 2.0])  # Per-column scaling\n",
    "\n",
    "print(\"Broadcasting Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Raw sensor data:\\n{raw_data}\\n\")\n",
    "\n",
    "# Broadcast scalar to matrix (subtract offset from all elements)\n",
    "centered = raw_data - offset\n",
    "print(f\"After subtracting offset {offset.item()}:\\n{centered}\\n\")\n",
    "\n",
    "# Broadcast row vector to matrix (per-column scaling)\n",
    "scaled = raw_data * scale\n",
    "print(f\"After per-column scaling {scale}:\\n{scaled}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting Examples\n",
      "============================================================\n",
      "Raw sensor data:\n",
      "tensor([[10., 20.],\n",
      "        [30., 40.]])\n",
      "\n",
      "Column means: tensor([[20., 30.]])\n",
      "\n",
      "Column stds: tensor([[14.1421, 14.1421]])\n",
      "\n",
      "Normalized data:\n",
      "tensor([[-0.7071, -0.7071],\n",
      "        [ 0.7071,  0.7071]])\n",
      "\n",
      "\n",
      "Normalized means: tensor([0., 0.])\n",
      "\n",
      "Normalized stds: tensor([0.7071, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "print(\"Broadcasting Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Raw sensor data:\\n{raw_data}\\n\")\n",
    "\n",
    "# More complex: normalize each column (mean=0, std=1)\n",
    "mean_per_column = raw_data.mean(dim=0, keepdim=True)\n",
    "std_per_column = raw_data.std(dim=0, keepdim=True)\n",
    "normalized = (raw_data - mean_per_column) / std_per_column\n",
    "\n",
    "print(f\"Column means: {mean_per_column}\")\n",
    "print(f\"\\nColumn stds: {std_per_column}\")\n",
    "print(f\"\\nNormalized data:\\n{normalized}\\n\")\n",
    "\n",
    "# Verify normalization\n",
    "print(f\"\\nNormalized means: {normalized.mean(dim=0)}\")\n",
    "print(f\"\\nNormalized stds: {normalized.std(dim=0, unbiased=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Reshaping Methods\n",
    "***\n",
    "Sometimes, we need to change the shape of a tensor without changing its data. We do this in order to prepare the tensor for a specific operation or to match the shape of another tensor. PyTorch provides several methods for reshaping tensors. Below is a table summarizing some of the most commonly used reshaping methods in PyTorch.\n",
    "\n",
    "| Method | Description | Example | Note |\n",
    "|--------|-------------|---------|------|\n",
    "| `reshape()` | New shape, maybe new memory | `x.reshape(2,3)` | May copy data |\n",
    "| `view()` | New shape, same memory | `x.view(2,3)` | Must be contiguous |\n",
    "| `squeeze()` | Remove single dims | `x.squeeze()` | Removes size 1 dims |\n",
    "| `unsqueeze()` | Add single dim | `x.unsqueeze(0)` | Adds size 1 dim |\n",
    "| `expand()` | Broadcast dimensions | `x.expand(2,3)` | No data copy |\n",
    "***\n",
    "\n",
    "> <img src=\"../figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 4**: Reshaping a tensor\n",
    "\n",
    "```python\n",
    "# Create a 1D tensor of shape (6,)\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "# Reshape to (2, 3)\n",
    "print(x.reshape(2, 3))  # Output: tensor([[1, 2, 3], [4, 5, 6]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping Examples\n",
      "============================================================\n",
      "Original time series: tensor([1., 2., 3., 4., 5., 6.]), shape: torch.Size([6])\n",
      "\n",
      "Reshaped to 2×3:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Reshaped to 1×2×3:\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.]]])\n",
      "\n",
      "Reshape with -1 (3, -1):\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "\n",
      "Original vector: torch.Size([3])\n",
      "Column vector: torch.Size([3, 1])\n",
      "Column vector:\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "\n",
      "After squeeze: torch.Size([3]), tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping tensors: crucial for data preparation\n",
    "\n",
    "# Example: 1D time series that needs to be reshaped for processing\n",
    "time_series = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"Reshaping Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original time series: {time_series}, shape: {time_series.shape}\\n\")\n",
    "\n",
    "# Reshape to 2D (e.g., for batch processing)\n",
    "reshaped_2x3 = time_series.reshape(2, 3)\n",
    "print(f\"Reshaped to 2×3:\\n{reshaped_2x3}\\n\")\n",
    "\n",
    "# Reshape to 3D (e.g., adding batch dimension)\n",
    "reshaped_3d = time_series.reshape(1, 2, 3)\n",
    "print(f\"Reshaped to 1×2×3:\\n{reshaped_3d}\\n\")\n",
    "\n",
    "# Use -1 to infer one dimension\n",
    "auto_reshape = time_series.reshape(3, -1)  # -1 infers size 2\n",
    "print(f\"Reshape with -1 (3, -1):\\n{auto_reshape}\\n\")\n",
    "\n",
    "# Unsqueeze: Add a dimension (useful for broadcasting)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "column_vector = vector.unsqueeze(1)  # Add dimension at position 1\n",
    "print(f\"Original vector: {vector.shape}\")\n",
    "print(f\"Column vector: {column_vector.shape}\")\n",
    "print(f\"Column vector:\\n{column_vector}\\n\")\n",
    "\n",
    "# Squeeze: Remove dimensions of size 1\n",
    "squeezed = column_vector.squeeze()\n",
    "print(f\"After squeeze: {squeezed.shape}, {squeezed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 5. Automatic Differentiation (Autograd)\n",
    "***\n",
    "\n",
    "Automatic differentiation is one of the most powerful features of PyTorch. It allows us to compute gradients automatically, which is essential for training neural networks. PyTorch uses a technique called **reverse mode differentiation** to compute gradients efficiently. This technique is based on the chain rule of calculus and allows us to compute gradients for complex functions with many variables.\n",
    "\n",
    "> <img src=\"../figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: [Autograd](https://pytorch.org/docs/stable/autograd.html) is the automatic differentiation engine in PyTorch. It provides a way to compute gradients automatically for tensors with `requires_grad=True`.\n",
    "\n",
    "Take for instance the following function:\n",
    "\n",
    "$$f(x) = x^2 + 42y^2 + 3$$\n",
    "\n",
    "where $x$ and $y$ are tensors. The gradient of this function with respect to $x$ and $y$ is given by:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = 2x$$\n",
    "$$\\frac{\\partial f}{\\partial y} = 84y$$\n",
    "\n",
    "using the chain rule of calculus. PyTorch allows us to compute these gradients automatically using the `backward()` method.\n",
    "\n",
    "The chain rule is a fundamental concept in calculus that allows us to compute the derivative of a composite function. It states that if we have two functions $f(x)$ and $g(x)$, then the derivative of their composition $f(g(x))$ is given by:\n",
    "\n",
    "$$\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "where $f'(g(x))$ is the derivative of $f$ with respect to $g$, and $g'(x)$ is the derivative of $g$ with respect to $x$. This means that we can compute the derivative of a composite function by computing the derivatives of its constituent functions and multiplying them together.\n",
    "\n",
    "## Autograd Concepts\n",
    "***\n",
    "\n",
    "In PyTorch, the autograd engine keeps track of all operations performed on tensors with `requires_grad=True`. It builds a computation graph dynamically as operations are performed. This graph is used to compute gradients when we call the `backward()` method.\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|---------|\n",
    "| `requires_grad` | Flag to track gradients | `x = torch.tensor(1.0, requires_grad=True)` |\n",
    "| `backward()` | Compute gradients | `y.backward()` |\n",
    "| `grad` | Access gradients | `x.grad` |\n",
    "***\n",
    "\n",
    "> <img src=\"../figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 5**: Using autograd to compute gradients\n",
    "\n",
    "```python\n",
    "# Create tensor with gradient tracking\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Compute function\n",
    "y = x * x\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "\n",
    "# Access gradient\n",
    "x.grad  # Should be 2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic Differentiation Example\n",
      "============================================================\n",
      "Function: y = 3x³ + 2x² - 5x + 1\n",
      "At x = 2.0:\n",
      "  y = 23.00\n",
      "  dy/dx = 39.00\n",
      "\n",
      "Analytical verification:\n",
      "  dy/dx = 9x² + 4x - 5\n",
      "  At x = 2: 9(4) + 4(2) - 5 = 36 + 8 - 5 = 39 ✓\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation: Essential for optimization and inverse problems\n",
    "\n",
    "# Example: Optimize a physical parameter\n",
    "# Suppose we have y = 3x³ + 2x² - 5x + 1\n",
    "# We want to find dy/dx at x = 2.0\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)  # Enable gradient tracking\n",
    "\n",
    "# Compute function (e.g., energy as a function of position)\n",
    "y = 3 * x**3 + 2 * x**2 - 5 * x + 1\n",
    "\n",
    "print(\"Automatic Differentiation Example\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Function: y = 3x³ + 2x² - 5x + 1\")\n",
    "print(f\"At x = {x.item():.1f}:\")\n",
    "print(f\"  y = {y.item():.2f}\")\n",
    "\n",
    "# Compute gradient automatically\n",
    "y.backward()\n",
    "\n",
    "print(f\"  dy/dx = {x.grad.item():.2f}\")\n",
    "print(\"\\nAnalytical verification:\")\n",
    "print(f\"  dy/dx = 9x² + 4x - 5\")\n",
    "print(f\"  At x = 2: 9(4) + 4(2) - 5 = 36 + 8 - 5 = 39 ✓\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 6. Converting Between NumPy and PyTorch\n",
    "***\n",
    "\n",
    "One of PyTorch's greatest strengths is seamless interoperability with NumPy. This allows you to:\n",
    "- Use existing NumPy-based code and data\n",
    "- Leverage PyTorch's GPU acceleration\n",
    "- Easily integrate with the scientific Python ecosystem (SciPy, Pandas, Matplotlib, etc.)\n",
    "\n",
    "> <img src=\"../figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Important**: NumPy arrays and PyTorch tensors can **share memory** on CPU, meaning changes to one affect the other (unless you explicitly copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy ↔ PyTorch Conversion\n",
      "============================================================\n",
      "NumPy → PyTorch:\n",
      "  NumPy array: [20.5 21.  19.8 22.1 20.3]\n",
      "  PyTorch tensor: tensor([20.5000, 21.0000, 19.8000, 22.1000, 20.3000], dtype=torch.float64)\n",
      "  Tensor dtype: torch.float64\n",
      "\n",
      "PyTorch → NumPy:\n",
      "  PyTorch tensor: tensor([1., 2., 3., 4.])\n",
      "  NumPy array: [1. 2. 3. 4.]\n",
      "  Array dtype: float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting between NumPy and PyTorch\n",
    "\n",
    "print(\"NumPy ↔ PyTorch Conversion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. NumPy to PyTorch\n",
    "# Use the temperature data from our earlier NumPy example\n",
    "numpy_temps = np.array([20.5, 21.0, 19.8, 22.1, 20.3])\n",
    "torch_temps = torch.from_numpy(numpy_temps)\n",
    "\n",
    "print(\"NumPy → PyTorch:\")\n",
    "print(f\"  NumPy array: {numpy_temps}\")\n",
    "print(f\"  PyTorch tensor: {torch_temps}\")\n",
    "print(f\"  Tensor dtype: {torch_temps.dtype}\\n\")\n",
    "\n",
    "# 2. PyTorch to NumPy\n",
    "torch_data = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "numpy_data = torch_data.numpy()\n",
    "\n",
    "print(\"PyTorch → NumPy:\")\n",
    "print(f\"  PyTorch tensor: {torch_data}\")\n",
    "print(f\"  NumPy array: {numpy_data}\")\n",
    "print(f\"  Array dtype: {numpy_data.dtype}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Sharing:\n",
      "Original NumPy: [1. 2. 3.]\n",
      "PyTorch tensor: tensor([1., 2., 3.], dtype=torch.float64)\n",
      "\n",
      "After modifying NumPy[0] = 999:\n",
      "  NumPy: [999.   2.   3.]\n",
      "  PyTorch: tensor([999.,   2.,   3.], dtype=torch.float64) (changed too!)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Memory sharing demonstration\n",
    "print(\"Memory Sharing:\")\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"Original NumPy: {np_array}\")\n",
    "print(f\"PyTorch tensor: {torch_tensor}\")\n",
    "\n",
    "# Modify NumPy array\n",
    "np_array[0] = 999.0\n",
    "print(f\"\\nAfter modifying NumPy[0] = 999:\")\n",
    "print(f\"  NumPy: {np_array}\")\n",
    "print(f\"  PyTorch: {torch_tensor} (changed too!)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 7. Working with Scientific Datasets: Higgs Boson\n",
    "***\n",
    "\n",
    "Now let's apply what we've learned to a real scientific dataset. We'll use the [Higgs Boson Dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS) from the UCI Machine Learning Repository.\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "The dataset contains 11 million Monte Carlo-generated collision events from the CERN Large Hadron Collider. Each event has:\n",
    "- **31 features**: 21 low-level kinematic properties and 7 high-level features derived by physicists\n",
    "- **1 label**: Binary classification (signal or background)\n",
    "\n",
    "**Scientific Context**: The Higgs boson is a fundamental particle discovered at CERN in 2012. Identifying Higgs events among background noise is a classic classification problem in high-energy physics.\n",
    "\n",
    "> <img src=\"../figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Note**: We'll load only the first 10,000 rows for this workshop to keep things fast.\n",
    "\n",
    "## 7.1 Loading the Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Higgs Boson Dataset...\n",
      "============================================================\n",
      "✓ Dataset loaded successfully!\n",
      "\n",
      "Dataset Shape: (10000, 33)\n",
      "  Samples: 10000\n",
      "  Features: 32\n",
      "  Labels: 2 classes\n",
      "\n",
      "Class distribution:\n",
      "Label\n",
      "b    6628\n",
      "s    3372\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Higgs Boson dataset\n",
    "# This may take a moment as we're downloading from UCI repository\n",
    "\n",
    "url = \"https://github.com/CLDiego/uom_fse_dl_workshop/raw/refs/heads/main/training_data/HIGGS.zip\"\n",
    "\n",
    "print(\"Loading Higgs Boson Dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Default column names (used if download fails)\n",
    "column_names = ['label'] + [f'feature_{i}' for i in range(1, 29)]\n",
    "\n",
    "# Load first 10,000 rows using the first row as header\n",
    "try:\n",
    "    higgs_df = pd.read_csv(url, nrows=10000, compression='zip', header=0)\n",
    "    column_names = list(higgs_df.columns)\n",
    "\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Falling back to sample data...\")\n",
    "    # Create sample data if download fails\n",
    "    higgs_df = pd.DataFrame(\n",
    "        np.random.randn(10000, 29),\n",
    "        columns=column_names\n",
    "    )\n",
    "    higgs_df['label'] = np.random.randint(0, 2, 10000)\n",
    "\n",
    "print(f\"\\nDataset Shape: {higgs_df.shape}\")\n",
    "print(f\"  Samples: {len(higgs_df)}\")\n",
    "print(f\"  Features: {len(higgs_df.columns) - 1}\")\n",
    "print(f\"  Labels: {higgs_df['Label'].nunique()} classes\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(higgs_df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "\n",
    "Let's examine the structure and statistics of our particle physics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>2.233584</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.347389</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>5.446378</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.245333</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  ...  PRI_jet_num  PRI_jet_leading_pt  \\\n",
       "0               3.064      41.928  ...            2              67.435   \n",
       "1               3.473       2.078  ...            1              46.226   \n",
       "2               3.148       9.336  ...            1              44.251   \n",
       "3               3.310       0.414  ...            0            -999.000   \n",
       "4               3.891      16.405  ...            0            -999.000   \n",
       "\n",
       "   PRI_jet_leading_eta  PRI_jet_leading_phi  PRI_jet_subleading_pt  \\\n",
       "0                2.150                0.444                 46.062   \n",
       "1                0.725                1.158               -999.000   \n",
       "2                2.053               -2.028               -999.000   \n",
       "3             -999.000             -999.000               -999.000   \n",
       "4             -999.000             -999.000               -999.000   \n",
       "\n",
       "   PRI_jet_subleading_eta  PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  \\\n",
       "0                    1.24                  -2.475         113.497  0.002653   \n",
       "1                 -999.00                -999.000          46.226  2.233584   \n",
       "2                 -999.00                -999.000          44.251  2.347389   \n",
       "3                 -999.00                -999.000          -0.000  5.446378   \n",
       "4                 -999.00                -999.000           0.000  6.245333   \n",
       "\n",
       "   Label  \n",
       "0      s  \n",
       "1      b  \n",
       "2      b  \n",
       "3      b  \n",
       "4      b  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "higgs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Summary (first 5 features):\n",
      "============================================================\n",
      "            EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
      "count   10000.00000  10000.000000                 10000.000000  10000.000000   \n",
      "mean   104999.50000    -45.304318                    49.132840     80.703992   \n",
      "std      2886.89568    401.854494                    34.525596     39.169303   \n",
      "min    100000.00000   -999.000000                     0.005000      7.330000   \n",
      "25%    102499.75000     78.232500                    19.930750     59.045750   \n",
      "50%    104999.50000    104.787000                    46.725500     73.537500   \n",
      "75%    107499.25000    130.504000                    73.176250     92.353000   \n",
      "max    109999.00000    813.396000                   444.719000    651.561000   \n",
      "\n",
      "           DER_pt_h  DER_deltaeta_jet_jet  \n",
      "count  10000.000000          10000.000000  \n",
      "mean      57.906264           -704.783037  \n",
      "std       68.997076            456.171676  \n",
      "min        0.000000           -999.000000  \n",
      "25%       12.875250           -999.000000  \n",
      "50%       38.310000           -999.000000  \n",
      "75%       79.167250              0.569000  \n",
      "max     2834.999000              8.459000  \n"
     ]
    }
   ],
   "source": [
    "print(\"Statistical Summary (first 5 features):\")\n",
    "print(\"=\" * 60)\n",
    "print(higgs_df.iloc[:, :6].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Converting Data to Tensors\n",
    "\n",
    "To use PyTorch, we need to convert our Pandas DataFrame to tensors. We'll separate:\n",
    "- **Features (X)**: The kinematic measurements\n",
    "- **Labels (y)**: The binary class (signal=1, background=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Conversion to Tensors\n",
      "============================================================\n",
      "Features (X):\n",
      "  Shape: torch.Size([10000, 32]) (samples × features)\n",
      "  Data type: torch.float32\n",
      "  Min: -999.0000, Max: 109999.0000\n",
      "\n",
      "Labels (y):\n",
      "  Shape: torch.Size([10000])\n",
      "  Data type: torch.int64\n",
      "  Unique values: tensor([0, 1])\n",
      "\n",
      "Example collision event (first sample):\n",
      "  Features: tensor([1.0000e+05, 1.3847e+02, 5.1655e+01, 9.7827e+01, 2.7980e+01]) ... (showing first 5 of 28)\n",
      "  Label: 1 (signal)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and labels\n",
    "X = higgs_df.drop('Label', axis=1).values  # All features as NumPy array\n",
    "y = higgs_df['Label'].values  # Labels as NumPy array\n",
    "\n",
    "# Convert labels from b and s to 0 and 1 \n",
    "y = np.where(y == 's', 1, 0)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X).float()  # Features as float32\n",
    "y_tensor = torch.from_numpy(y).long()   # Labels as long (for classification)\n",
    "\n",
    "print(\"Dataset Conversion to Tensors\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features (X):\")\n",
    "print(f\"  Shape: {X_tensor.shape} (samples × features)\")\n",
    "print(f\"  Data type: {X_tensor.dtype}\")\n",
    "print(f\"  Min: {X_tensor.min().item():.4f}, Max: {X_tensor.max().item():.4f}\")\n",
    "\n",
    "print(f\"\\nLabels (y):\")\n",
    "print(f\"  Shape: {y_tensor.shape}\")\n",
    "print(f\"  Data type: {y_tensor.dtype}\")\n",
    "print(f\"  Unique values: {torch.unique(y_tensor)}\")\n",
    "\n",
    "# Show a single sample\n",
    "print(f\"\\nExample collision event (first sample):\")\n",
    "print(f\"  Features: {X_tensor[0, :5]} ... (showing first 5 of 28)\")\n",
    "print(f\"  Label: {y_tensor[0].item()} ({'signal' if y_tensor[0] == 1 else 'background'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 8. Creating a Custom Dataset Class\n",
    "\n",
    "For more complex data handling, PyTorch provides the `torch.utils.data.Dataset` class. By creating a custom dataset class, we can:\n",
    "- Load data on-demand (useful for large datasets that don't fit in memory)\n",
    "- Apply transformations consistently\n",
    "- Integrate seamlessly with PyTorch's `DataLoader` for batching\n",
    "\n",
    "> <img src=\"../figs/icons/docs.svg\" width=\"20\" style=\"filter: invert(50%) sepia(50%) saturate(2000%) hue-rotate(90deg) brightness(915%) contrast(100%);\"/> **Documentation**: See the [PyTorch Data Tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more details.\n",
    "\n",
    "A custom Dataset class must implement:\n",
    "1. `__init__`: Initialize the dataset (load file paths, read metadata, etc.)\n",
    "2. `__len__`: Return the number of samples\n",
    "3. `__getitem__`: Return a single sample given an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom HiggsDataset Created\n",
      "============================================================\n",
      "Dataset size: 10000 samples\n",
      "\n",
      "Accessing samples:\n",
      "\n",
      "Sample 0:\n",
      "  Features shape: torch.Size([32])\n",
      "  First 5 features: tensor([1.0000e+05, 1.3847e+02, 5.1655e+01, 9.7827e+01, 2.7980e+01])\n",
      "  Label: 1 (signal)\n",
      "\n",
      "Sample 1:\n",
      "  Features shape: torch.Size([32])\n",
      "  First 5 features: tensor([1.0000e+05, 1.6094e+02, 6.8768e+01, 1.0324e+02, 4.8146e+01])\n",
      "  Label: 0 (background)\n",
      "\n",
      "Sample 2:\n",
      "  Features shape: torch.Size([32])\n",
      "  First 5 features: tensor([ 1.0000e+05, -9.9900e+02,  1.6217e+02,  1.2595e+02,  3.5635e+01])\n",
      "  Label: 0 (background)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HiggsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Higgs Boson particle collision data.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame with 'label' column and feature columns\n",
    "            transform: Optional transform to apply to features (e.g., normalization)\n",
    "        \"\"\"\n",
    "        self.features = torch.from_numpy(\n",
    "            dataframe.drop('Label', axis=1).values\n",
    "        ).float()\n",
    "        self.labels = torch.from_numpy(\n",
    "            dataframe['Label'].apply(lambda x: 1 if x == 's' else 0).values\n",
    "        ).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample (features, label) at index idx.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (features, label) where features is a tensor of shape (28,)\n",
    "                   and label is a scalar tensor\n",
    "        \"\"\"\n",
    "        features = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transformation if specified\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "            \n",
    "        return features, label\n",
    "\n",
    "# Create dataset instance\n",
    "higgs_dataset = HiggsDataset(higgs_df)\n",
    "\n",
    "print(\"Custom HiggsDataset Created\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset size: {len(higgs_dataset)} samples\")\n",
    "print(f\"\\nAccessing samples:\")\n",
    "\n",
    "# Access individual samples\n",
    "for i in range(3):\n",
    "    features, label = higgs_dataset[i]\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Features shape: {features.shape}\")\n",
    "    print(f\"  First 5 features: {features[:5]}\")\n",
    "    print(f\"  Label: {label.item()} ({'signal' if label == 1 else 'background'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Using DataLoader for Batching\n",
    "\n",
    "The `DataLoader` wraps a Dataset and provides:\n",
    "- **Batching**: Combine multiple samples into batches\n",
    "- **Shuffling**: Randomize sample order (important for training)\n",
    "- **Parallel loading**: Use multiple workers for faster data loading\n",
    "- **Automatic batching**: Handles stacking samples into batch tensors\n",
    "\n",
    "This is essential for training neural networks, where we process data in mini-batches rather than one sample at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader Configuration\n",
      "============================================================\n",
      "Batch size: 64\n",
      "Number of batches: 157\n",
      "Total samples: 10000\n",
      "\n",
      "First Batch:\n",
      "  Features batch shape: torch.Size([64, 32]) (batch × features)\n",
      "  Labels batch shape: torch.Size([64])\n",
      "\n",
      "  Sample features from batch:\n",
      "tensor([[ 1.0374e+05, -9.9900e+02,  1.0195e+02,  7.7436e+01,  2.8670e+01],\n",
      "        [ 1.0929e+05,  1.6261e+02,  2.0210e+01,  1.1138e+02,  4.3890e+00]])\n",
      "\n",
      "  Sample labels from batch:\n",
      "  tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1])\n",
      "\n",
      "============================================================\n",
      "✓ Data pipeline ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(\n",
    "    higgs_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Randomize order\n",
    "    num_workers=0  # Set to 0 for compatibility; increase for parallel loading\n",
    ")\n",
    "\n",
    "print(\"DataLoader Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Total samples: {len(higgs_dataset)}\")\n",
    "\n",
    "# Iterate through the first batch\n",
    "for batch_idx, (features_batch, labels_batch) in enumerate(dataloader):\n",
    "    if batch_idx == 0:  # Only show first batch\n",
    "        print(f\"\\nFirst Batch:\")\n",
    "        print(f\"  Features batch shape: {features_batch.shape} (batch × features)\")\n",
    "        print(f\"  Labels batch shape: {labels_batch.shape}\")\n",
    "        print(f\"\\n  Sample features from batch:\")\n",
    "        pprint(features_batch[:2, :5])  # First 2 samples, first 5 features\n",
    "        print(f\"\\n  Sample labels from batch:\")\n",
    "        print(f\"  {labels_batch[:10]}\")  # First 10 labels\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Data pipeline ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 9. GPU Acceleration\n",
    "***\n",
    "\n",
    "One of PyTorch's main advantages is seamless GPU support. GPUs (Graphics Processing Units) excel at parallel computations, making them ideal for:\n",
    "- Large matrix operations\n",
    "- Deep learning model training\n",
    "- Monte Carlo simulations\n",
    "- Image processing\n",
    "- Any computation that can be parallelized\n",
    "\n",
    "## Moving Tensors to GPU\n",
    "\n",
    "To use the GPU, we need to move tensors from CPU to GPU memory using the `.to()` method.\n",
    "\n",
    "> <img src=\"../figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Important**: All tensors in an operation must be on the same device (CPU or GPU). Mixing devices will cause errors.\n",
    "\n",
    "> <img src=\"../figs/icons/idea.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Tip**: For code that works on both CPU and GPU, use `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` and move all tensors to `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Acceleration Setup\n",
      "============================================================\n",
      "Device: cpu\n",
      "No GPU available - computations will run on CPU\n",
      "\n",
      "Original tensor device: cpu\n",
      "After .to(device): cpu\n",
      "Result device: cpu\n",
      "Result shape: torch.Size([1000, 1000])\n",
      "\n",
      "Moved back to CPU: cpu\n",
      "\n",
      "============================================================\n",
      "✓ GPU operations complete!\n"
     ]
    }
   ],
   "source": [
    "# Check device availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"GPU Acceleration Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available - computations will run on CPU\")\n",
    "\n",
    "# Example: Move a tensor to the device\n",
    "data = torch.randn(1000, 1000)\n",
    "print(f\"\\nOriginal tensor device: {data.device}\")\n",
    "\n",
    "# Move to GPU (if available)\n",
    "data_gpu = data.to(device)\n",
    "print(f\"After .to(device): {data_gpu.device}\")\n",
    "\n",
    "# Perform computation on the device\n",
    "result = data_gpu @ data_gpu.T  # Matrix multiplication\n",
    "print(f\"Result device: {result.device}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "\n",
    "# Move back to CPU (needed for NumPy conversion)\n",
    "result_cpu = result.cpu()\n",
    "print(f\"\\nMoved back to CPU: {result_cpu.device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ GPU operations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use GPU vs CPU\n",
    "\n",
    "> <img src=\"../figs/icons/idea.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Decision Guide**:\n",
    "\n",
    "**Use GPU for**:\n",
    "- Training deep neural networks\n",
    "- Large matrix operations (size > 1000×1000)\n",
    "- Batch processing of images/signals\n",
    "- Long-running simulations\n",
    "- Operations repeated many times\n",
    "\n",
    "**Use CPU for**:\n",
    "- Small computations (overhead of GPU transfer isn't worth it)\n",
    "- Operations not parallelizable\n",
    "- Debugging (easier to inspect values)\n",
    "- When GPU memory is limited\n",
    "\n",
    "> <img src=\"../figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Device Consistency Rule**: All tensors and models in an operation must be on the same device. Common pattern:\n",
    "\n",
    "```python\n",
    "# Good practice: move everything to device at the start\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Now operations work\n",
    "output = model(x)\n",
    "loss = loss_fn(output, y)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uom-fse-dl-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
