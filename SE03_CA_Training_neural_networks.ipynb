{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/se_03.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/solutions/SE03_SL_Training_neural_networks.ipynb)\n",
    "\n",
    "## Workshop Overview\n",
    "***\n",
    "In this workshop, we explore autoencoders for unsupervised anomaly detection using real-world medical data. You'll learn how to build neural networks that learn to compress and reconstruct data, then use reconstruction error to identify abnormal patterns in heart sounds.\n",
    "\n",
    "**Prerequisites**: Neural network basics (SE02), PyTorch fundamentals (SE01)\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand the PyTorch training workflow from data to deployment\n",
    "- Build and train autoencoders for dimensionality reduction\n",
    "- Apply autoencoders for anomaly detection in medical audio\n",
    "- Learn data preprocessing techniques for audio signals\n",
    "- Implement proper train/validation/test splits\n",
    "- Monitor and prevent overfitting using validation curves\n",
    "\n",
    "**Clinical Context**: Early detection of cardiovascular and pulmonary diseases through heart and lung sound analysis can significantly improve patient outcomes. This workshop demonstrates how deep learning can assist in identifying abnormal patterns that may indicate underlying health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab: downloading utils...\")\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt\",\n",
    "        \"-O\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"-x\",\n",
    "        \"-nH\",\n",
    "        \"--cut-dirs=3\",\n",
    "        \"-i\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(\"Running locally: skipping Colab utils download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Setup paths for helper utilities\n",
    "helper_utils = Path(Path.cwd().parent)\n",
    "if str(helper_utils) not in sys.path:\n",
    "    sys.path.append(str(helper_utils))\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Utilities\n",
    "import utils\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch Setup Information\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âœ— No GPU detected - using CPU\")\n",
    "    print(\"  (For GPU: Runtime > Change runtime type > Hardware accelerator > GPU)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"30\"/> 1. The PyTorch Workflow\n",
    "***\n",
    "\n",
    "Most deep learning projects follow a systematic workflow. Whether you're building image classifiers, time series predictors, or anomaly detectors, the fundamental steps remain consistent. Understanding this workflow helps you structure your projects effectively.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/pytorch_workflow.png\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "## The Complete Workflow\n",
    "\n",
    "| Step | Description | Key Considerations |\n",
    "|------|------------|-------------------|\n",
    "| ðŸ“Š **Obtain Data** | Collect and explore your dataset | Data quality, source reliability, licensing |\n",
    "| ðŸ”§ **Prepare Data** | Convert to PyTorch-compatible format | Tensors, data types, shapes |\n",
    "| ðŸ§¹ **Preprocess Data** | Clean, normalize, and split data | Train/val/test splits, normalization strategy |\n",
    "| ðŸ“‰ **Choose Loss Function** | Define what to minimize | MSE for regression, CE for classification |\n",
    "| âš¡ **Activation Function** | Choose non-linearity for neurons | ReLU for hidden layers, task-specific for output |\n",
    "| ðŸ—ï¸ **Model Architecture** | Define network structure | Layer types, sizes, depth |\n",
    "| ðŸŽ¯ **Choose Optimizer** | Select parameter update algorithm | Adam, SGD, learning rate |\n",
    "| ðŸ”„ **Create Training Loop** | Implement forward/backward passes | Gradient computation, parameter updates |\n",
    "| ðŸ‹ï¸ **Fit Model** | Train on data | Epochs, batch size, early stopping |\n",
    "| ðŸ“ˆ **Evaluate Model** | Test performance | Validation metrics, generalization |\n",
    "| âš™ï¸ **Improve Model** | Fine-tune and optimize | Hyperparameter tuning, regularization |\n",
    "| ðŸ’¾ **Save/Deploy Model** | Productionize your solution | Model serialization, deployment strategy |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Remember**: This workflow is iterative. You'll often cycle back to earlier steps as you refine your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Obtain Data\n",
    "***\n",
    "\n",
    "### The Heart and Lung Sounds Dataset (HLS-CMDS)\n",
    "\n",
    "In this workshop, we'll use the [Heart and Lung Sounds - Clinical Manikin Digital Stethoscope (HLS-CMDS) dataset](https://github.com/Torabiy/HLS-CMDS). This dataset contains 535 recordings of heart and lung sounds captured using a digital stethoscope from a clinical manikin, simulating real physiological conditions.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Dataset Overview**:\n",
    "> - **Total Recordings**: 535 audio files (.wav format)\n",
    "> - **Recording Types**: 50 heart sounds, 50 lung sounds, 145 mixed sounds (+ 145 source heart + 145 source lung)\n",
    "> - **Auscultation Locations**: 12 chest landmarks\n",
    "> - **Sample Rate**: Varies by recording\n",
    "> - **Duration**: Typically 5-10 seconds per recording\n",
    "\n",
    "### Sound Categories\n",
    "\n",
    "The dataset includes both **normal** and **abnormal** cardiopulmonary sounds:\n",
    "\n",
    "| Category | Sound Types | Clinical Significance |\n",
    "|----------|-------------|----------------------|\n",
    "| **Normal Heart** | Regular cardiac rhythm | Healthy baseline |\n",
    "| **Heart Abnormalities** | Murmurs (early/mid/late systolic, diastolic), Third/Fourth heart sounds, Atrial fibrillation, Tachycardia, AV block | Potential valve disease, arrhythmias, heart failure |\n",
    "| **Normal Lung** | Clear breath sounds | Healthy baseline |\n",
    "| **Lung Abnormalities** | Wheezing, Crackles (fine/coarse), Rhonchi, Pleural rub | Asthma, pneumonia, bronchitis, pleural inflammation |\n",
    "\n",
    "### Why Autoencoders for Anomaly Detection?\n",
    "\n",
    "Traditional supervised learning requires labeled examples of every abnormality type. However:\n",
    "\n",
    "1. **Normal data is abundant** - healthy recordings are easy to collect\n",
    "2. **Abnormalities are rare** - pathological sounds are less common\n",
    "3. **Unknown patterns exist** - new or rare conditions may not be in training data\n",
    "\n",
    "**Autoencoders solve this** by learning to reconstruct *normal* sounds. When presented with abnormal sounds, they produce higher reconstruction errors, flagging them as anomalies.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Key Insight**: We'll train our autoencoder ONLY on normal heart sounds (filtering the Mix dataset), then use reconstruction error as an anomaly score. This is critical for anomaly detection - the model must learn what \"normal\" looks like, not abnormal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup the HLS-CMDS dataset\n",
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('HLS-CMDS',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True,\n",
    "                                   remove_compressed=True)\n",
    "\n",
    "print(f\"âœ… Dataset downloaded to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the dataset\n",
    "print(\"ðŸ“‚ Dataset structure:\")\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    level = root.replace(str(dataset_path), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    \n",
    "    # Show CSV files and a few WAV files\n",
    "    csv_files = [f for f in files if f.endswith('.csv')]\n",
    "    wav_files = [f for f in files if f.endswith('.wav') or f.endswith('.WAV')]\n",
    "    \n",
    "    for file in csv_files:\n",
    "        print(f'{subindent}{file}')\n",
    "    \n",
    "    if wav_files:\n",
    "        for file in wav_files[:3]:\n",
    "            print(f'{subindent}{file}')\n",
    "        if len(wav_files) > 3:\n",
    "            print(f'{subindent}... and {len(wav_files) - 3} more WAV files')\n",
    "    \n",
    "    if level > 1:  # Limit depth\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING METADATA AND AUDIO FILES\")\n",
    "print(\"=\" * 70)\n",
    "# Load metadata CSV files\n",
    "heart_metadata_path = dataset_path / 'HS.csv'\n",
    "mix_metadata_path = dataset_path / 'Mix.csv'\n",
    "\n",
    "# Get paths to audio folders\n",
    "heart_sounds_folder = dataset_path / 'HS'\n",
    "mix_sounds_folder = dataset_path / 'Mix'\n",
    "\n",
    "# Load Mix metadata (contains 145 heart, 145 lung, 145 mixed sounds)\n",
    "if mix_metadata_path.exists():\n",
    "    mix_metadata = pd.read_csv(mix_metadata_path)\n",
    "    print(f\"\\nMix dataset loaded:\")\n",
    "    print(f\"   Total recordings: {len(mix_metadata)}\")\n",
    "    print(f\"   Columns: {list(mix_metadata.columns)}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Warning: Mix.csv not found!\")\n",
    "\n",
    "# Load HS metadata for testing\n",
    "if heart_metadata_path.exists():\n",
    "    heart_metadata = pd.read_csv(heart_metadata_path)\n",
    "    print(f\"\\n Heart sounds (HS) dataset loaded:\")\n",
    "    print(f\"   Total recordings: {len(heart_metadata)}\")\n",
    "    print(f\" Columns: {list(heart_metadata.columns)}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Warning: HS.csv not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "***\n",
    "\n",
    "Before diving into model building, it's essential to explore the dataset. This helps us understand the data distribution, identify any issues, and plan our preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random heart sound recording and its mel spectrogram\n",
    "random_id = random.choice(heart_metadata['Heart Sound ID'])\n",
    "print(f\"Randomly selected Heart Sound ID: {random_id}\")\n",
    "random_audio_path = heart_sounds_folder / (random_id + \".wav\")\n",
    "\n",
    "display(ipd.Audio(str(random_audio_path), autoplay=True))\n",
    "\n",
    "y, sr = librosa.load(random_audio_path, sr=None)\n",
    "S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots with plotly for interactive visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(f'{random_id} - Waveform'.upper(), f'{random_id} - Mel Spectrogram'.upper()),\n",
    "    column_widths=[0.5, 0.75]\n",
    ")\n",
    "\n",
    "# Waveform\n",
    "time_axis = np.linspace(0, len(y) / sr, len(y))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=time_axis,\n",
    "        y=y,\n",
    "        mode='lines',\n",
    "        line=dict(color='steelblue', width=1),\n",
    "        name='Waveform'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Mel Spectrogram\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=S_dB,\n",
    "        x=librosa.frames_to_time(np.arange(S_dB.shape[1]), sr=sr),\n",
    "        y=librosa.mel_frequencies(n_mels=S_dB.shape[0]),\n",
    "        colorbar=dict(title='dB', x=1.15),\n",
    "        name='Mel Spectrogram'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text='Time (s)'.upper(), row=1, col=1)\n",
    "fig.update_yaxes(title_text='Amplitude'.upper(), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Time (s)'.upper(), row=1, col=2)\n",
    "fig.update_yaxes(title_text='Frequency (Hz)'.upper(), type='log', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive bar chart with overlapping bars\n",
    "mix_counts = mix_metadata['Heart Sound Type'].value_counts()\n",
    "heart_counts = heart_metadata['Heart Sound Type'].value_counts()\n",
    "\n",
    "# Get all unique heart sound types from both datasets\n",
    "all_types = sorted(set(mix_counts.index).union(set(heart_counts.index)))\n",
    "\n",
    "# Create aligned data for both datasets\n",
    "mix_values = [mix_counts.get(t, 0) for t in all_types]\n",
    "heart_values = [heart_counts.get(t, 0) for t in all_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Mix dataset (more transparent, behind)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=all_types, \n",
    "        y=mix_values, \n",
    "        name=\"Mix Dataset\",\n",
    "        marker_color='steelblue',\n",
    "        opacity=0.6\n",
    "    )\n",
    ")\n",
    "\n",
    "# HS dataset (less transparent, in front)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=all_types, \n",
    "        y=heart_values, \n",
    "        name=\"HS Dataset\",\n",
    "        marker_color='coral',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Heart Sound Distribution Comparison\".upper(),\n",
    "    xaxis_title=\"Heart Sound Type\".upper(),\n",
    "    yaxis_title=\"Count\".upper(),\n",
    "    barmode='overlay',  # Overlay bars instead of grouping\n",
    "    height=500,\n",
    "    xaxis={'tickangle': -45}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recording lengths in Mix dataset (normal sounds only)\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYZING RECORDING LENGTHS FOR DATA AUGMENTATION PLANNING...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "normal_mix_files = mix_metadata[mix_metadata['Heart Sound Type'] == 'Normal']['Heart Sound ID'].tolist()\n",
    "\n",
    "durations = []\n",
    "sample_rates = []\n",
    "\n",
    "for sound_id in normal_mix_files:\n",
    "    sound_id = str(sound_id).strip()\n",
    "    if sound_id == 'nan' or pd.isna(sound_id):\n",
    "        continue\n",
    "        \n",
    "    file_path = mix_sounds_folder / (sound_id + '.wav')\n",
    "    if file_path.exists():\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        duration = len(audio) / sr\n",
    "        durations.append(duration)\n",
    "        sample_rates.append(sr)\n",
    "\n",
    "durations = np.array(durations)\n",
    "print(f\"Normal heart sound recordings from Mix dataset:\")\n",
    "print(f\"   Number of recordings: {len(durations)}\")\n",
    "print(f\"   Duration - Mean: {durations.mean():.2f}s, Std: {durations.std():.2f}s\")\n",
    "print(f\"   Duration - Min: {durations.min():.2f}s, Max: {durations.max():.2f}s\")\n",
    "print(f\"   Sample rates: {set(sample_rates)}\")\n",
    "print(f\"\\nðŸ’¡ Strategy: Slice recordings into overlapping segments for augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation for Small Datasets\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Why Augmentation?**: With only 13 normal training samples, we risk overfitting. Data augmentation artificially increases the dataset size by creating variations of existing samples.\n",
    "\n",
    "**Audio Segmentation Strategy**\n",
    "\n",
    "Since heart sounds are cyclical, we can slice long recordings into shorter segments. Each segment captures multiple heartbeat cycles and can be treated as an independent sample.\n",
    "\n",
    "| Augmentation Parameter | Value | Rationale |\n",
    "|------------------------|-------|-----------|\n",
    "| **Original Duration** | 15 seconds | All normal recordings are 15s |\n",
    "| **Segment Duration** | 3 seconds | Captures 3 heartbeats at 60 BPM |\n",
    "| **Overlap** | 1.5 seconds (50%) | Ensures no heartbeats are cut off |\n",
    "| **Segments per Recording** | ~9 segments | With 50% overlap from 15s |\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "For a recording of duration $T$ with segment length $L$ and overlap $O$:\n",
    "\n",
    "$$N_{segments} = \\lfloor \\frac{T - L}{L - O} \\rfloor + 1$$\n",
    "\n",
    "For our case: $N_{segments} = \\lfloor \\frac{15 - 3}{3 - 1.5} \\rfloor + 1 = 9$ segments\n",
    "\n",
    "**Expected Augmentation Results:**\n",
    "- Original: 13 samples\n",
    "- After augmentation: 13 Ã— 9 = **117 samples** \n",
    "- Increase: **9Ã— more training data** ðŸš€\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Important**: Each segment still represents a \"normal\" heart sound pattern, maintaining label integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Functions\n",
    "def slice_audio_into_segments(audio, sr, segment_duration=3.0, overlap=0.5):\n",
    "    \"\"\"\n",
    "    Slice an audio signal into overlapping segments\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio signal (numpy array)\n",
    "        sr: Sample rate\n",
    "        segment_duration: Duration of each segment in seconds\n",
    "        overlap: Overlap fraction (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of audio segments\n",
    "    \"\"\"\n",
    "    segment_samples = int(segment_duration * sr)\n",
    "    hop_samples = int(segment_samples * (1 - overlap))\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    \n",
    "    while start + segment_samples <= len(audio):\n",
    "        segment = audio[start:start + segment_samples]\n",
    "        segments.append(segment)\n",
    "        start += hop_samples\n",
    "    \n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Not Use Raw Audio Directly?\n",
    "***\n",
    "Raw audio waveforms present significant challenges for machine learning:\n",
    "\n",
    "| Challenge | Issue | Impact on ML |\n",
    "|-----------|-------|--------------|\n",
    "| **High Dimensionality** | 15s audio at 22kHz = 330,000 data points | Computationally expensive, slow training |\n",
    "| **Temporal Redundancy** | Adjacent samples are highly correlated | Inefficient representation |\n",
    "| **No Feature Emphasis** | All frequencies treated equally | Model can't distinguish important patterns |\n",
    "| **Noise Sensitivity** | Background noise affects every sample | Poor generalization |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **The Solution**: We need a compact, meaningful representation that captures the essential characteristics of sound while discarding irrelevant details.\n",
    "\n",
    "### What Are MFCCs?\n",
    "\n",
    "**Mel Frequency Cepstral Coefficients (MFCCs)** are a compact representation of audio that mimics how humans perceive sound. They capture the frequency content of audio in a way that emphasizes perceptually important information.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Key Insight**: The human ear doesn't perceive frequencies linearly - we're more sensitive to differences in lower frequencies. MFCCs account for this using the **mel scale**.\n",
    "\n",
    "### The Mel Scale\n",
    "\n",
    "The mel scale is a perceptual scale of pitches judged by listeners to be equal in distance from one another:\n",
    "\n",
    "$$m = 2595 \\log_{10}\\left(1 + \\frac{f}{700}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the subjective pitch in mels\n",
    "- $f$ is the actual frequency in Hz\n",
    "\n",
    "| Frequency Range | Human Sensitivity | Mel Scale Behavior |\n",
    "|-----------------|-------------------|-------------------|\n",
    "| **Low (< 1000 Hz)** | High sensitivity to small changes | Finer resolution (more detail) |\n",
    "| **High (> 4000 Hz)** | Lower sensitivity to changes | Coarser resolution (less detail) |\n",
    "\n",
    "### MFCC Extraction Pipeline\n",
    "\n",
    "The transformation from raw audio to MFCCs involves several steps:\n",
    "\n",
    "```\n",
    "Raw Audio â†’ Short-Time Fourier Transform (STFT) â†’ Mel Filterbank â†’ Log Power â†’ Discrete Cosine Transform (DCT) â†’ MFCCs\n",
    "```\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "\n",
    "1. **STFT**: Convert time-domain signal to frequency-domain\n",
    "   - Breaks audio into short overlapping windows\n",
    "   - Computes frequency content of each window\n",
    "   \n",
    "2. **Mel Filterbank**: Apply mel-scaled filters\n",
    "   - Groups frequencies according to human perception\n",
    "   - More filters at low frequencies, fewer at high frequencies\n",
    "   \n",
    "3. **Log Power**: Take logarithm of filter outputs\n",
    "   - Mimics human loudness perception (logarithmic)\n",
    "   - Compresses dynamic range\n",
    "   \n",
    "4. **DCT**: Decorrelate the coefficients\n",
    "   - Produces compact, independent features\n",
    "   - First few coefficients capture most information\n",
    "\n",
    "### Why MFCCs for Heart Sound Analysis?\n",
    "\n",
    "MFCCs are particularly well-suited for distinguishing normal from abnormal heart sounds:\n",
    "\n",
    "| Heart Sound Feature | How MFCCs Capture It | Diagnostic Value |\n",
    "|---------------------|---------------------|------------------|\n",
    "| **Murmur Frequency** | Specific frequency bands activated | Valve abnormalities |\n",
    "| **Timing Patterns** | Temporal structure in coefficients | Rhythm irregularities |\n",
    "| **Intensity Variations** | Log-scale energy representation | Intensity of abnormalities |\n",
    "| **Harmonic Structure** | First few coefficients capture harmonics | Sound quality differences |\n",
    "\n",
    "### MFCC Benefits for Our Task\n",
    "\n",
    "| Benefit | Description | Impact on Our Model |\n",
    "|---------|-------------|---------------------|\n",
    "| **Dimensionality Reduction** | 330,000 samples â†’ ~1,690 features (13 coefficients Ã— 130 time frames) | 195Ã— reduction, faster training |\n",
    "| **Perceptual Relevance** | Emphasizes diagnostically important frequencies | Better anomaly detection |\n",
    "| **Noise Robustness** | Filters out high-frequency noise | More reliable features |\n",
    "| **Standard Practice** | Widely used in audio ML | Proven effectiveness, reproducibility |\n",
    "| **Fixed-Length Output** | Same feature dimensions regardless of audio length | Easy to batch and process |\n",
    "\n",
    "### Our MFCC Configuration\n",
    "\n",
    "For this workshop, we extract:\n",
    "\n",
    "```python\n",
    "n_mfcc = 13          # Number of MFCC coefficients\n",
    "max_len = 130        # Maximum time frames (padded/truncated)\n",
    "feature_dim = 1690   # Final feature vector size (13 Ã— 130)\n",
    "```\n",
    "\n",
    "**Why 13 coefficients?**\n",
    "- Captures essential frequency information\n",
    "- Standard in speech/audio processing\n",
    "- Balances information vs. computational cost\n",
    "- Higher coefficients capture less perceptually relevant detail\n",
    "\n",
    "**Why 130 time frames?**\n",
    "- Accommodates 3-second segments at typical frame rates\n",
    "- Allows padding/truncation for consistency\n",
    "- Fixed-length input required for neural networks\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Important**: MFCCs are the standard feature representation for audio classification tasks. By using them, we're focusing the autoencoder on learning meaningful acoustic patterns rather than irrelevant temporal details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_from_audio(audio, sr, n_mfcc=13, max_len=130):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from an audio signal (not file)\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio signal (numpy array)\n",
    "        sr: Sample rate\n",
    "        n_mfcc: Number of MFCC coefficients to extract\n",
    "        max_len: Maximum length to pad/truncate the features\n",
    "    \n",
    "    Returns:\n",
    "        Flattened MFCC feature vector\n",
    "    \"\"\"\n",
    "    # Extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Pad or truncate to fixed length\n",
    "    # - mfccs is a 2D array with shape (n_mfcc, time_frames) where:\n",
    "    # - pad_width specifies how much padding to add to each dimension:\n",
    "    #   * (0, 0) for the first dimension means: add 0 rows before and 0 rows after\n",
    "    #     â†’ Don't pad the MFCC coefficients dimension (keep all 13 coefficients)\n",
    "    #   * (0, pad_width) for the second dimension means: add 0 columns before and pad_width columns after\n",
    "    #     â†’ Pad the time dimension at the end to reach max_len (130 time frames)\n",
    "    # - mode='constant' means fill padded values with zeros (silence)\n",
    "    # - This ensures all samples have identical shape (13, 130) = 1,690 features when flattened\n",
    "    #   which is required for consistent input to the neural network\n",
    "\n",
    "    if mfccs.shape[1] < max_len:\n",
    "        pad_width = max_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :max_len]\n",
    "    \n",
    "    # Flatten to 1D vector\n",
    "    return mfccs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_features(file_names, \n",
    "                       sounds_folder, \n",
    "                       augment=False,\n",
    "                       segment_duration=3.0,\n",
    "                       overlap=0.5,\n",
    "                       metadata=None, \n",
    "                       metadata_label_col=None, \n",
    "                       desc=\"Processing sounds\"):\n",
    "    \"\"\"\n",
    "    Load audio files, optionally apply segmentation augmentation, and extract MFCC features\n",
    "    \n",
    "    Args:\n",
    "        file_names: List of sound IDs (file names without or with extension)\n",
    "        sounds_folder: Path to the folder containing audio files\n",
    "        augment: Whether to apply segmentation augmentation (default: False)\n",
    "        segment_duration: Duration of each segment in seconds (for augmentation)\n",
    "        overlap: Overlap fraction for segments (for augmentation)\n",
    "        metadata: Optional DataFrame with metadata for extracting labels\n",
    "        metadata_label_col: Column name in metadata for labels (e.g., 'Heart Sound Type')\n",
    "        desc: Description for progress bar\n",
    "    \n",
    "    Returns:\n",
    "        features_array: NumPy array of extracted features (shape: [n_samples, n_features])\n",
    "        sound_ids: List of sound IDs corresponding to each sample\n",
    "        labels: List of labels (if metadata provided, otherwise None)\n",
    "        augmentation_info: Dictionary with augmentation statistics\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    sound_ids_list = []\n",
    "    labels_list = [] if metadata is not None and metadata_label_col is not None else None\n",
    "    \n",
    "    total_segments = 0\n",
    "    original_count = 0\n",
    "    \n",
    "    for sound_id in tqdm(file_names, desc=desc):\n",
    "        # Ensure sound_id is a string and clean it\n",
    "        sound_id = str(sound_id).strip()\n",
    "        if sound_id == 'nan' or pd.isna(sound_id):\n",
    "            continue\n",
    "        \n",
    "        # Try to find the file with different extensions\n",
    "        file_path = sounds_folder / (sound_id + '.wav') if not sound_id.endswith('.wav') else sounds_folder / sound_id\n",
    "        \n",
    "        # Try alternate extension if not found\n",
    "        if not file_path.exists():\n",
    "            file_path = sounds_folder / sound_id.replace('.wav', '.WAV')\n",
    "        \n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                # Load audio\n",
    "                audio, sr = librosa.load(file_path, sr=None)\n",
    "                original_count += 1\n",
    "                \n",
    "                if augment:\n",
    "                    # Slice into segments\n",
    "                    segments = slice_audio_into_segments(audio, sr, segment_duration, overlap)\n",
    "                    \n",
    "                    # Extract features from each segment\n",
    "                    for i, segment in enumerate(segments):\n",
    "                        features = extract_mfcc_from_audio(segment, sr)\n",
    "                        features_list.append(features)\n",
    "                        sound_ids_list.append(f\"{sound_id}_seg{i}\")\n",
    "                        \n",
    "                        # Extract label from metadata if provided\n",
    "                        if labels_list is not None:\n",
    "                            label = metadata[metadata['Heart Sound ID'] == sound_id][metadata_label_col].values[0]\n",
    "                            labels_list.append(label)\n",
    "                        \n",
    "                        total_segments += 1\n",
    "                else:\n",
    "                    # No augmentation - extract features from full audio\n",
    "                    features = extract_mfcc_from_audio(audio, sr)\n",
    "                    features_list.append(features)\n",
    "                    sound_ids_list.append(sound_id)\n",
    "                    \n",
    "                    # Extract label from metadata if provided\n",
    "                    if labels_list is not None:\n",
    "                        label = metadata[metadata['Heart Sound ID'] == sound_id][metadata_label_col].values[0]\n",
    "                        labels_list.append(label)\n",
    "                    \n",
    "                    total_segments += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sound_id}: {e}\")\n",
    "    \n",
    "    features_array = np.array(features_list)\n",
    "    \n",
    "    augmentation_info = {\n",
    "        'original_files': original_count,\n",
    "        'total_samples': total_segments,\n",
    "        'augmentation_factor': total_segments / original_count if original_count > 0 else 1\n",
    "    }\n",
    "    \n",
    "    return features_array, sound_ids_list, labels_list, augmentation_info\n",
    "\n",
    "\n",
    "print(\"âœ… Data augmentation functions defined!\")\n",
    "print(\"   Use augment=True for training data (creates 3s segments)\")\n",
    "print(\"   Use augment=False for full-length recordings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 and 3: Prepare and Preprocess Data\n",
    "***\n",
    "\n",
    "Let's implement the data preparation and preprocessing steps. This includes loading the audio files, extracting MFCC features, and creating PyTorch datasets and dataloaders for training our autoencoder. \n",
    "\n",
    "First, we are going to start by creating the augmented dataset by slicing the original audio recordings into overlapping segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from Mix heart sounds (for training)\n",
    "# CRITICAL: Filter for ONLY normal heart sounds + Apply augmentation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING DATA PREPARATION WITH AUGMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Filter Mix metadata for normal heart sounds only\n",
    "normal_mix_mask = mix_metadata['Heart Sound Type'] == 'Normal'\n",
    "abnormal_mix_mask = ~normal_mix_mask\n",
    "\n",
    "# Extract only normal heart sound IDs for training\n",
    "mix_normal_heart_files = mix_metadata[normal_mix_mask]['Heart Sound ID'].tolist()\n",
    "\n",
    "# Load with augmentation\n",
    "train_features, train_sound_ids, _, aug_info = load_audio_features(\n",
    "    file_names=mix_normal_heart_files,\n",
    "    sounds_folder=mix_sounds_folder,\n",
    "    augment=True,\n",
    "    segment_duration=3.0,\n",
    "    overlap=0.5,\n",
    "    desc=\"Processing Normal Mix heart sounds\"\n",
    ")\n",
    "\n",
    "print(f\"   Original recordings: {aug_info['original_files']}\")\n",
    "print(f\"   Total training samples after augmentation: {aug_info['total_samples']}\")\n",
    "print(f\"   Augmentation factor: {aug_info['augmentation_factor']:.1f}x\")\n",
    "print(f\"   Feature shape per sample: {train_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HS dataset for testing with same 3s segmentation as training data\n",
    "# CRITICAL: Use 3s segments for consistency with training data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST DATA PREPARATION (With 3s Segmentation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "normal_mask = heart_metadata['Heart Sound Type'] == 'Normal'\n",
    "abnormal_mask = ~normal_mask\n",
    "\n",
    "normal_test_files = heart_metadata[normal_mask]['Heart Sound ID'].tolist()\n",
    "abnormal_test_files = heart_metadata[abnormal_mask]['Heart Sound ID'].tolist()\n",
    "\n",
    "# Load normal test sounds WITH 3s segmentation (same as training)\n",
    "normal_test_features, _, _, normal_aug_info = load_audio_features(\n",
    "    file_names=normal_test_files,\n",
    "    sounds_folder=heart_sounds_folder,\n",
    "    augment=True,  # Use same segmentation as training\n",
    "    segment_duration=3.0,\n",
    "    overlap=0.5,\n",
    "    desc=\"Processing normal HS sounds (3s segments)\"\n",
    ")\n",
    "print(f\"âœ… Loaded {len(normal_test_features)} normal 3s segments from {len(normal_test_files)} recordings\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load abnormal test sounds WITH 3s segmentation\n",
    "abnormal_test_features, _, abnormal_test_labels, abnormal_aug_info = load_audio_features(\n",
    "    file_names=abnormal_test_files,\n",
    "    sounds_folder=heart_sounds_folder,\n",
    "    augment=True,  # Use same segmentation as training\n",
    "    segment_duration=3.0,\n",
    "    overlap=0.5,\n",
    "    metadata=heart_metadata,\n",
    "    metadata_label_col='Heart Sound Type',\n",
    "    desc=\"Processing abnormal HS sounds (3s segments)\"\n",
    ")\n",
    "print(f\"âœ… Loaded {len(abnormal_test_features)} abnormal 3s segments from {len(abnormal_test_files)} recordings\")\n",
    "\n",
    "# Display abnormal sound types\n",
    "if abnormal_test_labels:\n",
    "    print(f\"\\nAbnormal sound types in test set:\")\n",
    "    for sound_type in sorted(set(abnormal_test_labels)):\n",
    "        count = abnormal_test_labels.count(sound_type)\n",
    "        print(f\"   {sound_type}: {count} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation, and Test Sets\n",
    "***\n",
    "\n",
    "Just as in supervised learning, we need to split our data into separate sets. For autoencoders:\n",
    "\n",
    "| Dataset | Purpose | Typical Split | Usage | Content for Our Task |\n",
    "|---------|---------|---------------|--------|----------------------|\n",
    "| **Training Set** | Learn to reconstruct normal patterns | 60-80% | Every training iteration | Primarily normal heart sounds |\n",
    "| **Validation Set** | Monitor reconstruction quality during training | 10-20% | During model development | Mix of normal and some abnormal sounds |\n",
    "| **Test Set** | Final evaluation of anomaly detection | 10-20% | Once, after training | Balanced mix of normal and abnormal sounds |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Key Difference**: Unlike supervised learning, our training set contains mostly normal examples. The model learns what \"normal\" looks like, and anomalies naturally produce higher reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation/test sets with consistent 3s segmentation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SPLITTING DATA INTO TRAIN/VAL/TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split Mix heart sounds (normal only) into training and validation (80/20)\n",
    "X_train, X_val = train_test_split(\n",
    "    train_features, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create labels for training/validation (all normal from Mix)\n",
    "y_train = np.zeros(len(X_train))\n",
    "y_val = np.zeros(len(X_val))\n",
    "\n",
    "# Test set from HS dataset (normal + abnormal)\n",
    "X_test = np.vstack([normal_test_features, abnormal_test_features])\n",
    "y_test = np.concatenate([\n",
    "    np.zeros(len(normal_test_features)),\n",
    "    np.ones(len(abnormal_test_features))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nTRAINING SET (Mix - NORMAL only):\")\n",
    "print(f\"   Samples: {X_train.shape[0]} (3s segments)\")\n",
    "print(f\"   Source: {aug_info['original_files']} recordings Ã— {aug_info['augmentation_factor']:.1f} augmentation\")\n",
    "print(f\"   Normal: {X_train.shape[0]} (100%)\")\n",
    "\n",
    "print(f\"\\nVALIDATION SET (Mix - NORMAL only):\")\n",
    "print(f\"   Samples: {X_val.shape[0]} (3s segments)\")\n",
    "print(f\"   Normal: {X_val.shape[0]} (100%)\")\n",
    "\n",
    "print(f\"\\nTEST SET (HS - Independent dataset):\")\n",
    "print(f\"   Samples: {X_test.shape[0]} (3s segments)\")\n",
    "print(f\"   Normal: {(y_test==0).sum()} segments from {len(normal_test_files)} recordings ({(y_test==0).sum()/len(y_test)*100:.1f}%)\")\n",
    "print(f\"   Abnormal: {(y_test==1).sum()} segments from {len(abnormal_test_files)} recordings ({(y_test==1).sum()/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… All datasets use consistent 3s segmentation!\")\n",
    "print(f\"   Feature dimensions: {X_train.shape[1]}\")\n",
    "print(f\"   Augmentation increased training data {aug_info['augmentation_factor']:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "***\n",
    "\n",
    "Normalization is crucial for neural network training. We'll use **Standard Scaling** (Z-score normalization) for our MFCC features:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $z$ is the normalized value  \n",
    "- $x$ is the original value\n",
    "- $\\mu$ is the mean computed from training data\n",
    "- $\\sigma$ is the standard deviation computed from training data\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Critical**: Always fit the scaler on training data only, then apply to validation and test sets. This prevents data leakage!\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: Standard Scaling with scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data only\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Inverse transform to get original values\n",
    "X_train_original = scaler.inverse_transform(X_train_scaled)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Data Normalization and Tensor Conversion ðŸŽ¯\n",
    "\n",
    "# In this exercise, you will:\n",
    "# 1. Normalize the audio features using StandardScaler\n",
    "# 2. Convert numpy arrays to PyTorch tensors\n",
    "# 3. Verify the normalization worked correctly\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Normalize the data using StandardScaler and convert to PyTorch tensors\n",
    "\n",
    "# Create a StandardScaler\n",
    "scaler = ___(___) \n",
    "\n",
    "# Fit the scaler ONLY on training data (to prevent data leakage!)\n",
    "scaler.___(X_train)\n",
    "\n",
    "# Transform all datasets using the fitted scaler\n",
    "X_train_scaled = scaler.___(X_train)\n",
    "X_val_scaled = scaler.___(X_val)\n",
    "X_test_scaled = scaler.___(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors with float32 data type\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.___)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.___)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.___)\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify normalization\n",
    "print(\"Normalization check:\")\n",
    "print(f\"   Training data - Mean: {X_train_tensor.mean().item():.6f}, Std: {X_train_tensor.std().item():.4f}\")\n",
    "print(f\"   Validation data - Mean: {X_val_tensor.mean().item():.6f}, Std: {X_val_tensor.std().item():.4f}\")\n",
    "print(f\"   Test data - Mean: {X_test_tensor.mean().item():.6f}, Std: {X_test_tensor.std().item():.4f}\")\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"   Train: {X_train_tensor.shape}\")\n",
    "print(f\"   Validation: {X_val_tensor.shape}\")\n",
    "print(f\"   Test: {X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Loss Function for Autoencoders\n",
    "***\n",
    "\n",
    "Unlike classification (cross-entropy) or supervised regression (MSE on targets), autoencoders use **reconstruction loss** - they compare the input with the reconstructed output.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Key Insight**: Autoencoders don't need separate labels! The input $\\mathbf{x}$ serves as both input and target.\n",
    "\n",
    "**Training objective:** Minimize $\\mathcal{L} = ||\\mathbf{x} - \\mathbf{\\hat{x}}||^2$\n",
    "\n",
    "| Loss Function | Formula | PyTorch Implementation | Use Case |\n",
    "|---------------|---------|----------------------|----------|\n",
    "| **MSE (L2 Loss)** | $$\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\hat{x}_i)^2$$ | `nn.MSELoss()` | General reconstruction |\n",
    "| **MAE (L1 Loss)** | $$\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^{n}\\|x_i - \\hat{x}_i\\|$$ | `nn.L1Loss()` | Robust to outliers |\n",
    "| **Binary Cross-Entropy** | $$\\mathcal{L} = -\\sum_{i=1}^{n}[x_i\\log(\\hat{x}_i) + (1-x_i)\\log(1-\\hat{x}_i)]$$ | `nn.BCELoss()` | Binary features [0,1] |\n",
    "\n",
    "**For our MFCC features**, MSE Loss is appropriate since:\n",
    "- Features are continuous (not binary)\n",
    "- We want to penalize larger errors more heavily\n",
    "- Standard practice for audio reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Loss Function Setup ðŸŽ¯\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Create the appropriate loss function for autoencoder reconstruction\n",
    "\n",
    "# For autoencoders, we use MSE loss to measure reconstruction quality\n",
    "loss_function = nn.___(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Activation Function\n",
    "***\n",
    "The next step is to choose an activation function for the model. The activation function introduces non-linearity to the model, allowing it to learn complex relationships in the data. The following table lists some common activation functions used in neural networks, along with their characteristics and best use cases:\n",
    "\n",
    "| Function | Formula | Range | PyTorch Implementation | Best Used For |\n",
    "|----------|---------|-------|-------------------|---------------|\n",
    "| ReLU | $$f(x) = \\max(0, x)$$ | $$[0, \\infty)$$ | `torch.nn.ReLU()` | Hidden layers in most networks |\n",
    "| Sigmoid | $$f(x) = \\frac{1}{1+e^{-x}}$$ | $$(0, 1)$$ | `torch.nn.Sigmoid()` | Binary classification, gates in LSTMs |\n",
    "| Tanh | $$f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$ | $$(-1, 1)$$ | `torch.nn.Tanh()` | Hidden layers when output normalization is needed |\n",
    "| Leaky ReLU | $$f(x) = \\max(\\alpha x, x)$$ | $$(-\\infty, \\infty)$$ | `torch.nn.LeakyReLU(negative_slope=0.01)` | Preventing \"dead neurons\" problem |\n",
    "| Softmax | $$f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$ | $$(0, 1)$$ | `torch.nn.Softmax(dim=1)` | Multi-class classification output layer |\n",
    "\n",
    "The choice of activation function depends on the specific problem and the architecture of the neural network. \n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(1500%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> - ReLU is the most commonly used activation function in hidden layers of deep networks due to its simplicity and effectiveness.\n",
    "> - The activation function for the output layer depends on the type of problem being solved (e.g., regression, binary classification, multi-class classification).\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\" style=\"filter: invert(19%) sepia(75%) saturate(6158%) hue-rotate(312deg) brightness(87%) contrast(116%);\"/> **Common Mistakes to Avoid**: \n",
    "> - Mixing activation functions in the same layer (e.g., using ReLU and sigmoid together) can lead to unexpected behavior.\n",
    "> - Using activation functions that saturate (like sigmoid) in hidden layers can lead to vanishing gradients, making training difficult.\n",
    "> - Forgetting to apply the activation function to the output layer can lead to incorrect predictions (e.g., not using softmax for multi-class classification).\n",
    "> - Not considering the range of the output when choosing the activation function (e.g., using sigmoid for regression tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Model Architecture - Introducing Autoencoders\n",
    "***\n",
    "\n",
    "### What is an Autoencoder?\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: An **autoencoder** is a neural network trained to reconstruct its input. It learns to compress data into a lower-dimensional representation (encoding) and then reconstruct it back (decoding).\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "An autoencoder consists of two main parts:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/autoencoder.png\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "***\n",
    "| Component | Description | Mathematical Representation |\n",
    "|-----------|-------------|----------------------------|\n",
    "| **Encoder** | Compresses input to lower dimensions | $\\mathbf{z} = f_{enc}(\\mathbf{x})$ |\n",
    "| **Latent Space** | Compressed representation (bottleneck) | $\\mathbf{z} \\in \\mathbb{R}^d$ where $d << n$ |\n",
    "| **Decoder** | Reconstructs input from latent space | $\\mathbf{\\hat{x}} = f_{dec}(\\mathbf{z})$ |\n",
    "\n",
    "### Why Autoencoders for Anomaly Detection?\n",
    "\n",
    "The key insight: **If trained only on normal data, the autoencoder learns to reconstruct normal patterns well but struggles with abnormal patterns.**\n",
    "\n",
    "| Scenario | Reconstruction Error | Interpretation |\n",
    "|----------|---------------------|----------------|\n",
    "| Normal heart sound | Low (â‰ˆ 0) | Model has seen similar patterns |\n",
    "| Abnormal heart sound | High (>> 0) | Model hasn't learned this pattern |\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For input $\\mathbf{x} \\in \\mathbb{R}^n$:\n",
    "\n",
    "**Encoding:**\n",
    "$$\\mathbf{z} = \\sigma(\\mathbf{W}_{enc}\\mathbf{x} + \\mathbf{b}_{enc})$$\n",
    "\n",
    "**Decoding:**\n",
    "$$\\mathbf{\\hat{x}} = \\sigma(\\mathbf{W}_{dec}\\mathbf{z} + \\mathbf{b}_{dec})$$\n",
    "\n",
    "**Reconstruction Error:**\n",
    "$$\\mathcal{L} = ||\\mathbf{x} - \\mathbf{\\hat{x}}||^2 = \\sum_{i=1}^{n}(x_i - \\hat{x}_i)^2$$\n",
    "\n",
    "### Network Design Considerations\n",
    "\n",
    "| Layer Type | Encoder | Decoder | Activation |\n",
    "|------------|---------|---------|------------|\n",
    "| **Input Layer** | Feature dimension (e.g., 1690) | Latent dimension (e.g., 32) | - |\n",
    "| **Hidden Layer 1** | 512 neurons | 128 neurons | ReLU |\n",
    "| **Hidden Layer 2** | 128 neurons | 512 neurons | ReLU |\n",
    "| **Output Layer** | 32 neurons (latent) | Original dimension | Linear/Sigmoid |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Symmetric Architecture**: Decoders typically mirror encoders. If encoder is [1690 â†’ 512 â†’ 128 â†’ 32], decoder is [32 â†’ 128 â†’ 512 â†’ 1690].\n",
    "\n",
    "### Latent Space Dimensionality\n",
    "\n",
    "The bottleneck dimension determines compression strength:\n",
    "\n",
    "- **Too large** (e.g., 512): Model may memorize noise, poor anomaly detection\n",
    "- **Too small** (e.g., 8): Loses important information, poor reconstruction  \n",
    "- **Sweet spot** (e.g., 32-64): Captures essential patterns, discards noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising Weights and Biases\n",
    "***\n",
    "\n",
    "In the previous session we looked at the concept of weights and biases. With our Perceptron we initialised the weights and biases to random values. In PyTorch, we can use different methods to initialise the weights and biases of a neural network.\n",
    "\n",
    "The importance of initialising weights and biases lies in the fact that they can significantly affect the convergence speed and performance of the neural network. Proper initialisation can help prevent issues such as vanishing or exploding gradients, which can hinder the training process.\n",
    "\n",
    "| Initialisation Method | Formula | PyTorch Code | Description |\n",
    "|-----------------------|----------|--------------|-------------|\n",
    "| Xavier/Glorot Initialisation | $$W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$$ | `torch.nn.init.xavier_uniform_(tensor)` | Suitable for sigmoid and tanh activations. |\n",
    "| He Initialisation | $$W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})$$ | `torch.nn.init.kaiming_uniform_(tensor)` | Suitable for ReLU activations. |\n",
    "| Kaiming Normal Initialisation | $$W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in}}})$$ | `torch.nn.init.kaiming_normal_(tensor)` | Suitable for ReLU activations. |\n",
    "| Kaiming Uniform Initialisation | $$W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})$$ | `torch.nn.init.kaiming_uniform_(tensor)` | Suitable for ReLU activations. |\n",
    "| Zero Initialisation | $$W = 0$$ | `torch.nn.init.zeros_(tensor)` | All weights are set to zero. Not recommended. |\n",
    "| Random Initialisation | $$W \\sim \\mathcal{U}(-1, 1)$$ | `torch.nn.init.uniform_(tensor)` | Weights are randomly initialised between -1 and 1. |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> - Use Xavier or He initialisation for most cases, as they are designed to maintain the variance of activations across layers.\n",
    "> - Avoid zero initialisation, as it can lead to symmetry problems where all neurons learn the same features.\n",
    "> - PyTorch uses Kaiming initialisation by default for `torch.nn.Linear` layers, which is suitable for ReLU activations.\n",
    "> - Experiment with different initialisation methods to see their impact on training speed and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build an Autoencoder ðŸŽ¯\n",
    "\n",
    "# In this exercise, you will:\n",
    "# 1. Create an autoencoder architecture with encoder and decoder\n",
    "# 2. Initialize weights properly\n",
    "# 3. Implement the forward pass for reconstruction\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Build the autoencoder architecture\n",
    "\n",
    "class HeartSoundAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32):\n",
    "        \"\"\"\n",
    "        Autoencoder for heart sound anomaly detection\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features (MFCC flattened)\n",
    "            latent_dim: Dimension of latent space (bottleneck)\n",
    "        \"\"\"\n",
    "        super(HeartSoundAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder: Compress input to latent representation\n",
    "        # Architecture: input_dim â†’ 512 â†’ 128 â†’ latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(___, ___),  # First layer: input_dim to 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(___, ___),  # Second layer: 512 to 128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(___, ___),  # Third layer: 128 to latent_dim\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder: Reconstruct input from latent representation\n",
    "        # Architecture: latent_dim â†’ 128 â†’ 512 â†’ input_dim (mirror of encoder)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(___, ___),  # First layer: latent_dim to 128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(___, ___),  # Second layer: 128 to 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(___, ___)   # Third layer: 512 to input_dim\n",
    "            # No activation on final layer - we want continuous output\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: encode then decode\"\"\"\n",
    "        # Encode to latent space\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # Decode to reconstruct input\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        \n",
    "        return x_reconstructed\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Get latent representation only\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Initialize the autoencoder\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "latent_dim = 32\n",
    "\n",
    "model = HeartSoundAutoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "\n",
    "print(\"ðŸ—ï¸ Autoencoder Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Input dimension: {input_dim}\")\n",
    "print(f\"   Latent dimension: {latent_dim}\")\n",
    "print(f\"   Compression ratio: {input_dim/latent_dim:.1f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Choose Optimizer and Loss Function  \n",
    "***\n",
    "\n",
    "### Optimizer Selection\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: Optimizers are algorithms that update model parameters to minimize the loss function using gradients computed via backpropagation.\n",
    "\n",
    "| Optimizer | PyTorch Implementation | Best Used For | Learning Rate |\n",
    "|-----------|----------------------|---------------|---------------|\n",
    "| **Adam** | `torch.optim.Adam(params, lr=0.001)` | Most deep learning tasks | 1e-3 to 1e-4 |\n",
    "| SGD | `torch.optim.SGD(params, lr=0.01, momentum=0.9)` | When you need momentum | 1e-2 to 1e-1 |\n",
    "| RMSProp | `torch.optim.RMSprop(params, lr=0.001)` | RNNs and time series | 1e-3 to 1e-4 |\n",
    "| AdamW | `torch.optim.AdamW(params, lr=0.001, weight_decay=0.01)` | When regularization matters | 1e-3 to 1e-4 |\n",
    "\n",
    "**For autoencoders**, Adam is typically the best choice due to its adaptive learning rates and momentum.\n",
    "\n",
    "### Loss Function Behavior\n",
    "\n",
    "The loss works in conjunction with the optimiser. While there are loss functions that can work for the same task, the choice of loss will have an effect on the final performance of the model. For instance, using MSE (L2-Norm) loss for a regression task will penalise larger errors more than smaller ones, while MAE (L1-Norm) loss treats all errors equally. This can lead to different model performance depending on the distribution of the data.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/losses.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Remember**: The reconstruction loss measures how well the autoencoder has learned to represent normal patterns. Lower loss on training data = better compression. Higher loss on abnormal data = successful anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Optimizer Setup ðŸŽ¯\n",
    "\n",
    "# In this exercise, you will:\n",
    "# 1. Select an appropriate optimizer for the autoencoder\n",
    "# 2. Set an appropriate learning rate\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Create an optimizer to train the model\n",
    "\n",
    "# Create an Adam optimizer for the autoencoder\n",
    "learning_rate = ___  # Typical range: 0.0001 to 0.01\n",
    "optimizer = torch.optim.___(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print configuration\n",
    "print(\"âš™ï¸ Training Configuration:\")\n",
    "print(f\"   Optimizer: {type(optimizer).__name__}\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"   Loss function: {type(loss_function).__name__}\")\n",
    "print(f\"   Number of trainable parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 and 9: Create Training Loop and Fit Model\n",
    "***\n",
    "\n",
    "### The Autoencoder Training Loop\n",
    "\n",
    "The training loop for autoencoders follows the standard PyTorch workflow, but with a key difference: **the input is also the target**. We're training the model to reconstruct its own input.\n",
    "\n",
    "| Step | Description | Code Example | Purpose |\n",
    "|------|-------------|--------------|---------|\n",
    "| 1. **Forward Pass** | Pass input through encoder and decoder | `reconstructed = model(input)` | Get reconstruction |\n",
    "| 2. **Loss Computation** | Compare input with reconstruction | `loss = criterion(reconstructed, input)` | Measure quality |\n",
    "| 3. **Backward Pass** | Compute gradients | `loss.backward()` | Calculate updates |\n",
    "| 4. **Parameter Updates** | Update weights | `optimizer.step()` | Improve model |\n",
    "| 5. **Gradient Reset** | Zero gradients | `optimizer.zero_grad()` | Prepare for next batch |\n",
    "\n",
    "### Monitoring Training\n",
    "\n",
    "For autoencoders, we track:\n",
    "- **Training loss**: Reconstruction error on normal training data (should decrease)\n",
    "- **Validation loss**: Reconstruction error on validation set (should decrease but stay close to training)\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Augmentation Benefit**: With 93 training samples (vs. 10 without augmentation), the model can learn more diverse normal patterns, leading to better generalization and more robust anomaly detection!\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 2**: Autoencoder Training Loop Structure\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Forward Pass: Reconstruct input\n",
    "    reconstructed = model(input_data)\n",
    "    \n",
    "    # 2. Loss Computation: Compare input with reconstruction\n",
    "    loss = criterion(reconstructed, input_data)  # Note: target = input!\n",
    "    \n",
    "    # 3. Backward Pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Parameter Updates\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 5. Gradient Reset\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Validation (no gradients needed)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_reconstructed = model(val_data)\n",
    "        val_loss = criterion(val_reconstructed, val_data)\n",
    "```\n",
    "\n",
    "### Epochs and Training Duration\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Definition**: An **epoch** is one complete pass through the entire training dataset.\n",
    "\n",
    "| Epochs | Training Time | Risk | Recommendation |\n",
    "|--------|---------------|------|----------------|\n",
    "| Too few (< 50) | Fast | Underfitting - poor reconstruction | Start here, increase if needed |\n",
    "| Optimal (100-300) | Moderate | Good balance | Monitor validation loss |\n",
    "| Too many (> 500) | Slow | Overfitting - memorizes noise | Use early stopping |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Training the Autoencoder ðŸŽ¯\n",
    "\n",
    "# In this exercise, you will:\n",
    "# 1. Implement a complete training loop for the autoencoder\n",
    "# 2. Monitor training and validation losses\n",
    "# 3. Track reconstruction quality over epochs\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Implement the training loop for the autoencoder\n",
    "\n",
    "def train_autoencoder(model, \n",
    "                     train_data, \n",
    "                     val_data, \n",
    "                     optimizer, \n",
    "                     criterion,\n",
    "                     epochs=200):\n",
    "    \"\"\"\n",
    "    Train an autoencoder model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch autoencoder model\n",
    "        train_data: Training features (input = target for autoencoders!)\n",
    "        val_data: Validation features\n",
    "        optimizer: PyTorch optimizer\n",
    "        criterion: Loss function\n",
    "        epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        train_losses: List of training losses per epoch\n",
    "        val_losses: List of validation losses per epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Autoencoder\"):\n",
    "        # === TRAINING PHASE ===\n",
    "        model.train()\n",
    "        \n",
    "        # 1. Zero gradients from previous iteration\n",
    "        optimizer.___()\n",
    "        \n",
    "        # 2. Forward pass: Reconstruct the input\n",
    "        reconstructed = model(___)\n",
    "        \n",
    "        # 3. Compute reconstruction loss (compare input with reconstruction)\n",
    "        # Remember: For autoencoders, the target IS the input!\n",
    "        train_loss = criterion(___, ___)\n",
    "        \n",
    "        # 4. Backward pass: Compute gradients\n",
    "        train_loss.___()\n",
    "        \n",
    "        # 5. Update weights using the optimizer\n",
    "        optimizer.___()\n",
    "        \n",
    "        # Store training loss\n",
    "        train_losses.append(train_loss.item())\n",
    "        \n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_reconstructed = model(val_data)\n",
    "            val_loss = criterion(val_reconstructed, val_data)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"ðŸ‹ï¸ Training autoencoder on normal heart sounds...\")\n",
    "train_losses, val_losses = train_autoencoder(\n",
    "    model=model,\n",
    "    train_data=X_train_tensor,\n",
    "    val_data=X_val_tensor,\n",
    "    optimizer=optimizer,\n",
    "    criterion=loss_function,\n",
    "    epochs=300\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"   Final training loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"   Final validation loss: {val_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress (Plotly)\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs,\n",
    "        y=train_losses,\n",
    "        mode='lines',\n",
    "        name='Training Loss',\n",
    "        line=dict(color='steelblue', width=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs,\n",
    "        y=val_losses,\n",
    "        mode='lines',\n",
    "        name='Validation Loss',\n",
    "        line=dict(color='coral', width=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='AUTOENCODER TRAINING PROGRESS',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Reconstruction Loss (MSE)',\n",
    "    template='plotly_white',\n",
    "    width=950,\n",
    "    height=450,\n",
    "    legend=dict(x=0.01, y=0.99)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, Underfitting, and Early Stopping\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> **Definition**: Overfitting occurs when a model learns the training data too well, including noise and outliers, leading to poor generalisation on unseen data. Underfitting occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "As we can see in the following figure, the training loss decreases over time, while the validation loss follows a similar trend. However, the validation loss starts to slowly deviate from the training loss after a certain number of epochs. This indicates that the model is starting to overfit the training data. The point at which the validation loss starts to increase is known as the \"early stopping\" point. This is the point at which we should stop training the model to prevent overfitting.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/over_under_fit.png\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Model - Anomaly Detection\n",
    "***\n",
    "\n",
    "### Using Reconstruction Error for Anomaly Detection\n",
    "\n",
    "The core principle: **Models trained on normal data struggle to reconstruct abnormal patterns.**\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/docs.svg\" width=\"20\"/> **Anomaly Score**: The reconstruction error serves as an anomaly score. Higher error = more abnormal.\n",
    "\n",
    "$$\\text{Anomaly Score} = ||\\mathbf{x} - \\mathbf{\\hat{x}}||^2 = \\sum_{i=1}^{n}(x_i - \\hat{x}_i)^2$$\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "1. **Compute reconstruction errors** for all test samples\n",
    "2. **Set a threshold** to distinguish normal from abnormal\n",
    "3. **Calculate metrics**: Accuracy, precision, recall, F1-score\n",
    "4. **Visualize** the distribution of errors for normal vs. abnormal sounds\n",
    "\n",
    "### Choosing the Threshold\n",
    "\n",
    "| Method | Description | When to Use |\n",
    "|--------|-------------|-------------|\n",
    "| **Statistical** | Mean + kÃ—std of normal training errors | When normal data is clean |\n",
    "| **Percentile** | 95th or 99th percentile of training errors | Robust to outliers |\n",
    "| **ROC-based** | Threshold that maximizes TPR - FPR | When you have validation labels |\n",
    "| **Domain-specific** | Clinical guidelines | When expert knowledge exists |\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "For anomaly detection:\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| **True Positive Rate (Recall)** | $$\\frac{TP}{TP + FN}$$ | % of abnormals correctly identified |\n",
    "| **False Positive Rate** | $$\\frac{FP}{FP + TN}$$ | % of normals incorrectly flagged |\n",
    "| **Precision** | $$\\frac{TP}{TP + FP}$$ | % of alerts that are true abnormals |\n",
    "| **F1 Score** | $$2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$ | Harmonic mean of precision and recall |\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 3**: Evaluate Autoencoder for Anomaly Detection\n",
    "\n",
    "```python\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Compute reconstruction errors\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(test_data)\n",
    "    reconstruction_errors = torch.mean((test_data - reconstructed)**2, dim=1)\n",
    "\n",
    "# Set threshold (e.g., 95th percentile of training errors)\n",
    "threshold = torch.quantile(train_errors, 0.95)\n",
    "\n",
    "# Classify as anomaly if error > threshold\n",
    "predictions = (reconstruction_errors > threshold).float()\n",
    "```\n",
    "\n",
    "### Clinical Significance\n",
    "\n",
    "In medical applications, we must balance:\n",
    "- **High Recall**: Don't miss serious conditions (minimize false negatives)\n",
    "- **Acceptable Precision**: Avoid alarm fatigue (minimize false positives)\n",
    "\n",
    "The threshold choice depends on clinical priorities and downstream workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Evaluate the Autoencoder for Anomaly Detection ðŸŽ¯\n",
    "\n",
    "# In this exercise, you will:\n",
    "# 1. Compute reconstruction errors for all test samples\n",
    "# 2. Set a threshold to distinguish normal from abnormal\n",
    "# 3. Calculate performance metrics\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Compute reconstruction errors and set a threshold for anomaly detection\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING AUTOENCODER FOR ANOMALY DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Compute reconstruction errors for training data (to set threshold)\n",
    "    train_reconstructed = model(___)\n",
    "    # Calculate mean squared error per sample (across all features)\n",
    "    train_errors = torch.mean((X_train_tensor - ___)**2, dim=1)\n",
    "    \n",
    "    # Compute reconstruction errors for test data\n",
    "    test_reconstructed = model(___)\n",
    "    test_errors = torch.mean((X_test_tensor - ___)**2, dim=1)\n",
    "\n",
    "# Set threshold as 95th percentile of training errors\n",
    "# Samples with error > threshold will be classified as anomalies\n",
    "threshold = torch.quantile(train_errors, ___).item()\n",
    "\n",
    "print(f\"\\nðŸ“Š Reconstruction Error Statistics:\")\n",
    "print(f\"   Training (normal) - Mean: {train_errors.mean().item():.6f}, Std: {train_errors.std().item():.6f}\")\n",
    "print(f\"   Test set - Mean: {test_errors.mean().item():.6f}, Std: {test_errors.std().item():.6f}\")\n",
    "print(f\"   Threshold (95th percentile): {threshold:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test samples as anomaly if error > threshold\n",
    "predictions = (test_errors > threshold).float()\n",
    "\n",
    "# Calculate performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test_tensor.numpy(), predictions.numpy())\n",
    "precision = precision_score(y_test_tensor.numpy(), predictions.numpy(), zero_division=0)\n",
    "recall = recall_score(y_test_tensor.numpy(), predictions.numpy())\n",
    "f1 = f1_score(y_test_tensor.numpy(), predictions.numpy())\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Anomaly Detection Performance:\")\n",
    "print(f\"   Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"   Precision: {precision:.3f} (of flagged anomalies, how many are true?)\")\n",
    "print(f\"   Recall:    {recall:.3f} (of true anomalies, how many did we catch?)\")\n",
    "print(f\"   F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_tensor.numpy(), predictions.numpy())\n",
    "print(f\"\\nðŸ“‹ Confusion Matrix:\")\n",
    "print(f\"               Predicted\")\n",
    "print(f\"             Normal  Abnormal\")\n",
    "print(f\"Actual Normal    {cm[0,0]:3d}     {cm[0,1]:3d}\")\n",
    "print(f\"      Abnormal   {cm[1,0]:3d}     {cm[1,1]:3d}\")\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction error distribution (Plotly)\n",
    "# Separate errors by class\n",
    "test_normal_errors = test_errors[y_test_tensor == 0].cpu().numpy()\n",
    "test_abnormal_errors = test_errors[y_test_tensor == 1].cpu().numpy()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Distribution of Reconstruction Errors\".upper(), \"Reconstruction Error by Class\".upper()),\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "# Histogram of errors\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=test_normal_errors,\n",
    "        nbinsx=30,\n",
    "        name=\"Normal\",\n",
    "        marker_color=\"steelblue\",\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=test_abnormal_errors,\n",
    "        nbinsx=30,\n",
    "        name=\"Abnormal\",\n",
    "        marker_color=\"coral\",\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Threshold line on histogram\n",
    "fig.add_vline(\n",
    "    x=threshold,\n",
    "    line=dict(color=\"red\", dash=\"dash\", width=2),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Box plot\n",
    "fig.add_trace(\n",
    "    go.Box(y=test_normal_errors, name=\"Normal\", marker_color=\"steelblue\", boxmean=True),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Box(y=test_abnormal_errors, name=\"Abnormal\", marker_color=\"coral\", boxmean=True),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Threshold line on box plot\n",
    "fig.add_hline(\n",
    "    y=threshold,\n",
    "    line=dict(color=\"red\", dash=\"dash\", width=2),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Reconstruction Error\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Reconstruction Error\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"overlay\",\n",
    "    height=500,\n",
    "    width=1200,\n",
    "    legend_title_text=\"Class\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Key findings:\")\n",
    "print(f\"   â€¢ Normal sounds: Mean reconstruction error = {test_normal_errors.mean():.4f}\")\n",
    "print(f\"   â€¢ Abnormal sounds: Mean reconstruction error = {test_abnormal_errors.mean():.4f}\")\n",
    "print(f\"   â€¢ Separation: {(test_abnormal_errors.mean() / test_normal_errors.mean()):.1f}x higher for abnormal\")\n",
    "print(f\"\\nâœ… The autoencoder successfully learned to distinguish normal from abnormal heart sounds!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uom-fse-dl-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
