{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/se04.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/solutions/SE04_SL_Convolutional_Neural_Networks.ipynb)\n",
    "\n",
    "## Workshop Overview\n",
    "***\n",
    "In this workshop, we explore Convolutional Neural Networks (CNNs) for computer vision tasks. Starting from the convolution operation itself, we build toward full CNN architectures for industrial image classification using the NEU-CLS surface defect dataset.\n",
    "\n",
    "**Prerequisites**: Neural networks (SE02), PyTorch fundamentals (SE01)\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand the convolution operation and its key parameters\n",
    "- Apply and design convolutional filters for feature extraction\n",
    "- Prepare image datasets using torchvision transforms and DataLoaders\n",
    "- Build CNN architectures from scratch with PyTorch\n",
    "- Train and evaluate CNNs on a real industrial classification task\n",
    "- Implement MobileNet with depthwise separable convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab: downloading utils...\")\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt\",\n",
    "        \"-O\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "    subprocess.run([\n",
    "        \"wget\",\n",
    "        \"-q\",\n",
    "        \"--show-progress\",\n",
    "        \"-x\",\n",
    "        \"-nH\",\n",
    "        \"--cut-dirs=3\",\n",
    "        \"-i\",\n",
    "        \"colab_utils.txt\",\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(\"Running locally: skipping Colab utils download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Setup paths for helper utilities\n",
    "helper_utils = Path(Path.cwd().parent)\n",
    "if str(helper_utils) not in sys.path:\n",
    "    sys.path.append(str(helper_utils))\n",
    "\n",
    "\n",
    "import utils \n",
    "import shutil\n",
    "import requests\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available. Please ensure you've enabled GPU in Runtime > Change runtime type\")\n",
    "\n",
    "ascent_url = 'https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/ascent.jpg'\n",
    "response = requests.get(ascent_url)\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"30\"/> 1. Convolutional Neural Networks (CNNs)\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Convolutional Neural Networks (CNNs) are a specialized type of neural network designed for processing structured grid-like data, such as images, by using mathematical operations called convolutions.\n",
    "\n",
    "CNNs have revolutionized computer vision tasks and are the foundation of many modern systems for image recognition, object detection, segmentation, and more. Their architecture is inspired by the organization of the visual cortex in animals, where individual neurons respond to stimuli in restricted regions called receptive fields.\n",
    "\n",
    "## 1.1 Why Standard Neural Networks Struggle with Images\n",
    "***\n",
    "Images present unique challenges that make standard fully-connected neural networks inefficient:\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| **Spatial Relationships** | Standard networks don't account for spatial relationships between pixels |\n",
    "| **Parameter Explosion** | A 224×224×3 image would require over 150,000 weights per neuron |\n",
    "| **Translation Invariance** | Objects can appear anywhere in an image but have the same meaning |\n",
    "| **Feature Hierarchy** | Images contain low-level features (edges, textures) that compose into higher-level features |\n",
    "***\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/image_rgb.png\" width=\"90%\">\n",
    "</div>\n",
    "\n",
    "CNNs address these challenges through specialized architecture components that we'll explore in this workshop.\n",
    "\n",
    "Let's begin by understanding the core operation that gives CNNs their name: convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"30\"/> 2. The Convolution Operation\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: A convolution in the context of CNNs is a mathematical operation that combines two functions by multiplying them and integrating over their overlapping regions.\n",
    "\n",
    "In simple terms, convolution involves sliding a small window (called a filter or kernel) over an image and performing an element-wise multiplication between the filter and the pixel values, then summing the results to produce a single output value for each position.\n",
    "\n",
    "## 2.1 How Convolution Works\n",
    "***\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Position the filter at the top-left corner of the image |\n",
    "| 2 | Perform element-wise multiplication between the filter and the corresponding image pixels |\n",
    "| 3 | Sum all the resulting values to get a single output value |\n",
    "| 4 | Move the filter to the next position (typically one pixel to the right) |\n",
    "| 5 | Repeat steps 2-4 until the entire image has been covered |\n",
    "\n",
    "***\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/convolution_hyperparameters.gif\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "This process creates what's called a feature map, which highlights specific patterns or features in the image that match the filter pattern.\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: In deep learning libraries, what's actually implemented is technically cross-correlation rather than convolution (the filter is not flipped). However, since the filters are learned during training, this distinction doesn't matter in practice.\n",
    "\n",
    "Let's first load an example image to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_image = Image.open(BytesIO(response.content)).resize((256, 256))\n",
    "asc_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Key Parameters in Convolution\n",
    "***\n",
    "The convolution operation is governed by several key parameters that affect the output dimensions and characteristics of the feature map. PyTorch provides a convenient way to implement convolutional layers using the `torch.nn.Conv2d` class. The key parameters include:\n",
    "\n",
    "| Parameter | Description | Effect on Output Dimensions |\n",
    "|-----------|-------------|---------------------------|\n",
    "| **Kernel Size** | The dimensions of the filter (e.g., 3×3, 5×5) | Larger kernels reduce output size more |\n",
    "| **Stride** | How many pixels the filter shifts at each step | Larger strides reduce output dimensions |\n",
    "| **Padding** | Adding extra pixels around the border | Can preserve input dimensions |\n",
    "| **Dilation** | Spacing between kernel elements | Increases receptive field without increasing parameters |\n",
    "\n",
    "Understanding how these parameters affect the output dimensions is crucial for designing effective CNN architectures. The formula for calculating the output dimensions of a convolutional layer is:\n",
    "\n",
    "$$\\text{Output Size} = \\left\\lfloor\\frac{\\text{Input Size} - \\text{Kernel Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1\\right\\rfloor$$\n",
    "\n",
    "where $\\lfloor \\cdot \\rfloor$ represents the floor operation (rounding down to the nearest integer).\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: This formula assumes that both the input and kernel are square, but it can be applied separately to height and width for rectangular inputs and kernels.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 1**: Implement a function that calculates the output dimensions of a convolutional layer given the input size, kernel size, stride, and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 1: Calculating Convolutional Output Dimensions\n",
    "# Implement a function to calculate the output dimensions after applying convolution\n",
    "# with different kernel sizes, strides, and padding values.\n",
    "\n",
    "def calculate_output_size(input_height:int, input_width:int, \n",
    "                          kernel_size:int, stride:int=1, padding:int=0) -> tuple:\n",
    "    \"\"\"Calculate the output dimensions after applying convolution.\n",
    "    \n",
    "    Args:\n",
    "        input_height (int): Height of the input feature map\n",
    "        input_width (int): Width of the input feature map\n",
    "        kernel_size (int): Size of the square kernel\n",
    "        stride (int, optional): Convolution stride. Defaults to 1.\n",
    "        padding (int, optional): Padding size. Defaults to 0.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (output_height, output_width)\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # TODO: COMPLETE THE CODE BELOW\n",
    "    # Apply the formula: floor((input_size - kernel_size + 2*padding) / stride + 1)\n",
    "    output_height = ___\n",
    "    output_width  = ___\n",
    "\n",
    "    return output_height, output_width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with different parameters\n",
    "# Case 1: Standard convolution with a 3x3 kernel, stride=1, no padding\n",
    "input1 = (28, 28)   # e.g., MNIST image size\n",
    "output1 = calculate_output_size(input1[0], input1[1], kernel_size=___)\n",
    "\n",
    "# Case 2: Convolution with padding=1 to preserve spatial dimensions\n",
    "input2 = (224, 224)  # e.g., Standard ImageNet size\n",
    "output2 = calculate_output_size(input2[0], input2[1], kernel_size=___, stride=___, padding=___)\n",
    "\n",
    "# Case 3: Convolution with stride=2 for downsampling\n",
    "input3 = (128, 128)\n",
    "output3 = calculate_output_size(input3[0], input3[1], kernel_size=___, stride=___, padding=___)\n",
    "\n",
    "# Case 4: Custom parameters\n",
    "input4 = (64, 64)\n",
    "output4 = calculate_output_size(input4[0], input4[1], kernel_size=___, stride=___, padding=___)\n",
    "\n",
    "print(f\"Case 1: {input1} → {output1} (3x3 kernel, stride=1, no padding)\")\n",
    "print(f\"Case 2: {input2} → {output2} (3x3 kernel, stride=1, padding=1)\")\n",
    "print(f\"Case 3: {input3} → {output3} (5x5 kernel, stride=2, padding=2)\")\n",
    "print(f\"Case 4: {input4} → {output4} (7x7 kernel, stride=2, padding=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 What is a Filter?\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: A filter (or kernel) is a small matrix used in convolutional operations to extract features from an image. It slides over the image, performing element-wise multiplication and summing the results to produce a single output value.\n",
    "\n",
    "Filters allow CNNs to learn and detect specific patterns, such as edges, textures, and shapes, by adjusting their weights during training. The concept of filters is central to computer vision tasks, and there are existing filters for common tasks, such as edge detection and blurring. Let's explore some of these filters and their effects on images.\n",
    "\n",
    "We are going to try the following filters:\n",
    "\n",
    "| Filter | Kernel | Description |\n",
    "|--------|--------|-------------|\n",
    "| **Edge Detection** | $$\\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1 \\end{bmatrix}$$ | Detects vertical edges |\n",
    "| **Sharpening** | $$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}$$ | Enhances edges and details |\n",
    "| **Embossing** | $$\\begin{bmatrix} -2 & -1 & 0 \\\\ -1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$$ | Creates a 3D effect |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 2**: Implement the `apply_filter_pytorch` function and apply three classic filters (edge detection, sharpening, embossing) to a test image using PyTorch's `Conv2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 2: Designing Convolutional Filters with PyTorch\n",
    "# In this exercise, you will implement common filters used in image processing using PyTorch\n",
    "\n",
    "def apply_filter_pytorch(image, kernel):\n",
    "    \"\"\"Apply a convolutional filter to an image using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image (grayscale or RGB)\n",
    "        kernel (numpy.ndarray): Convolutional kernel/filter\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Filtered image\n",
    "    \"\"\"\n",
    "    # Make a copy of the image to avoid modifying the original\n",
    "    image_copy = image.copy().astype(np.float32)\n",
    "    \n",
    "    # Rearrange to PyTorch format (B, C, H, W)\n",
    "    image_tensor = torch.from_numpy(image_copy).permute(2, 0, 1).unsqueeze(0)\n",
    "    channels = image_copy.shape[2]\n",
    "    \n",
    "    # Convert kernel to PyTorch tensor\n",
    "    kernel_tensor = torch.from_numpy(kernel.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Create a Conv2d layer using the kernel as fixed (non-learnable) weights\n",
    "    # groups=channels applies the same kernel to each colour channel independently\n",
    "    conv_layer = torch.nn.Conv2d(in_channels=channels,\n",
    "                                 out_channels=channels,\n",
    "                                 kernel_size=kernel.shape[0],\n",
    "                                 bias=False,\n",
    "                                 padding=kernel.shape[0]//2,\n",
    "                                 groups=channels)\n",
    "    \n",
    "    # Set the convolutional weights to our custom kernel\n",
    "    with torch.no_grad():\n",
    "        for i in range(channels):\n",
    "            conv_layer.weight.data[i] = kernel_tensor\n",
    "\n",
    "    ###################\n",
    "    # TODO: COMPLETE THE CODE BELOW\n",
    "    # Apply the conv_layer to image_tensor (use torch.no_grad() since we don't need gradients)\n",
    "    with torch.no_grad():\n",
    "        filtered = ___\n",
    "\n",
    "    # Convert the output tensor back to numpy:\n",
    "    # squeeze batch dim, permute (C,H,W) → (H,W,C), then call .numpy()\n",
    "    filtered_image = ___\n",
    "\n",
    "    # Clip pixel values to [0, 255] and cast to uint8\n",
    "    filtered_image = ___\n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design several common convolutional filters\n",
    "# (values are taken from the table in the markdown above)\n",
    "\n",
    "# 1. Edge detection filter — highlights boundaries between regions\n",
    "edge_detection_kernel = np.array([\n",
    "    [ 1,  0, -1],\n",
    "    [ 0,  0,  0],\n",
    "    [-1,  0,  1]\n",
    "])\n",
    "\n",
    "# 2. Sharpening filter — enhances details by boosting contrast at edges\n",
    "sharpen_kernel = np.array([\n",
    "    [ 0, -1,  0],\n",
    "    [-1,  5, -1],\n",
    "    [ 0, -1,  0]\n",
    "])\n",
    "\n",
    "# 3. Embossing filter — gives a 3-D relief effect\n",
    "emboss_kernel = np.array([\n",
    "    [-2, -1,  0],\n",
    "    [-1,  1,  1],\n",
    "    [ 0,  1,  2]\n",
    "])\n",
    "\n",
    "# Apply the filters to the test image using your apply_filter_pytorch function\n",
    "test_image = np.array(asc_image)\n",
    "edge_detect_image = apply_filter_pytorch(test_image, edge_detection_kernel)\n",
    "sharpened_image   = apply_filter_pytorch(test_image, sharpen_kernel)\n",
    "embossed_image    = apply_filter_pytorch(test_image, emboss_kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "\n",
    "axes[0, 0].imshow(test_image);       axes[0, 0].set_title(\"Original Image\");     axes[0, 0].axis('off')\n",
    "axes[0, 1].imshow(edge_detect_image); axes[0, 1].set_title(\"Edge Detection\");     axes[0, 1].axis('off')\n",
    "axes[1, 0].imshow(sharpened_image);   axes[1, 0].set_title(\"Sharpening\");         axes[1, 0].axis('off')\n",
    "axes[1, 1].imshow(embossed_image);    axes[1, 1].set_title(\"Embossing\");          axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we used predefined filters, but in practice, the filters are learned during training. The network learns to adjust the filter weights to detect relevant features for the specific task at hand.\n",
    "\n",
    "Let's see how the output of a simple convolution operation looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = torch.nn.Conv2d(\n",
    "    in_channels=3, \n",
    "    out_channels=3, \n",
    "    kernel_size=3, \n",
    "    stride=1, \n",
    "    padding=12, \n",
    ")\n",
    "\n",
    "torch.nn.init.xavier_uniform_(conv2d.weight)\n",
    "\n",
    "# Change the shape of the image to (C, H, W)\n",
    "torch_asc = torch.from_numpy(np.array(asc_image)).permute(2,0,1)\n",
    "torch_asc = torch_asc.unsqueeze(0).float() # Add batch dimension\n",
    "\n",
    "conv2d.eval()\n",
    "filtered_asc = conv2d(torch_asc)\n",
    "# Reverse the transformation to get back to (H, W, C)\n",
    "filtered_asc = filtered_asc.squeeze(0).detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Make sure the values are in the range [0, 255]\n",
    "# and convert to uint8 for PIL\n",
    "filtered_asc = np.clip(filtered_asc, 0, 255).astype(np.uint8) \n",
    "filtered_asc_img = Image.fromarray(filtered_asc)\n",
    "filtered_asc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"30\"/> 3. Preparing image data\n",
    "***\n",
    "Since images can be seen as 3D tensors, we need to convert them into a format suitable for processing. In PyTorch, images are typically represented as 4D tensors with the shape `(batch_size, channels, height, width)`. For a single image, the shape would be `(1, 3, height, width)`.\n",
    "\n",
    "To prepare the image data, we will use the `torchvision` library, which provides convenient functions for loading and transforming images. \n",
    "\n",
    "## 3.1 Torchvision transforms\n",
    "***\n",
    "Python uses `PIL` (Python Imaging Library) to handle images, and while `PIL` is great for basic image manipulation, it can be slow for large datasets. To speed up the process, we can use `torchvision.transforms`, which provides a set of common image transformations that can be applied to images in a more efficient way.\n",
    "\n",
    "| Transform | PyTorch Function | Description |\n",
    "|-----------|------------------|-------------|\n",
    "| **Resize** | `transforms.Resize(size)` | Resizes the image to the specified size |\n",
    "| **CenterCrop** | `transforms.CenterCrop(size)` | Crops the image at the center to the specified size |\n",
    "| **RandomCrop** | `transforms.RandomCrop(size)` | Crops the image randomly to the specified size |\n",
    "| **RandomHorizontalFlip** | `transforms.RandomHorizontalFlip(p)` | Flips the image horizontally with probability `p` |\n",
    "| **RandomRotation** | `transforms.RandomRotation(degrees)` | Rotates the image randomly within the specified degrees |\n",
    "| **Normalize** | `transforms.Normalize(mean, std)` | Normalizes the image tensor with the specified mean and standard deviation |\n",
    "| **ColorJitter** | `transforms.ColorJitter(brightness, contrast, saturation, hue)` | Randomly changes the brightness, contrast, saturation, and hue of the image |\n",
    "| **ToTensor** | `transforms.ToTensor()` | Converts the image to a PyTorch tensor |\n",
    "\n",
    "These transformations can be combined to create a preprocessing pipeline that prepares the images for training. The `transforms.Compose` function allows us to chain multiple transformations together.\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Notes**:\n",
    "> - Resizing is important because CNNs require fixed-size inputs.\n",
    "> - The `ToTensor` transformation converts the image to a PyTorch tensor, and it also scales the pixel values to the range [0, 1]. \n",
    "> - Normalization is a common practice in deep learning to ensure that the input data has a mean of 0 and a standard deviation of 1. This helps the model converge faster during training.\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: Composing transformations\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "ts = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "```\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 3**: Implement and visualize various image transformations — resize, crop, flip, rotation, colour jitter, and normalisation — using `torchvision.transforms.Compose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 3: Implementing Image Transformations\n",
    "# In this exercise, you will implement and visualize various image transformations\n",
    "# commonly used in computer vision tasks\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def apply_transformations(image_path):\n",
    "    \"\"\"Apply and visualize various image transformations.\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the input image\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of transformed images\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path) if isinstance(image_path, (str, Path)) else image_path\n",
    "\n",
    "    # 1. Basic resize to 128x128\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 2. Center crop transformation\n",
    "    center_crop_transform = transforms.Compose([\n",
    "        transforms.Resize(150),\n",
    "        transforms.CenterCrop(100),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 3. Random crop transformation\n",
    "    random_crop_transform = transforms.Compose([\n",
    "        transforms.Resize(150),\n",
    "        transforms.RandomCrop(100),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 4. Random horizontal flip — p=1.0 flips 100% of the time for visualisation\n",
    "    hflip_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 5. Random rotation transformation\n",
    "    rotate_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    ###################\n",
    "    # TODO: COMPLETE THE CODE BELOW\n",
    "    # 6. Color jitter — randomly perturbs brightness, contrast, saturation, and hue\n",
    "    #    Hint: brightness and contrast in [0, 1]; saturation in [0, 1]; hue in [0, 0.5]\n",
    "    color_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ColorJitter(brightness=___, contrast=___, saturation=___, hue=___),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 7. Combined / practical data-augmentation pipeline\n",
    "    #    Hint: chain RandomResizedCrop → RandomHorizontalFlip → RandomRotation → ColorJitter\n",
    "    combined_transform = transforms.Compose([\n",
    "        transforms.Resize(150),\n",
    "        transforms.RandomResizedCrop(___, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(___),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 8. Normalisation — zero-mean / unit-std per channel (ImageNet statistics)\n",
    "    #    Hint: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    norm_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=___, std=___)\n",
    "    ])\n",
    "\n",
    "    transforms_list = {\n",
    "        'Original': transforms.ToTensor(),\n",
    "        'Resized': resize_transform,\n",
    "        'Center Crop': center_crop_transform,\n",
    "        'Random Crop': random_crop_transform,\n",
    "        'Horizontal Flip': hflip_transform,\n",
    "        'Random Rotation': rotate_transform,\n",
    "        'Color Jitter': color_transform,\n",
    "        'Combined': combined_transform,\n",
    "        'Normalized': norm_transform\n",
    "    }\n",
    "\n",
    "    transforms_dict = {\n",
    "        'Original':       transforms.ToTensor()(img),\n",
    "        'Resized':        resize_transform(img),\n",
    "        'Center Crop':    center_crop_transform(img),\n",
    "        'Random Crop':    random_crop_transform(img),\n",
    "        'Horizontal Flip': hflip_transform(img),\n",
    "        'Random Rotation': rotate_transform(img),\n",
    "        'Color Jitter':   color_transform(img),\n",
    "        'Combined':       combined_transform(img),\n",
    "        'Normalized':     norm_transform(img)\n",
    "    }\n",
    "\n",
    "    return transforms_dict, transforms_list\n",
    "\n",
    "\n",
    "# Apply transformations to the ascent image we loaded earlier and visualise the results\n",
    "ts_dict, ts_list = apply_transformations(asc_image)\n",
    "utils.plotting.se04_visualize_transformations(ts_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 NEU-CLS Surface Defect Dataset\n",
    "***\n",
    "In this session, we work with the **NEU-CLS** dataset (Surface Defect Classification), publicly released by the Surface Inspection Laboratory of Northeastern University. The dataset is collected from real industrial production lines and is designed for the automatic classification of strip surface defects.\n",
    "\n",
    "The dataset contains **6 defect classes**, each with exactly **300 grayscale images** at a resolution of **200×200 pixels**:\n",
    "\n",
    "| Class | Examples | Description |\n",
    "|-------|----------|-------------|\n",
    "| **Crazing** | 300 | Network of fine surface cracks |\n",
    "| **Inclusion** | 300 | Foreign material embedded in the surface |\n",
    "| **Patches** | 300 | Irregular patches on the surface |\n",
    "| **Pitted Surface** | 300 | Small pits distributed across the surface |\n",
    "| **Rolled-in Scale** | 300 | Oxide scale rolled into the surface during production |\n",
    "| **Scratches** | 300 | Linear scratches on the surface |\n",
    "\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Notes**:\n",
    "> - Unlike the binary crack/no-crack task, this is a **6-class** multi-class classification problem.\n",
    "> - All classes are **perfectly balanced** (300 images each) — no need for class balancing.\n",
    "> - Images are **grayscale** (1 channel) — remember this when building the CNN input layer.\n",
    "\n",
    "The dataset is distributed in **YOLO format** with  `images/` and `labels/` sub-folders. Since we only care about *classification* (not bounding box detection), we will read the class label from each annotation file and reorganise the images into the `ImageFolder`-compatible structure required by PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd() / \"datasets\"\n",
    "dataset_path = utils.data.download_dataset(\"neu-cls\",\n",
    "                                            dest_path=data_path,\n",
    "                                            extract=True,\n",
    "                                            remove_compressed=True)\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random as _random\n",
    "\n",
    "# NEU-CLS class mapping (YOLO class id → folder name)\n",
    "# These are sorted alphabetically, matching the order ImageFolder will assign indices.\n",
    "NEU_CLASSES = {\n",
    "    0: 'crazing',\n",
    "    1: 'inclusion',\n",
    "    2: 'patches',\n",
    "    3: 'pitted_surface',\n",
    "    4: 'rolled-in_scale',\n",
    "    5: 'scratches'\n",
    "}\n",
    "\n",
    "def get_class_names(dataset_path):\n",
    "    \"\"\"Try to read class names from a data.yaml file; fall back to NEU_CLASSES.\"\"\"\n",
    "    yaml_files = list(dataset_path.rglob('data.yaml')) + list(dataset_path.rglob('*.yaml'))\n",
    "    if yaml_files:\n",
    "        try:\n",
    "            import yaml\n",
    "            with open(yaml_files[0], 'r') as f:\n",
    "                cfg = yaml.safe_load(f)\n",
    "            names = cfg.get('names', None)\n",
    "            if names:\n",
    "                if isinstance(names, list):\n",
    "                    return {i: n.lower().replace(' ', '_') for i, n in enumerate(names)}\n",
    "                return {int(k): v.lower().replace(' ', '_') for k, v in names.items()}\n",
    "        except Exception:\n",
    "            pass\n",
    "    return NEU_CLASSES\n",
    "\n",
    "def reorganize_yolo_to_imagefolder(dataset_path, output_path,\n",
    "                                   train_ratio=0.70, val_ratio=0.15,\n",
    "                                   seed=42):\n",
    "    \"\"\"\n",
    "    Reorganise NEU-CLS from YOLO format into an ImageFolder-compatible structure\n",
    "    with a proper *stratified* train / val / test split.\n",
    "\n",
    "    Pools images from BOTH the YOLO 'train' and 'valid' directories so the ~30\n",
    "    validation images are no longer the sole source of val/test data.\n",
    "\n",
    "    Split (default): 70% train · 15% val · 15% test — per class.\n",
    "\n",
    "    Returns:\n",
    "        dict: mapping of class index → class name\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    output_path  = Path(output_path)\n",
    "    class_names  = get_class_names(dataset_path)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Collect every labelled image from all YOLO splits grouped by class\n",
    "    # ------------------------------------------------------------------\n",
    "    by_class: dict = {}\n",
    "    for yolo_split in ('train/train', 'valid/valid'):\n",
    "        img_dir = dataset_path / yolo_split / 'images'\n",
    "        lbl_dir = dataset_path / yolo_split / 'labels'\n",
    "        if not img_dir.exists():\n",
    "            print(f\"  Warning: {img_dir} not found — skipping.\")\n",
    "            continue\n",
    "        for img_path in sorted(img_dir.glob('*.*')):\n",
    "            lbl_path = lbl_dir / (img_path.stem + '.txt')\n",
    "            if not lbl_path.exists():\n",
    "                continue\n",
    "            lines = [l.strip() for l in lbl_path.read_text().splitlines() if l.strip()]\n",
    "            if not lines:\n",
    "                continue\n",
    "            class_id   = int(lines[0].split()[0])\n",
    "            class_name = class_names.get(class_id, f'class_{class_id}')\n",
    "            by_class.setdefault(class_name, []).append(img_path)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. Stratified split: shuffle each class → 70 / 15 / 15\n",
    "    # ------------------------------------------------------------------\n",
    "    _random.seed(seed)\n",
    "    train_count = val_count = test_count = 0\n",
    "\n",
    "    for cls_name, imgs in by_class.items():\n",
    "        _random.shuffle(imgs)\n",
    "        n       = len(imgs)\n",
    "        n_train = int(n * train_ratio)\n",
    "        n_val   = int(n * val_ratio)\n",
    "        splits  = {\n",
    "            'train': imgs[:n_train],\n",
    "            'val':   imgs[n_train : n_train + n_val],\n",
    "            'test':  imgs[n_train + n_val :],\n",
    "        }\n",
    "        for split_name, split_imgs in splits.items():\n",
    "            dst = output_path / split_name / cls_name\n",
    "            dst.mkdir(parents=True, exist_ok=True)\n",
    "            for img_path in split_imgs:\n",
    "                shutil.copy(img_path, dst / img_path.name)\n",
    "        train_count += len(splits['train'])\n",
    "        val_count   += len(splits['val'])\n",
    "        test_count  += len(splits['test'])\n",
    "\n",
    "    print(f\"  Copied {train_count} images → {output_path / 'train'}\")\n",
    "    print(f\"  Copied {val_count}   images → {output_path / 'val'}\")\n",
    "    print(f\"  Copied {test_count}  images → {output_path / 'test'}\")\n",
    "    return class_names\n",
    "\n",
    "# ── Run the reorganisation ──────────────────────────────────────────────────\n",
    "# Force re-creation so the new stratified split always takes effect.\n",
    "clf_data_path = data_path / 'neu_cls_imagefolder'\n",
    "\n",
    "if clf_data_path.exists():\n",
    "    print(\"Removing old ImageFolder structure to rebuild with proper 70/15/15 split …\")\n",
    "    shutil.rmtree(clf_data_path)\n",
    "\n",
    "print(\"Reorganising dataset with stratified 70 / 15 / 15 split …\")\n",
    "class_names_map = reorganize_yolo_to_imagefolder(dataset_path, clf_data_path)\n",
    "\n",
    "train_folder = clf_data_path / 'train'\n",
    "val_folder   = clf_data_path / 'val'\n",
    "test_folder  = clf_data_path / 'test'\n",
    "\n",
    "print(f\"\\nClasses : {list(class_names_map.values())}\")\n",
    "print(f\"Train   : {sum(1 for _ in train_folder.rglob('*.*') if _.is_file())} images\")\n",
    "print(f\"Val     : {sum(1 for _ in val_folder.rglob('*.*')   if _.is_file())} images\")\n",
    "print(f\"Test    : {sum(1 for _ in test_folder.rglob('*.*')  if _.is_file())} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one sample image from each defect class\n",
    "class_dirs = sorted([d for d in train_folder.iterdir() if d.is_dir()])\n",
    "fig, axes = plt.subplots(1, len(class_dirs), figsize=(16, 3))\n",
    "for ax, cls_dir in zip(axes, class_dirs):\n",
    "    img_files = list(cls_dir.glob('*.*'))\n",
    "    if img_files:\n",
    "        img = Image.open(img_files[0])\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(cls_dir.name.replace('_', '\\n'), fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('NEU-CLS: One Sample per Defect Class', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display image shape information\n",
    "sample_img = Image.open(img_files[0])\n",
    "print(f\"Image size : {sample_img.size}  (W × H)\")\n",
    "print(f\"Image mode : {sample_img.mode}\")\n",
    "print(f\"Classes    : {[d.name for d in class_dirs]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 PyTorch ImageFolder\n",
    "***\n",
    "While we can load images using `PIL`, PyTorch provides a more efficient way to handle large datasets through the `torchvision.datasets` module. This module contains the `ImageFolder` class, which allows us to load images from a directory structure where each subdirectory represents a class. The `ImageFolder` class automatically assigns labels based on the subdirectory names.\n",
    "\n",
    "The `ImageFolder` class requires a root directory containing subdirectories for each class. The directory structure — which we just created — looks like this:\n",
    "\n",
    "```bash\n",
    "neu_cls_imagefolder/\n",
    "    ├── train/\n",
    "    │   ├── crazing/         ← class 0\n",
    "    │   ├── inclusion/       ← class 1\n",
    "    │   ├── patches/         ← class 2\n",
    "    │   ├── pitted_surface/  ← class 3\n",
    "    │   ├── rolled-in_scale/ ← class 4\n",
    "    │   └── scratches/       ← class 5\n",
    "    ├── val/\n",
    "    │   └── (same 6 sub-folders)\n",
    "    └── test/\n",
    "        └── (same 6 sub-folders)\n",
    "```\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Important**: `ImageFolder` sorts class sub-folders **alphabetically** and assigns integer labels accordingly (0, 1, 2, …). This is why the `NEU_CLASSES` dictionary we built in the previous cell was sorted to match.\n",
    "\n",
    "The key parameters of the `ImageFolder` class are:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| **root** | The root directory containing the dataset |\n",
    "| **transform** | A function/transform to apply to the images |\n",
    "| **target_transform** | A function/transform to apply to the target (label) |\n",
    "| **loader** | A function to load the images (default is `PIL.Image.open`) |\n",
    "| **is_valid_file** | A function to check if a file is valid (default is `None`) |\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 2**: Using ImageFolder\n",
    "\n",
    "```python\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset = ImageFolder(root='path/to/dataset', transform=ts)\n",
    "# Accessing the first image and its label\n",
    "image, label = dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")\n",
    "print(f\"Classes: {dataset.classes}\")\n",
    "```\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 4**: Define data augmentation transforms for the training set and minimal transforms for validation/test, then load the NEU-CLS dataset splits using `ImageFolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4: Data Augmentation and Loading with PyTorch\n",
    "# Implement:\n",
    "# 1. Data augmentation transforms for the NEU-CLS training set\n",
    "# 2. Minimal transforms for validation and test sets\n",
    "# 3. Load datasets using ImageFolder\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Define the training transform pipeline.\n",
    "# ⚠️  Images are GRAYSCALE (1 channel) — start with transforms.Grayscale(num_output_channels=1).\n",
    "# Include at least: Resize → augmentation transforms → ToTensor\n",
    "# Hint: useful augmentations for defect images: RandomHorizontalFlip, RandomVerticalFlip, RandomRotation\n",
    "ts_train = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # ensure single channel\n",
    "    transforms.Resize(___),                        # resize to (128, 128)\n",
    "    transforms.___(),                              # random left-right flip\n",
    "    transforms.___(p=___),                         # random up-down flip (p=0.2)\n",
    "    transforms.___(___),                           # random rotation ±15°\n",
    "    transforms.ToTensor(),                         # → [0, 1] tensor\n",
    "])\n",
    "\n",
    "# Validation / test transforms — no augmentation, just resize and normalise.\n",
    "# Hint: Grayscale → Resize → ToTensor (same sizes as training)\n",
    "ts_test_val = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(___),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets using ImageFolder\n",
    "# Hint: ImageFolder(root=<folder_path>, transform=<transform_pipeline>)\n",
    "train_data = ImageFolder(root=___, transform=___)\n",
    "val_data   = ImageFolder(root=___, transform=___)\n",
    "test_data  = ImageFolder(root=___, transform=___)\n",
    "\n",
    "print(f\"Classes ({len(train_data.classes)}): {train_data.classes}\")\n",
    "print(f\"Train samples : {len(train_data)}\")\n",
    "print(f\"Val   samples : {len(val_data)}\")\n",
    "print(f\"Test  samples : {len(test_data)}\")\n",
    "\n",
    "# Verify shape — should be (1, 128, 128) for a single-channel image\n",
    "sample_img, sample_lbl = train_data[0]\n",
    "print(f\"\\nSample tensor shape : {sample_img.shape}  (C × H × W)\")\n",
    "print(f\"Sample label        : {sample_lbl}  ({train_data.classes[sample_lbl]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 PyTorch DataLoaders\n",
    "***\n",
    "As we discussed in the previous session, when training a model we need to load the data in batches. PyTorch provides the `DataLoader` class to handle this efficiently. The `DataLoader` class takes a dataset and provides an iterable over the dataset, allowing us to load data in batches.\n",
    "\n",
    "The model expects our image data to be formatted as a 4D tensor with the shape `(batch_size, channels, height, width)`.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/image_batches.png\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "The `DataLoader` class provides several key parameters to customize the data loading process:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| **dataset** | The dataset to load data from (e.g., `ImageFolder`) |\n",
    "| **batch_size** | The number of samples per batch |\n",
    "| **shuffle** | Whether to shuffle the data at every epoch |\n",
    "| **num_workers** | The number of subprocesses to use for data loading |\n",
    "| **pin_memory** | Whether to pin memory for faster data transfer to GPU |\n",
    "| **drop_last** | Whether to drop the last incomplete batch |\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 3**: Creating a DataLoader\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# Iterate through the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch shape: {images.shape}, Labels: {labels}\")\n",
    "    break  # Just to show the first batch\n",
    "```\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 5**: Wrap the three `ImageFolder` datasets in `DataLoader` objects, using `shuffle=True` for training and `shuffle=False` for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 5: DataLoaders\n",
    "# Wrap the three ImageFolder datasets in DataLoaders.\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Create DataLoaders for train, validation, and test sets.\n",
    "# Hint: use shuffle=True for training (randomises batch order each epoch)\n",
    "#       and shuffle=False for val/test (deterministic evaluation)\n",
    "train_dl = DataLoader(___, batch_size=___, shuffle=___)\n",
    "val_dl   = DataLoader(___, batch_size=___, shuffle=___)\n",
    "test_dl  = DataLoader(___, batch_size=___, shuffle=___)\n",
    "\n",
    "# Inspect one batch to confirm shape: (batch_size, channels, height, width)\n",
    "imgs, lbls = next(iter(train_dl))\n",
    "print(f\"Batch shape : {imgs.shape}  — (N, C, H, W)\")\n",
    "print(f\"Labels      : {lbls[:8].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"30\"/> 4. Implementing CNNs\n",
    "***\n",
    "The architecture of a CNN is not that different from a standard neural network. The main difference is that CNNs use convolutional layers instead of fully connected layers. This means that after each convolutional layer, we typically apply a non-linear activation function (like ReLU). \n",
    "\n",
    "The output of the CNN is then passed through one or more fully connected layers to produce the final output. Thus, we need to keep track of the output size after each layer to ensure that the dimensions match up correctly.\n",
    "\n",
    "A diagram of a conventional CNN architecture is shown below.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/cnn.png\" width=\"90%\">\n",
    "</div>\n",
    "\n",
    "We are going to implement a simple CNN architecture for the NEU-CLS surface defect classification task. The architecture consists of the following layers:\n",
    "\n",
    "| Type | Layer | Input Size | Output Size | Activation Function |\n",
    "|-------|-------|------------|-------------|---------------------|\n",
    "| Convolution | `Conv2d` | `(1, 128, 128)` | `(16, 128, 128)` | ReLU |\n",
    "| Pooling | `MaxPool2d` | `(16, 128, 128)` | `(16, 64, 64)` | — |\n",
    "| Convolution | `Conv2d` | `(16, 64, 64)` | `(16, 64, 64)` | ReLU |\n",
    "| Fully Connected | `Linear` | `(16 × 64 × 64)` | `32` | ReLU |\n",
    "| Fully Connected | `Linear` | `32` | `6` | None |\n",
    "\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: The input has **1 channel** (grayscale) and the output has **6 neurons** (one per defect class). The `MaxPool2d(2×2)` layer halves the spatial dimensions after the first convolution, keeping the fully-connected head manageable while capturing larger receptive fields.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 6**: Implement the `simpleCNN` class with one convolutional layer, max-pooling, and two fully-connected layers for 6-class surface defect classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 6: Implementing a Simple CNN Model\n",
    "# Build a minimal CNN for 6-class surface defect classification.\n",
    "# Key differences from a typical RGB model:\n",
    "#   • in_channels = 1   (grayscale images)\n",
    "#   • n_classes   = 6   (six defect types)\n",
    "\n",
    "class simpleCNN(torch.nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(simpleCNN, self).__init__()\n",
    "\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Conv layer: 1 input channel → 16 feature maps, 3×3 kernel, same padding (padding=1)\n",
    "        # Hint: torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv1 = ___\n",
    "\n",
    "        # MaxPool 2×2: halves spatial dimensions → 128×128 → 64×64\n",
    "        # Hint: torch.nn.MaxPool2d(kernel_size, stride)\n",
    "        self.pool  = ___\n",
    "\n",
    "        # Conv layer 2 after pool: 16 feature maps → 16 feature maps, kernel 3×3, same padding\n",
    "        self.conv2 = ___\n",
    "\n",
    "        # After conv2 : feature map shape is (16, 64, 64)\n",
    "        # Flatten → 16 × 64 × 64 = 65 536 values → down to 32\n",
    "        # Hint: torch.nn.Linear(in_features, out_features)\n",
    "        self.fc1 = ___\n",
    "        self.fc2 = ___  # output: one logit per class\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Step 1: Apply conv1, ReLU activation, then max-pool\n",
    "        # Step 2: Flatten (x.view(-1, 16 * 64 * 64))\n",
    "        # Step 3: Apply fc1 with ReLU, then fc2 for final logits\n",
    "        x = ___\n",
    "        x = ___  # flatten\n",
    "        x = ___\n",
    "        x = ___\n",
    "        return x\n",
    "\n",
    "model_v1 = simpleCNN(len(train_data.classes))\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Define the loss function (multi-class → CrossEntropyLoss)\n",
    "# and the optimiser (Adam, lr=3e-3)\n",
    "criterion = torch.nn.___()\n",
    "optimiser = torch.optim.___(model_v1.parameters(), lr=___)\n",
    "num_epochs = ___\n",
    "\n",
    "print(model_v1)\n",
    "print(f\"\\nOutput classes : {len(train_data.classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = utils.ml.train_model(model_v1,\n",
    "                                criterion,\n",
    "                                optimiser,\n",
    "                                train_loader=train_dl,\n",
    "                                val_loader=val_dl,\n",
    "                                num_epochs=num_epochs,\n",
    "                                plot_loss=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting Predictions\n",
    "***\n",
    "The output of the last **fully-connected** layer gives us raw **logits** — one per class (six in our case). To obtain a predicted class label we take the index of the highest logit using `torch.argmax` (or equivalently `torch.max(..., dim=1)`).\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: PyTorch's `CrossEntropyLoss` applies softmax internally, so we do **not** add a softmax layer at the end of the model during training. For inference we also skip softmax when we only need the predicted label (we just pick the maximum logit).\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 4**: Obtaining the predicted class\n",
    "\n",
    "```python\n",
    "model_v1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dl:\n",
    "        outputs = model_v1(images)   # (batch, 6) logits\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        print(\"Logits shape:\", outputs.shape)\n",
    "        print(\"Predictions :\", predicted[:8].tolist())\n",
    "        print(\"True labels :\", labels[:8].tolist())\n",
    "        break\n",
    "```\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 7**: Compute the test accuracy and classification report for the simple CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 7: Evaluating the Simple CNN\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Compute the test accuracy and classification report for model_v1.\n",
    "# Hint: use utils.ml.compute_accuracy(model, dataloader)\n",
    "#       and   utils.ml.compute_classification_report(model, dataloader, class_names=...)\n",
    "acc = utils.ml.compute_accuracy(___, ___)\n",
    "print(f\"Test accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "cls_report = utils.ml.compute_classification_report(\n",
    "    ___, ___, class_names=___\n",
    ")\n",
    "print('-' * 60)\n",
    "print(f\"Classification Report:\\n{cls_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model predictions\n",
    "utils.plotting.show_model_predictions(model_v1, test_dl, class_names=train_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 Building MobileNet from Scratch\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: MobileNet is a lightweight CNN architecture designed for mobile and embedded vision applications, introduced by Google in 2017. Its key innovation is replacing standard convolutions with **depthwise separable convolutions**, dramatically reducing the number of parameters and computations without sacrificing accuracy.\n",
    "\n",
    "### Why MobileNet?\n",
    "\n",
    "Standard convolutions combine spatial filtering and channel mixing in one expensive step. MobileNet splits this into two cheaper operations:\n",
    "\n",
    "| Operation | What it does | Parameters |\n",
    "|-----------|-------------|-----------|\n",
    "| **Depthwise Conv** (`groups=in_channels`) | Applies one filter per input channel independently | $K \\times K \\times C_{in}$ |\n",
    "| **Pointwise Conv** (1×1 Conv) | Mixes the channel information | $C_{in} \\times C_{out}$ |\n",
    "\n",
    "The computational saving ratio vs a standard convolution is approximately:\n",
    "\n",
    "$$\\frac{1}{C_{out}} + \\frac{1}{K^2}$$\n",
    "\n",
    "For $K=3$, $C_{out}=64$: the depthwise separable block uses **~9× fewer operations**!\n",
    "\n",
    "### 4.3 Depthwise Separable Convolution — The Building Block\n",
    "***\n",
    "Each MobileNet block consists of:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/depthwise_conv.png\" width=\"90%\">\n",
    "</div>\n",
    "\n",
    "***\n",
    "```\n",
    "Input\n",
    "  │\n",
    "  ▼\n",
    "┌─────────────────────────────────────┐\n",
    "│  Depthwise Conv2d (groups=in_ch)    │   ← One spatial filter per channel\n",
    "│  BatchNorm2d  →  ReLU               │\n",
    "├─────────────────────────────────────┤\n",
    "│  Pointwise Conv2d (kernel=1×1)      │   ← Recombines channels\n",
    "│  BatchNorm2d  →  ReLU               │\n",
    "└─────────────────────────────────────┘\n",
    "  │\n",
    "  ▼\n",
    "Output\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 4.4 Our MobileNet Architecture\n",
    "***\n",
    "We build a miniaturised MobileNetV1 for 128×128 grayscale inputs. The initial stride-2 convolution and three stride-2 depthwise blocks progressively halve the spatial size while the number of channels grows:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/mobilnet.png\" width=\"90%\">\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "| Stage | Layer | Output Shape | Notes |\n",
    "|-------|-------|-------------|-------|\n",
    "| Entry | Conv2d (stride 2) | 64×64×32 | Standard conv |\n",
    "| DS Block 1 | DW+PW (stride 1) | 64×64×64 | |\n",
    "| DS Block 2 | DW+PW (stride 2) | 32×32×128 | ↓ spatial |\n",
    "| DS Block 3 | DW+PW (stride 1) | 32×32×128 | |\n",
    "| DS Block 4 | DW+PW (stride 2) | 16×16×256 | ↓ spatial |\n",
    "| DS Block 5 | DW+PW (stride 1) | 16×16×256 | |\n",
    "| GAP | AdaptiveAvgPool2d(1) | 1×1×256 | Global Average Pool |\n",
    "| Classifier | Linear + Dropout | 6 | One logit per class |\n",
    "\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Key Pattern**: Use `groups=in_channels` in `torch.nn.Conv2d` to create a **depthwise** convolution. Then follow it with a **1×1** `Conv2d` (pointwise) to recombine channels. This pair is what gives MobileNet its efficiency.\n",
    "\n",
    "### 4.5 Regularisation in MobileNet\n",
    "***\n",
    "MobileNet uses two main regularisation strategies:\n",
    "\n",
    "| Technique | PyTorch | Purpose |\n",
    "|-----------|---------|---------|\n",
    "| **Batch Normalisation** | `torch.nn.BatchNorm2d` | Stabilises training, acts as a mild regulariser |\n",
    "| **Dropout** | `torch.nn.Dropout(p)` | Prevents over-reliance on individual features in the classifier |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 8**: Implement `DepthwiseSeparableConv` and `MobileNetV1` from scratch, using depthwise separable convolutions with batch normalisation and global average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 8: Implementing MobileNet from Scratch\n",
    "# Build a MobileNetV1-inspired architecture using depthwise separable convolutions.\n",
    "#\n",
    "# Steps:\n",
    "#  1. Define a reusable DepthwiseSeparableConv block (DW conv + PW conv)\n",
    "#  2. Stack several blocks with increasing channels and strategic stride=2 for downsampling\n",
    "#  3. Replace the Flatten+Linear head with Global Average Pooling (AdaptiveAvgPool2d)\n",
    "\n",
    "class DepthwiseSeparableConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    One MobileNet building block:\n",
    "      Depthwise Conv (groups=in_channels) → BN → ReLU\n",
    "      Pointwise Conv (kernel=1×1)         → BN → ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Depthwise Conv: applies one 3×3 filter per input channel independently.\n",
    "        # Key: set groups=in_channels so each channel gets its own filter.\n",
    "        self.depthwise = ___\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        # Pointwise Conv: 1×1 convolution to recombine channel information.\n",
    "        # Hint: kernel_size=1, no bias needed\n",
    "        self.pointwise = ___\n",
    "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Apply depthwise conv → BN → ReLU, then pointwise conv → BN → ReLU\n",
    "        x = ___\n",
    "        x = ___\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileNetV1(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight MobileNetV1-inspired model for 6-class surface defect classification.\n",
    "\n",
    "    Input:  (N, 1, 128, 128)  — grayscale images\n",
    "    Output: (N, n_classes)    — raw logits\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes: int, in_channels: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Entry convolution (standard, stride=2 to halve the spatial size)\n",
    "        self.entry_conv = torch.nn.Conv2d(\n",
    "            in_channels, 32, kernel_size=3, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.entry_bn = torch.nn.BatchNorm2d(32)\n",
    "\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Stack DepthwiseSeparableConv blocks following the architecture table:\n",
    "        # Spatial sizes below assume 128×128 input → 64×64 after entry conv\n",
    "        #   Block 1: 32  → 64,  stride=1   (keeps 64×64 spatial size)\n",
    "        #   Block 2: 64  → 128, stride=2   (↓ to 32×32)\n",
    "        #   Block 3: 128 → 128, stride=1\n",
    "        #   Block 4: 128 → 256, stride=2   (↓ to 16×16)\n",
    "        #   Block 5: 256 → 256, stride=1\n",
    "        self.blocks = torch.nn.Sequential(\n",
    "            ___\n",
    "        )\n",
    "\n",
    "        # Classifier head — Global Average Pooling + Dropout + Linear\n",
    "        self.gap     = torch.nn.AdaptiveAvgPool2d((1, 1))  # → (N, 256, 1, 1)\n",
    "        self.flatten = torch.nn.Flatten()                   # → (N, 256)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.fc      = torch.nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###################\n",
    "        # TODO: COMPLETE THE CODE BELOW\n",
    "        # Pass x through: entry_conv → BN → ReLU → blocks → GAP → flatten → dropout → fc\n",
    "        x = torch.nn.functional.relu(self.entry_bn(self.entry_conv(x)))\n",
    "        x = self.___(x)   # depthwise separable blocks\n",
    "        x = self.___(x)   # global average pool\n",
    "        x = self.___(x)   # flatten\n",
    "        x = self.___(x)   # dropout\n",
    "        x = self.___(x)   # fully-connected classifier\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate and inspect the model\n",
    "model_v2 = MobileNetV1(n_classes=len(train_data.classes))\n",
    "print(model_v2)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_v2.parameters() if p.requires_grad)\n",
    "print(f\"\\nTrainable parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = MobileNetV1(n_classes=len(train_data.classes))\n",
    "\n",
    "criterion_reg = torch.nn.CrossEntropyLoss()\n",
    "optimiser_reg = torch.optim.AdamW(\n",
    "    model_v2.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,   # L2 regularisation (weight decay)\n",
    ")\n",
    "num_epochs_reg = 30\n",
    "\n",
    "# Reduce learning rate when validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimiser_reg,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = utils.ml.train_model(model_v2,\n",
    "                  criterion_reg,\n",
    "                  optimiser_reg,\n",
    "                  train_loader=train_dl,\n",
    "                  val_loader=val_dl,\n",
    "                  num_epochs=num_epochs_reg,\n",
    "                  early_stopping=True,\n",
    "                  patience=5,\n",
    "                  tolerance=1e-2,\n",
    "                  save_path=Path.cwd() / \"my_models\" / \"se04_mobilenet.pt\",\n",
    "                  plot_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint saved during training\n",
    "model_v2.load_state_dict(torch.load(Path.cwd() / \"my_models\" / \"se04_mobilenet.pt\"))\n",
    "print(\"Best MobileNet model loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Evaluating the MobileNet\n",
    "***\n",
    "Now that the MobileNet has been trained, let's evaluate its performance on the test set and compare it with the simple CNN.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Exercise 9**: Compute the test accuracy and full classification report for the trained MobileNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 9: Evaluating the MobileNet\n",
    "\n",
    "###################\n",
    "# TODO: COMPLETE THE CODE BELOW\n",
    "# Compute the test accuracy and full classification report for model_v2.\n",
    "# Hint: use utils.ml.compute_accuracy(model, dataloader)\n",
    "#       and   utils.ml.compute_classification_report(model, dataloader, class_names=...)\n",
    "acc = utils.ml.compute_accuracy(___, ___)\n",
    "print(f\"Test accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "cls_report = utils.ml.compute_classification_report(\n",
    "    ___, ___, class_names=___\n",
    ")\n",
    "print('-' * 60)\n",
    "print(f\"Classification Report:\\n{cls_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 🔍 Overfitting Diagnostic: Train / Val / Test accuracy comparison\n",
    "# A well-generalising model shows small gaps between all three splits.\n",
    "# A large train - test gap is the tell-tale sign of overfitting.\n",
    "\n",
    "train_acc = utils.ml.compute_accuracy(model_v2, train_dl)\n",
    "val_acc   = utils.ml.compute_accuracy(model_v2, val_dl)\n",
    "test_acc  = utils.ml.compute_accuracy(model_v2, test_dl)\n",
    "\n",
    "print(f\"{'Split':<10} {'Accuracy':>10}\")\n",
    "print(\"-\" * 22)\n",
    "print(f\"{'Train':<10} {train_acc * 100:>9.2f}%\")\n",
    "print(f\"{'Val':<10} {val_acc   * 100:>9.2f}%\")\n",
    "print(f\"{'Test':<10} {test_acc  * 100:>9.2f}%\")\n",
    "print(\"-\" * 22)\n",
    "gap = (train_acc - test_acc) * 100\n",
    "print(f\"\\nTrain → Test gap : {gap:.2f} pp\")\n",
    "if gap < 5:\n",
    "    print(\"✅ Gap < 5 pp — model generalises well, no significant overfitting.\")\n",
    "elif gap < 10:\n",
    "    print(\"⚠️  Gap 5–10 pp — mild overfitting; consider more augmentation or stronger dropout.\")\n",
    "else:\n",
    "    print(\"❌  Gap > 10 pp — clear overfitting; reduce model capacity or add regularisation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plotting.show_model_predictions(model_v2, test_dl, class_names=train_data.classes, num_images=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uom-fse-dl-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
